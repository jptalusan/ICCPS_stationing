{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import gtfs_kit as gk\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from shapely.geometry import Polygon, LineString\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gtfs_kit as gk\n",
    "import glob\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) \n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/22 13:16:52 WARN Utils: Your hostname, Slade-173477 resolves to a loopback address: 127.0.1.1; using 10.2.219.10 instead (on interface eno1)\n",
      "23/01/22 13:16:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/22 13:16:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/01/22 13:16:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config('spark.executor.cores', '16').config('spark.executor.memory', '80g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '80g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "        .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "        .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")\\\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")\\\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gtfs_date',\n",
       " 'dayofweek',\n",
       " 'hour',\n",
       " 'gtfs_route_id',\n",
       " 'gtfs_direction_id',\n",
       " 'stop_id',\n",
       " 'transit_date',\n",
       " 'trip_id',\n",
       " 'day',\n",
       " 'overload_id',\n",
       " 'vehicle_id',\n",
       " 'block_abbr',\n",
       " 'activation_date',\n",
       " 'activation_date_str',\n",
       " 'actual_hdwy',\n",
       " 'arrival_time',\n",
       " 'arrival_time_str',\n",
       " 'block_stop_order',\n",
       " 'deactivation_date',\n",
       " 'deactivation_date_str',\n",
       " 'delay_time',\n",
       " 'departure_time',\n",
       " 'departure_time_str',\n",
       " 'dwell_time',\n",
       " 'is_bunched',\n",
       " 'is_gapped',\n",
       " 'is_target',\n",
       " 'load',\n",
       " 'load_factor',\n",
       " 'map_latitude',\n",
       " 'map_longitude',\n",
       " 'offs',\n",
       " 'ons',\n",
       " 'pattern_num',\n",
       " 'prev_depart',\n",
       " 'prev_sched',\n",
       " 'route_direction_name',\n",
       " 'route_id',\n",
       " 'sched_hdwy',\n",
       " 'scheduled_time',\n",
       " 'scheduled_time_str',\n",
       " 'source_pattern_id',\n",
       " 'stop_id_list',\n",
       " 'stop_id_original',\n",
       " 'stop_name',\n",
       " 'stop_sequence',\n",
       " 'stop_sequence_list',\n",
       " 'transit_date_str',\n",
       " 'update_date',\n",
       " 'vehicle_capacity',\n",
       " 'zero_load_at_trip_end',\n",
       " 'count',\n",
       " 'darksky_temperature',\n",
       " 'darksky_humidity',\n",
       " 'darksky_nearest_storm_distance',\n",
       " 'darksky_precipitation_intensity',\n",
       " 'darksky_precipitation_probability',\n",
       " 'darksky_pressure',\n",
       " 'darksky_wind_gust',\n",
       " 'darksky_wind_speed',\n",
       " 'gtfs_file',\n",
       " 'gtfs_shape_id',\n",
       " 'gtfs_start_date',\n",
       " 'gtfs_end_date',\n",
       " 'gtfs_number_of_scheduled_trips',\n",
       " 'gtfs_number_of_scheduled_trips_at_stop',\n",
       " 'year',\n",
       " 'month']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apc_data_path = 'data/apc_weather_gtfs_20221216.parquet'\n",
    "apc_data = spark.read.load(apc_data_path)\n",
    "apc_data.createOrReplaceTempView(\"apc\")\n",
    "apc_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           earliest|             latest|\n",
      "+-------------------+-------------------+\n",
      "|2020-01-01 00:00:00|2022-11-11 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_earlist_date = F.min('transit_date').alias('earliest')\n",
    "col_latest_date = F.max('transit_date').alias('latest')\n",
    "df_result = apc_data.select(col_earlist_date, col_latest_date)\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate travel time to next stop data\n",
    "apc_data_path = 'data/apc_weather_gtfs_20221216.parquet'\n",
    "apc_data = spark.read.load(apc_data_path)\n",
    "apc_data.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "get_columns = ['transit_date', 'route_id', 'trip_id', 'stop_id_original', 'stop_sequence', 'load', 'arrival_time', 'departure_time', 'scheduled_time']\n",
    "get_str = \",\".join([c for c in get_columns])\n",
    "\n",
    "query = f\"\"\"\n",
    "        SELECT {get_str}\n",
    "        FROM apc\n",
    "        WHERE transit_date < '2022-05-01'\n",
    "        \"\"\"\n",
    "        \n",
    "apc_data = spark.sql(query)\n",
    "apc_data = apc_data.filter(F.col(\"load\") >= 0)\n",
    "apc_data = apc_data.orderBy(F.col(\"transit_date\"))\n",
    "df = apc_data.toPandas()\n",
    "df.groupby(['transit_date', 'trip_id']).ngroups\n",
    "df_arr = []\n",
    "i = 0\n",
    "curr_month = set()\n",
    "for (transit_date, trip_id), trip_df in df.groupby(['transit_date', 'trip_id']):\n",
    "    tdf = trip_df.sort_values(['stop_sequence'])\n",
    "    tdf['next_stop_id'] = tdf['stop_id_original'].shift(-1)\n",
    "    tdf['next_arrival'] = tdf['arrival_time'].shift(-1)\n",
    "    tdf['tt_to_next_stop'] = tdf['next_arrival'] - tdf['departure_time']\n",
    "    tdf['tt_to_next_stop'] = tdf['tt_to_next_stop'].dt.total_seconds()\n",
    "    df_arr.append(tdf)\n",
    "    x = str(transit_date.year) + '_' + str(transit_date.month)\n",
    "    if x not in curr_month:\n",
    "        print(x)\n",
    "        curr_month.add(x)\n",
    "    i+=1\n",
    "    if i == 3:\n",
    "        break\n",
    "_df = pd.concat(df_arr)\n",
    "_df = _df.drop('arrival_time', axis=1)\n",
    "_df = _df.reset_index(drop=True)\n",
    "# _df.to_parquet('data/tt_to_next_stop_2022_04_31.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the travel time factors using the `merging.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cbc6a907b83b27da5523d46a7bac98be149ac6d56d0f6f27499d324bd57fb04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
