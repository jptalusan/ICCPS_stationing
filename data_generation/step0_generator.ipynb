{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jptalusan/anaconda3/envs/py39/lib/python3.9/site-packages/geopandas/_compat.py:123: UserWarning: The Shapely GEOS version (3.10.3-CAPI-1.16.1) is incompatible with the GEOS version PyGEOS was compiled with (3.11.1-CAPI-1.17.1). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/06 15:35:17 WARN Utils: Your hostname, scope-vanderbilt resolves to a loopback address: 127.0.1.1; using 10.2.218.69 instead (on interface enp8s0)\n",
      "23/01/06 15:35:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/06 15:35:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import gtfs_kit as gk\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon, LineString, Point\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '80g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '40g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "        .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "        .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files for `EmpiricalTravelModelLookup.py`\n",
    "```\n",
    "disruption_path = f'{base_dir}/common/disruption_probabilities.pkl'\n",
    "self.sampled_disruption = pd.read_pickle(disruption_path)\n",
    "\n",
    "self.logger = logger\n",
    "\n",
    "with open(f'{base_dir}/common/sampled_travel_times_dict.pkl', 'rb') as handle:\n",
    "    self.sampled_travel_time = pickle.load(handle)\n",
    "\n",
    "with open(f'{base_dir}/common/stops_tt_dd_node_dict.pkl', 'rb') as handle:\n",
    "    self.stops_tt_dd_dict = pickle.load(handle)\n",
    "\n",
    "with open(f'{base_dir}/common/stops_node_matching_dict.pkl', 'rb') as handle:\n",
    "    self.stop_nodes_dict = pickle.load(handle)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disruption dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get service disruption dataset\n",
    "fp = os.path.join('data/Service Disruptions_07_2019_08_2022.csv')\n",
    "disruptions_df = pd.read_csv(fp)\n",
    "disruptions_df.head()\n",
    "disruptions_df['DATETIME'] = disruptions_df['DATE'] + ' ' + disruptions_df['TIME']\n",
    "disruptions_df['DATE'] = pd.to_datetime(disruptions_df['DATE'], format='%m/%d/%y', errors='coerce')\n",
    "disruptions_df['TIME'] = pd.to_datetime(disruptions_df['TIME'], format='%H:%M:%S', errors='coerce')\n",
    "disruptions_df['DATETIME'] = pd.to_datetime(disruptions_df['DATETIME'], format='%m/%d/%y %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Remove mechanical and weather disruptions\n",
    "disruptions_df = disruptions_df.query(\"REASON != 'Mechanical' and REASON != 'Weather'\")\n",
    "\n",
    "disruptions_df['BLOCK'] = disruptions_df['BLOCK'].astype('int32')# disruptions_sp = disruptions_sp.withColumn(\"BLOCK\", F.col(\"BLOCK\").cast(IntegerType()))\n",
    "disruptions_counts_df = disruptions_df.groupby('START_STOP_ABBR').agg('count')[['REASON']].reset_index().sort_values('REASON')\n",
    "\n",
    "# Count the number of trips throughout this time\n",
    "start_date = disruptions_df.sort_values(by=['DATETIME']).iloc[0]['DATETIME']\n",
    "end_date   = disruptions_df.sort_values(by=['DATETIME']).iloc[-1]['DATETIME']\n",
    "\n",
    "print(start_date, end_date)\n",
    "\n",
    "# Filter APC data on these dates\n",
    "# # filter subset\n",
    "get_columns = ['transit_date', 'trip_id', 'departure_time', 'stop_id_original']\n",
    "get_str = \", \".join([c for c in get_columns])\n",
    "\n",
    "f = os.path.join('/home/jptalusan/mta_stationing_problem/data/processed/apc_weather_gtfs.parquet')\n",
    "apcdata = spark.read.load(f)\n",
    "apcdata.createOrReplaceTempView(\"apc\")\n",
    "query = f\"\"\"\n",
    "SELECT {get_str}\n",
    "FROM apc\n",
    "WHERE (transit_date >= '{start_date.date()}') AND (transit_date <= '{end_date.date()}')\n",
    "\"\"\"\n",
    "apcdataafternegdelete = spark.sql(query)\n",
    "apcdataafternegdelete = apcdataafternegdelete.dropna()\n",
    "trips_df = apcdataafternegdelete.toPandas()\n",
    "trips_df = trips_df.groupby('stop_id_original').agg('count').sort_values('trip_id').reset_index()\n",
    "\n",
    "# Merging them toegether\n",
    "merged_df = pd.merge(trips_df, disruptions_counts_df[['START_STOP_ABBR', 'REASON']], left_on='stop_id_original', right_on='START_STOP_ABBR')\n",
    "merged_df['probability'] = merged_df['REASON'] / merged_df['transit_date']\n",
    "merged_df['probability'] = merged_df['probability']/merged_df['probability'].max()\n",
    "merged_df.sort_values('probability').tail(10)\n",
    "\n",
    "all_stop_probabilities = trips_df[['stop_id_original']]\n",
    "all_stop_probabilities = pd.merge(all_stop_probabilities, merged_df[['stop_id_original', 'probability']], on='stop_id_original', how='outer').fillna(0)\n",
    "all_stop_probabilities.sort_values('probability')\n",
    "\n",
    "# fp = os.path.join('results/disruption_probabilities.pkl')\n",
    "# all_stop_probabilities.to_pickle(fp)\n",
    "all_stop_probabilities\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travel times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool, cpu_count\n",
    "# import datetime as dt\n",
    "# import pandas as pd\n",
    "# import random\n",
    "\n",
    "# def get_traveltimes(tdf):\n",
    "#     tdf = tdf.sort_values('stop_sequence')\n",
    "#     # HACK: For null arrival times in the middle.\n",
    "#     tdf['stop_sequence'] = range(1, len(tdf) + 1)\n",
    "#     if len(tdf) <= 2:\n",
    "#         return pd.DataFrame()\n",
    "#     # HACK: This is for correcting the issue that the first stop's arrival_time starts much earlier than the scheduled time\n",
    "\n",
    "#     tdf = tdf.reset_index(drop=True)\n",
    "#     # tdf['scheduled_timestamp'] = (tdf['arrival_time'] - dt.datetime(1970,1,1)).dt.total_seconds()\n",
    "#     tdf['arrival_at_next_stop'] = tdf.arrival_time.shift(-1)\n",
    "#     tdf['time_to_next_stop'] = tdf['arrival_at_next_stop'] - tdf['departure_time']\n",
    "#     tdf.at[0, 'time_to_next_stop'] = (tdf.at[1, 'arrival_time'] - tdf.at[0, 'scheduled_time']).total_seconds()\n",
    "#     tdf = tdf.drop('scheduled_timestamp', axis=1)\n",
    "#     tdf = tdf.fillna(0)\n",
    "#     return tdf\n",
    "    \n",
    "# def applyParallel(dfGrouped, func):\n",
    "#     with Pool(cpu_count()-2) as p:\n",
    "#         ret_list = p.map(func, [group for name, group in dfGrouped])\n",
    "#     return pd.concat(ret_list)\n",
    "\n",
    "# start_date = dt.datetime.strptime('2020-03-21 06:28:00', '%Y-%m-%d %H:%M:%S')\n",
    "# end_date = dt.datetime.strptime('2022-03-22 06:38:00', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# # Read APC data.\n",
    "# f = os.path.join('/home/jptalusan/mta_stationing_problem/data/processed/apc_weather_gtfs_20221216.parquet')\n",
    "# apcdata = spark.read.load(f)\n",
    "# get_columns = ['trip_id', 'transit_date', 'arrival_time', 'departure_time', 'block_abbr', 'scheduled_time', 'vehicle_id',\n",
    "#               'stop_sequence', 'stop_id_original', 'route_id', 'route_direction_name']\n",
    "# get_str = \", \".join([c for c in get_columns])\n",
    "# apcdata.createOrReplaceTempView(\"apc\")\n",
    "# # # filter subset\n",
    "# query = f\"\"\"\n",
    "#        SELECT {get_str}\n",
    "#        FROM apc\n",
    "#        WHERE (transit_date >= '{start_date.date()}') AND (transit_date <= '{end_date.date()}')\n",
    "#        \"\"\"\n",
    "# print(query)\n",
    "# apcdata = spark.sql(query)\n",
    "# apcdata = apcdata.withColumn(\"route_id_direction\", F.concat_ws('_',apcdata.route_id, apcdata.route_direction_name))\n",
    "# apcdata = apcdata.drop('route_id', 'route_direction_name')\n",
    "# baseline_data = apcdata.toPandas()\n",
    "# baseline_data = baseline_data.dropna(subset=['arrival_time', 'departure_time'])\n",
    "\n",
    "# # Compute travel times\n",
    "# out_arr = applyParallel(baseline_data.groupby(['block_abbr', 'route_id_direction', 'transit_date', 'trip_id']), get_traveltimes)\n",
    "# tdf = out_arr.groupby(['route_id_direction', 'block_abbr', 'stop_sequence', 'stop_id_original']).agg({'time_to_next_stop':list})\n",
    "# # fp = os.path.join('results/travel_time_by_scheduled_time.pkl')\n",
    "# # tdf.to_pickle(fp)\n",
    "\n",
    "# random.seed(100)\n",
    "\n",
    "# tdf = tdf.reset_index()\n",
    "# tdf['sampled_travel_time'] = tdf.reset_index()['time_to_next_stop'].apply(lambda x: random.choice(x))\n",
    "# tdf['sampled_travel_time'] = abs(tdf['sampled_travel_time'])\n",
    "# # fp = os.path.join('results/sampled_travel_times.pkl')\n",
    "# # tdf.to_pickle(fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stops_tt_dd_node_dict\n",
    "* Run `pair_tt_dd.py`: this requires a multiprocessing which is why its .py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import pickle\n",
    "\n",
    "# f = os.path.join('/home/jptalusan/mta_stationing_problem/data/processed/apc_weather_gtfs_20221216.parquet')\n",
    "# apcdata = spark.read.load(f)\n",
    "# apcdata.columns\n",
    "# get_columns = ['stop_sequence', 'stop_id_original', 'stop_name', 'map_latitude', 'map_longitude']\n",
    "# get_str = \", \".join([c for c in get_columns])\n",
    "# apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "# # # filter subset\n",
    "# query = f\"\"\"\n",
    "# SELECT {get_str}\n",
    "# FROM apc\n",
    "# LIMIT 1000\n",
    "# \"\"\"\n",
    "# apcdata = spark.sql(query)\n",
    "# apcdata = apcdata.drop_duplicates(['stop_id_original'])\n",
    "# apcdf = apcdata.toPandas()\n",
    "\n",
    "# fp = os.path.join('data', 'shapefiles', \"tncounty\")\n",
    "# gdf_county = gpd.read_file(fp)\n",
    "# gdf_dav = gdf_county[gdf_county[\"NAME\"] == \"Davidson\"]\n",
    "# gdf_dav = gdf_dav.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# G = ox.graph_from_polygon(gdf_dav.geometry.iloc[0], network_type='drive')\n",
    "# G = ox.add_edge_speeds(G)\n",
    "# G = ox.add_edge_travel_times(G)\n",
    "\n",
    "# apcdf['nearest_node'] = ox.nearest_nodes(G, apcdf['map_longitude'], apcdf['map_latitude'])\n",
    "# apcdf['nearest_edge'] = ox.nearest_edges(G, apcdf['map_longitude'], apcdf['map_latitude'])\n",
    "\n",
    "# fp = os.path.join('results', 'stops_node_matching.pkl')\n",
    "# apcdf.to_pickle(fp)\n",
    "# # fp = os.path.join('data', 'davidson_graph.graphml')\n",
    "# # ox.save_graphml(G, fp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Run `pair_tt_dd.py` after cell above..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stops_node_matching_dict.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = '/home/jptalusan/mta_simulator/code/data/stops_node_matching.pkl'\n",
    "# stop_nodes = pd.read_pickle(fp)\n",
    "# stop_nodes = stop_nodes.set_index('stop_id_original')\n",
    "# stop_nodes_dict = stop_nodes.drop(['stop_sequence', \n",
    "#                                    'stop_name', 'map_latitude', \n",
    "#                                    'map_longitude', \n",
    "#                                    'nearest_edge'], axis=1).to_dict('index')\n",
    "# with open('results/stops_node_matching_dict.pkl', 'wb') as handle:\n",
    "#     pickle.dump(stop_nodes_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create trip and vehicle schedules and passenger chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `generate_day_trips.py`\n",
    "This will generate files in:\n",
    "* `data_generation/results/test_data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create config and execute files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SAMPLE CONFIG.json ####\n",
    "\n",
    "# {\n",
    "#   \"starting_date_str\": \"20210607\",\n",
    "#   \"iter_limit\": 200,\n",
    "#   \"pool_thread_count\": 10,\n",
    "#   \"mcts_discount_factor\": 0.99997,\n",
    "#   \"uct_tradeoff\": 8074255,\n",
    "#   \"lookahead_horizon_delta_t\": 3600,\n",
    "#   \"rollout_horizon_delta_t\": 3600,\n",
    "#   \"allowed_computation_time\": 15,\n",
    "#   \"vehicle_count\": \"\",\n",
    "#   \"oracle\": false,\n",
    "#   \"method\": \"MCTS\",\n",
    "#   \"use_intervals\": true,\n",
    "#   \"use_timepoints\": true,\n",
    "#   \"save_metrics\": false,\n",
    "#   \"send_mail\": true,\n",
    "#   \"reallocation\": true,\n",
    "#   \"early_end\": false,\n",
    "#   \"scenario\": \"1B\",\n",
    "#   \"mcts_log_name\": \"20210607_ALL_IT200_nobreak\"\n",
    "# }\n",
    "\n",
    "#### SAMPLE EXECUTOR.SH ####\n",
    "\n",
    "# session=\"20210607\"\n",
    "\n",
    "# # Check if the session exists, discarding output\n",
    "# # We can check $? for the exit status (zero for success, non-zero for failure)\n",
    "# tmux has-session -t $session 2>/dev/null\n",
    "\n",
    "# if [ $? != 0 ]; then\n",
    "#   # Set up your session\n",
    "#   tmux new-session -d -s $session\n",
    "# else\n",
    "#   tmux kill-session -t $session\n",
    "# fi\n",
    "\n",
    "# # ################################### WINDOW 0 ###################################\n",
    "\n",
    "# window=0\n",
    "# tmux rename-window -t $session:$window '1'\n",
    "# tmux send-keys 'conda activate py39' 'C-m'\n",
    "# cfile=\"configs/all_served\"\n",
    "# logName=${cfile#*/}\n",
    "# tmux send-keys 'python run_mcts_no_inject.py -c '$cfile'' C-m\n",
    "# tmux send-keys 'python emailer.py -c '$cfile'' C-m\n",
    "# tmux send-keys 'tmux capture-pane -pJ -S - > 'logs/$logName'.log' C-m\n",
    "\n",
    "# tmux split-window -h\n",
    "\n",
    "# tmux send-keys 'conda activate py39' 'C-m'\n",
    "# cfile=\"configs/all_served_100\"\n",
    "# logName=${cfile#*/}\n",
    "# tmux send-keys 'python run_mcts_no_inject.py -c '$cfile'' C-m\n",
    "# tmux send-keys 'python emailer.py -c '$cfile'' C-m\n",
    "# tmux send-keys 'tmux capture-pane -pJ -S - > 'logs/$logName'.log' C-m\n",
    "# tmux a -t $session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create json configs\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "\n",
    "date_range = pd.date_range('2022-10-09', '2022-10-09', freq='24h')\n",
    "# date_range = pd.date_range('2021-12-15', '2021-12-15', freq='24h')\n",
    "date_range = [dr.strftime('%Y-%m-%d') for dr in date_range]\n",
    "\n",
    "configs_dir = 'results/test_data/configs/'\n",
    "config_names = []\n",
    "for date in date_range:\n",
    "    date = date.replace(\"-\", \"\")\n",
    "    for iters in [100]:\n",
    "        for mean_label, mean in [('P025', 118274608), ('P05', 59137304), ('P1', 29568652), ('P2', 14784326), ('P4', 7392163)]:\n",
    "            mcts_log_name = f\"{date}_{iters}IT_{mean_label}M_10CAP\"\n",
    "            \n",
    "            config = dict()\n",
    "            config[\"starting_date_str\"] = date\n",
    "            config[\"iter_limit\"] = iters\n",
    "            config[\"pool_thread_count\"] = 10\n",
    "            config[\"uct_tradeoff\"] = mean\n",
    "            config[\"mcts_discount_factor\"] = 0.99997\n",
    "            config[\"lookahead_horizon_delta_t\"] = 3600\n",
    "            config[\"rollout_horizon_delta_t\"] = 3600\n",
    "            config[\"allowed_computation_time\"] = 15\n",
    "            config[\"vehicle_count\"] = \"10CAP\"\n",
    "            config[\"oracle\"] = False\n",
    "            config[\"method\"] = \"MCTS\"\n",
    "            config[\"use_intervals\"] = True\n",
    "            config[\"use_timepoints\"] = True\n",
    "            config[\"send_mail\"] = True\n",
    "            config[\"save_metrics\"] = False\n",
    "            config[\"reallocation\"] = True\n",
    "            config[\"scenario\"] = \"1B\"\n",
    "            config[\"mcts_log_name\"] = mcts_log_name\n",
    "            \n",
    "            with open(f'{configs_dir}/mcts_{date}_{iters}IT_{mean_label}M.json', 'w') as fp:\n",
    "                json.dump(config, fp, indent=2)\n",
    "                \n",
    "            config_names.append(mcts_log_name)\n",
    "\n",
    "devices = 1\n",
    "windows = 2\n",
    "\n",
    "date_range = list(np.array_split(date_range, devices))\n",
    "for device in range(devices):\n",
    "    lines = []\n",
    "    d = list(np.array_split(date_range[device], windows))\n",
    "    \n",
    "    lines.append(f\"session=device_{device}_200IT\")\n",
    "    lines.append(\"tmux has-session -t $session 2>/dev/null\")\n",
    "    lines.append(\"if [ $? != 0 ]; then\")\n",
    "    lines.append(\"  tmux new-session -d -s $session\")\n",
    "    lines.append(\"else\")\n",
    "    lines.append(\"  tmux kill-session -t $session\")\n",
    "    lines.append(\"fi\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    lines.append(f\"window=0\")\n",
    "    # lines.append(\"tmux rename-window -t $session:$window '1'\")\n",
    "    \n",
    "    for window in range(windows):\n",
    "        lines.append(\"tmux send-keys 'conda activate py39' 'C-m'\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        for iter in [100]:\n",
    "            for _d in d[window]:\n",
    "                for mean_label, mean in [('P025', 118274608), ('P05', 59137304), ('P1', 29568652), ('P2', 14784326), ('P4', 7392163)]:\n",
    "                    mcts_log_name = f\"mcts_{_d.replace('-', '')}_{iter}IT_{mean_label}M\"\n",
    "                    lines.append(f\"cfile=\\\"configs/{mcts_log_name}\\\"\")\n",
    "                    lines.append(\"logName=${cfile#*/}\")\n",
    "                    lines.append(\"tmux send-keys 'python run_mcts_no_inject.py -c '$cfile'' C-m\")\n",
    "                    lines.append(\"tmux send-keys 'python emailer.py -c '$cfile'' C-m\")\n",
    "                    lines.append(\"tmux send-keys 'tmux capture-pane -pJ -S - > 'logs/$logName'.log' C-m\")\n",
    "                    lines.append(\"\")\n",
    "        \n",
    "        if window < windows - 1:\n",
    "            lines.append(\"tmux split-window -h\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    lines.append(\"tmux select-layout tiled\")\n",
    "    lines.append(\"tmux a -t $session\")\n",
    "    \n",
    "    with open(f'results/test_data/executers/execute_{device}_100IT.sh', 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create json configs\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "\n",
    "date_range = pd.date_range('2022-10-01', '2022-10-31', freq='24h')\n",
    "# date_range = pd.date_range('2021-12-15', '2021-12-15', freq='24h')\n",
    "date_range = [dr.strftime('%Y-%m-%d') for dr in date_range]\n",
    "\n",
    "configs_dir = 'results/test_data/configs/'\n",
    "config_names = []\n",
    "for date in date_range:\n",
    "    date = date.replace(\"-\", \"\")\n",
    "    for iters in [0]:\n",
    "        mcts_log_name = f\"BL_{date}_{iters}IT_10CAP_NOREALLOC\"\n",
    "        config = dict()\n",
    "        config[\"starting_date_str\"] = date\n",
    "        config[\"iter_limit\"] = iters\n",
    "        config[\"pool_thread_count\"] = 10\n",
    "        config[\"uct_tradeoff\"] = 31344050\n",
    "        config[\"mcts_discount_factor\"] = 0.99997\n",
    "        config[\"lookahead_horizon_delta_t\"] = 3600\n",
    "        config[\"rollout_horizon_delta_t\"] = 3600\n",
    "        config[\"allowed_computation_time\"] = 15\n",
    "        config[\"vehicle_count\"] = \"10CAP\"\n",
    "        config[\"oracle\"] = True\n",
    "        config[\"method\"] = \"baseline\"\n",
    "        config[\"use_intervals\"] = True\n",
    "        config[\"use_timepoints\"] = True\n",
    "        config[\"send_mail\"] = True\n",
    "        config[\"save_metrics\"] = False\n",
    "        config[\"reallocation\"] = False\n",
    "        config[\"scenario\"] = \"1B\"\n",
    "        config[\"mcts_log_name\"] = mcts_log_name\n",
    "        \n",
    "        with open(f'{configs_dir}/bl_{date}_{iters}IT.json', 'w') as fp:\n",
    "            json.dump(config, fp, indent=2)\n",
    "            \n",
    "        config_names.append(mcts_log_name)\n",
    "\n",
    "devices = 1\n",
    "windows = 4\n",
    "\n",
    "date_range = list(np.array_split(date_range, devices))\n",
    "for device in range(devices):\n",
    "    lines = []\n",
    "    d = list(np.array_split(date_range[device], windows))\n",
    "    \n",
    "    lines.append(f\"session=device_{device}_200IT\")\n",
    "    lines.append(\"tmux has-session -t $session 2>/dev/null\")\n",
    "    lines.append(\"if [ $? != 0 ]; then\")\n",
    "    lines.append(\"  tmux new-session -d -s $session\")\n",
    "    lines.append(\"else\")\n",
    "    lines.append(\"  tmux kill-session -t $session\")\n",
    "    lines.append(\"fi\")\n",
    "    lines.append(\"\")\n",
    "    \n",
    "    lines.append(f\"window=0\")\n",
    "    # lines.append(\"tmux rename-window -t $session:$window '1'\")\n",
    "    \n",
    "    for window in range(windows):\n",
    "        lines.append(\"tmux send-keys 'conda activate py39' 'C-m'\")\n",
    "        lines.append(\"\")\n",
    "        \n",
    "        for iter in [200]:\n",
    "            for _d in d[window]:\n",
    "                mcts_log_name = f\"bl_{_d.replace('-', '')}_{iter}IT\"\n",
    "                lines.append(f\"cfile=\\\"configs/{mcts_log_name}\\\"\")\n",
    "                lines.append(\"logName=${cfile#*/}\")\n",
    "                lines.append(\"tmux send-keys 'python run_mcts_no_inject.py -c '$cfile'' C-m\")\n",
    "                # lines.append(\"tmux send-keys 'python emailer.py -c '$cfile'' C-m\")\n",
    "                lines.append(\"tmux send-keys 'tmux capture-pane -pJ -S - > 'logs/$logName'.log' C-m\")\n",
    "                lines.append(\"\")\n",
    "        \n",
    "        if window < windows - 1:\n",
    "            lines.append(\"tmux split-window -h\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    lines.append(\"tmux select-layout tiled\")\n",
    "    lines.append(\"tmux a -t $session\")\n",
    "    \n",
    "    with open(f'results/test_data/executers/execute_bl_{device}_0IT.sh', 'w') as f:\n",
    "        for line in lines:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:45:29) \n[GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
