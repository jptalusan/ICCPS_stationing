{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation\n",
    "* Generates data for a desired date based on the available APC data and passed through the model for load prediction.\n",
    "* Will provide a distribution of bins which can be used for stochasticity\n",
    "## Generates the following files:\n",
    "* `trip_plan.json`\n",
    "* `vehicle_plan.json`\n",
    "* `sampled_loads.pkl`\n",
    "* `chains.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import sys\n",
    "import datetime as dt\n",
    "import importlib\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "import IPython\n",
    "from copy import deepcopy\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import swifter\n",
    "pd.set_option('display.max_columns', None)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '80g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '40g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "        .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "        .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apc_data_for_date(filter_date):\n",
    "    print(\"Running this...\")\n",
    "    filepath = '/home/jptalusan/mta_stationing_problem/data/processed/apc_weather_gtfs_20220921.parquet'\n",
    "    apcdata = spark.read.load(filepath)\n",
    "    apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "    plot_date = filter_date.strftime('%Y-%m-%d')\n",
    "    get_columns = ['trip_id', 'transit_date', 'arrival_time', 'scheduled_time',\n",
    "                'block_abbr', 'stop_sequence', 'stop_id_original',\n",
    "                'vehicle_id', 'vehicle_capacity',\n",
    "                'load', \n",
    "                'darksky_temperature', \n",
    "                'darksky_humidity', \n",
    "                'darksky_precipitation_probability', \n",
    "                'route_direction_name', 'route_id', 'overload_id',\n",
    "                'dayofweek',  'year', 'month', 'hour', 'zero_load_at_trip_end',\n",
    "                'sched_hdwy']\n",
    "    get_str = \", \".join([c for c in get_columns])\n",
    "    query = f\"\"\"\n",
    "    SELECT {get_str}\n",
    "    FROM apc\n",
    "    WHERE (transit_date == '{plot_date}')\n",
    "    ORDER BY arrival_time\n",
    "    \"\"\"\n",
    "    apcdata = spark.sql(query)\n",
    "    apcdata = apcdata.withColumn(\"route_id_dir\", F.concat_ws(\"_\", apcdata.route_id, apcdata.route_direction_name))\n",
    "    apcdata = apcdata.withColumn(\"day\", F.dayofmonth(apcdata.arrival_time))\n",
    "    apcdata = apcdata.drop(\"route_direction_name\")\n",
    "    apcdata = apcdata.withColumn(\"load\", F.when(apcdata.load < 0, 0).otherwise(apcdata.load))\n",
    "    apcdata = apcdata.na.fill(value=0,subset=[\"zero_load_at_trip_end\"])\n",
    "    return apcdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(input_df, ohe_encoder, label_encoders, num_scaler, columns, keep_columns=[], target='y_class'):\n",
    "    num_columns = ['darksky_temperature', 'darksky_humidity', 'darksky_precipitation_probability', 'sched_hdwy']\n",
    "    cat_columns = ['month', 'hour', 'day', 'stop_sequence', 'stop_id_original', 'year', 'time_window']\n",
    "    ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end']\n",
    "\n",
    "    # OHE\n",
    "    input_df[ohe_encoder.get_feature_names_out()] = ohe_encoder.transform(input_df[ohe_columns]).toarray()\n",
    "    # input_df = input_df.drop(columns=ohe_columns)\n",
    "\n",
    "    # Label encoder\n",
    "    for cat in cat_columns:\n",
    "        print(cat)\n",
    "        encoder = label_encoders[cat]\n",
    "        input_df[cat] = encoder.transform(input_df[cat])\n",
    "    \n",
    "    # Num scaler\n",
    "    input_df[num_columns] = num_scaler.transform(input_df[num_columns])\n",
    "    input_df['y_class']  = input_df.y_class.astype('int')\n",
    "\n",
    "    if keep_columns:\n",
    "        columns = keep_columns + columns\n",
    "    # Rearrange columns\n",
    "    input_df = input_df[columns]\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def assign_data_to_bins(df, TARGET='load'):\n",
    "    bins = pd.IntervalIndex.from_tuples([(-1, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)])\n",
    "    mycut = pd.cut(df[TARGET].tolist(), bins=bins)\n",
    "    df['y_class'] = mycut.codes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEWINDOW = 15\n",
    "def add_features(df):\n",
    "    df = df[df.arrival_time.notna()]\n",
    "    df = df.fillna(method=\"bfill\")\n",
    "\n",
    "    df['day'] = df[\"arrival_time\"].dt.day\n",
    "    df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "    # Adding extra features\n",
    "    # Holidays\n",
    "    fp = os.path.join('data', 'US Holiday Dates (2004-2021).csv')\n",
    "    holidays_df = pd.read_csv(fp)\n",
    "    holidays_df['Date'] = pd.to_datetime(holidays_df['Date'])\n",
    "    holidays_df['is_holiday'] = True\n",
    "    df = df.merge(holidays_df[['Date', 'is_holiday']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_holiday'] = df['is_holiday'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "        \n",
    "    # School breaks\n",
    "    fp = os.path.join('data', 'School Breaks (2019-2022).pkl')\n",
    "    school_break_df = pd.read_pickle(fp)\n",
    "    school_break_df['is_school_break'] = True\n",
    "    df = df.merge(school_break_df[['Date', 'is_school_break']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_school_break'] = df['is_school_break'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "\n",
    "    df['minute'] = df['arrival_time'].dt.minute\n",
    "    df['minuteByWindow'] = df['minute'] // TIMEWINDOW\n",
    "    df['temp'] = df['minuteByWindow'] + (df['hour'] * 60 / TIMEWINDOW)\n",
    "    df['time_window'] = np.floor(df['temp']).astype('int')\n",
    "    df = df.drop(columns=['minute', 'minuteByWindow', 'temp'])\n",
    "\n",
    "    # HACK\n",
    "    # df = df[df['hour'] != 3]\n",
    "    # df = df[df['stop_sequence'] != 0]\n",
    "\n",
    "    df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "    df = assign_data_to_bins(df, TARGET='load')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_simple_lstm_generator(num_features, num_classes, learning_rate=1e-4):\n",
    "    # define model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "\n",
    "    input_shape = (None, None, num_features)\n",
    "    model.build(input_shape)\n",
    "    return model\n",
    "\n",
    "def generate_simple_lstm_predictions(input_df, model, past, future):\n",
    "    past_df = input_df[0:past]\n",
    "    future_df = input_df[past:]\n",
    "    predictions = []\n",
    "    pred_probs = []\n",
    "    if future == None:\n",
    "        future = len(future_df)\n",
    "    for f in range(future):\n",
    "        pred = model.predict(past_df.to_numpy().reshape(1, *past_df.shape))\n",
    "        pred_probs.append(pred)\n",
    "        y_pred = np.argmax(pred)\n",
    "        predictions.append(y_pred)\n",
    "        \n",
    "        # Add information from future\n",
    "        last_row = future_df.iloc[[0]]\n",
    "        last_row['y_class'] = y_pred\n",
    "        past_df = pd.concat([past_df[1:], last_row])\n",
    "        \n",
    "        # Move future to remove used row\n",
    "        future_df = future_df[1:]\n",
    "    return predictions, pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overload_regular_bus_trips(regular, overload):\n",
    "    m = regular.merge(overload, how='left', on=['trip_id', 'transit_date', 'scheduled_time', 'block_abbr', 'stop_sequence', 'stop_id_original', 'route_id_dir', 'route_id'])\n",
    "    \n",
    "    m['arrival_time'] = np.max(m[['arrival_time_x', 'arrival_time_y']], axis=1)\n",
    "    \n",
    "    m['zero_load_at_trip_end'] = m['zero_load_at_trip_end_x']\n",
    "    \n",
    "    m.loc[~m['arrival_time_x'].isnull(), \"load\"] = m['load_x']\n",
    "    # m.loc[~m['arrival_time_x'].isnull(), \"ons\"] = m['ons_x']\n",
    "    # m.loc[~m['arrival_time_x'].isnull(), \"offs\"] = m['offs_x']\n",
    "    \n",
    "    m.loc[~m['arrival_time_y'].isnull(), \"load\"] = m['load_y']\n",
    "    # m.loc[~m['arrival_time_y'].isnull(), \"ons\"] = m['ons_y']\n",
    "    # m.loc[~m['arrival_time_y'].isnull(), \"offs\"] = m['offs_y']\n",
    "    \n",
    "    m['vehicle_id'] = m['vehicle_id_x']\n",
    "    m['vehicle_capacity'] = m['vehicle_capacity_x']\n",
    "    m['overload_id'] = m['overload_id_x']\n",
    "    m = m[m.columns.drop(list(m.filter(regex='_x')))]\n",
    "    m = m[m.columns.drop(list(m.filter(regex='_y')))]\n",
    "    # m = m[regular.columns]\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "latest = tf.train.latest_checkpoint('models/no_speed')\n",
    "columns = joblib.load('models/LL_X_columns.joblib')\n",
    "label_encoders = joblib.load('models/LL_Label_encoders.joblib')\n",
    "ohe_encoder = joblib.load('models/LL_OHE_encoder.joblib')\n",
    "num_scaler = joblib.load('models/LL_Num_scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '2021-03-05'\n",
    "start_time = '08:00:00'\n",
    "end_time = '12:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running this...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 02:41:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2021-10-18, 2021-11-23, 2021-12-15, 2022-01-\n",
    "date_to_predict = dt.datetime.strptime(DATE, '%Y-%m-%d')\n",
    "apcdata = get_apc_data_for_date(date_to_predict)\n",
    "df = apcdata.toPandas()\n",
    "\n",
    "# HACK\n",
    "# a = df.query(\"trip_id == '233300' and vehicle_id == '722'\").sort_values('stop_sequence')\n",
    "# b = df.query(\"trip_id == '233300' and vehicle_id == '1830'\").sort_values('stop_sequence')\n",
    "# m1 = merge_overload_regular_bus_trips(a, b)\n",
    "\n",
    "# a = df.query(\"trip_id == '259635' and vehicle_id == '2019'\").sort_values('stop_sequence')\n",
    "# b = df.query(\"trip_id == '259635' and vehicle_id == '1914'\").sort_values('stop_sequence')\n",
    "# m2 = merge_overload_regular_bus_trips(a, b)\n",
    "\n",
    "df = df.query(\"overload_id == 0\")\n",
    "# overload_trips = df.query(\"overload_id > 0\").trip_id.unique()\n",
    "# df = df[~df['trip_id'].isin(overload_trips)]\n",
    "# df = pd.concat([tdf, m1])\n",
    "df = df.dropna(subset=['arrival_time'])\n",
    "# df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# HACK\n",
    "# df = df.query(\"route_id != 95\")\n",
    "# df = df[~df['stop_id_original'].isin(['PEARL', 'JOHASHEN', 'ROS10AEN'])]\n",
    "\n",
    "df = add_features(df)\n",
    "raw_df = deepcopy(df)\n",
    "\n",
    "# HACK\n",
    "# df.loc[df['time_window'].isin([6, 7, 8]), 'time_window'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month\n",
      "hour\n",
      "day\n",
      "stop_sequence\n",
      "stop_id_original\n",
      "year\n",
      "time_window\n"
     ]
    }
   ],
   "source": [
    "input_df = prepare_input_data(df, ohe_encoder, label_encoders, num_scaler, columns, target='y_class')\n",
    "ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end']\n",
    "input_df = input_df.drop(columns=ohe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 20:05:23.424372: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 20:05:23.878143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11402 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:0b:00.0, compute capability: 6.1\n",
      "  0%|          | 0/1005 [00:00<?, ?it/s]2022-11-30 20:05:25.237817: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8401\n",
      "100%|██████████| 1005/1005 [14:42<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "tf.keras.backend.clear_session()\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "FUTURE = None\n",
    "PAST = 5\n",
    "\n",
    "NUM_TRIPS = None\n",
    "if NUM_TRIPS == None:\n",
    "    rand_trips = df.trip_id.unique().tolist()\n",
    "else:\n",
    "    rand_trips = random.sample(df.trip_id.unique().tolist(), NUM_TRIPS)\n",
    "\n",
    "model = setup_simple_lstm_generator(input_df.shape[1], NUM_CLASSES)\n",
    "model.load_weights(latest)\n",
    "\n",
    "trip_res = []\n",
    "load_arr = []\n",
    "for trip_id in tqdm(rand_trips):\n",
    "    _df = df.query(\"trip_id == @trip_id\")\n",
    "    try:\n",
    "        _input_df = input_df.loc[_df.index]\n",
    "        _y_pred, y_pred_probs = generate_simple_lstm_predictions(_input_df, model, PAST, FUTURE)\n",
    "        \n",
    "        # Introducing stochasticity\n",
    "        y_pred = [np.random.choice(len(ypp.flatten()), size=1, p=ypp.flatten())[0] for ypp in y_pred_probs]\n",
    "        loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in y_pred]\n",
    "        \n",
    "        _raw_df = raw_df.loc[_df.index]\n",
    "        y_true = _raw_df[0:PAST]['load'].tolist()\n",
    "        a = y_true + loads\n",
    "        _raw_df['sampled_loads'] = a\n",
    "        \n",
    "        y_true_classes = _raw_df[0:PAST]['y_class'].tolist()\n",
    "        _raw_df['y_pred_classes'] = y_true_classes + y_pred\n",
    "        _raw_df['y_pred_probs'] = [[-1] * NUM_CLASSES]*len(y_true_classes) + [ypp[0] for ypp in y_pred_probs]\n",
    "        \n",
    "        trip_res.append(_raw_df)\n",
    "    except:\n",
    "        print(f\"FAILED:{trip_id}\")\n",
    "        continue\n",
    "\n",
    "trip_res = pd.concat(trip_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_columns = ['trip_id', 'transit_date', 'arrival_time', 'scheduled_time', 'block_abbr', \n",
    "            'stop_sequence', 'stop_id_original', 'route_id_dir', 'zero_load_at_trip_end', \n",
    "            'y_pred_classes', 'y_pred_probs', 'sampled_loads', 'vehicle_id', 'vehicle_capacity']\n",
    "_trip_res = trip_res[_columns]\n",
    "\n",
    "# fp = 'results/sampled_loads.pkl'\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "_trip_res.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching with GTFS time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     trip_df\u001b[39m.\u001b[39mloc[trip_df\u001b[39m.\u001b[39mindex[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mtimepoint\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     trip_res_arr\u001b[39m.\u001b[39mappend(trip_df)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m trip_res_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(trip_res_arr)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# trip_res_df.to_pickle(fp)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m trip_res_df\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/reshape/concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    156\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m    along the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    348\u001b[0m         objs,\n\u001b[1;32m    349\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    350\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    351\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    352\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    353\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    354\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    355\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    356\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    357\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    360\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/reshape/concat.py:404\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    401\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[1;32m    403\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# # fp = 'results/sampled_loads.pkl'\n",
    "# # trip_res_df = pd.read_pickle(fp)\n",
    "# # trip_res_df['trip_id'] = trip_res_df['trip_id'].astype('int')\n",
    "# trip_res_df = _trip_res\n",
    "\n",
    "# trip_res_df = pd.merge(trip_res_df, raw_df[['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original']], \n",
    "#                        left_on=['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original'], \n",
    "#                        right_on=['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original'], how='left')\n",
    "# trip_res_df['trip_id'] = trip_res_df['trip_id'].astype('int')\n",
    "\n",
    "# # print(trip_res_df.shape)\n",
    "# stop_times_fp = 'data/GTFS/OCT2021/stop_times.txt'\n",
    "# stop_times_df = pd.read_csv(stop_times_fp)\n",
    "# # stop_times_df.query(\"trip_id == 264733\")\n",
    "\n",
    "# trip_res_df = pd.merge(trip_res_df, stop_times_df[['trip_id', 'stop_id', 'timepoint']], left_on=['trip_id', 'stop_id_original'], right_on=['trip_id', 'stop_id'])\n",
    "# trip_res_df.query(\"trip_id == 264733\")\n",
    "# trip_res_df = trip_res_df.drop_duplicates(subset=['trip_id', 'stop_id_original', 'arrival_time', 'scheduled_time'])\n",
    "\n",
    "# trip_res_arr = []\n",
    "# for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "#     trip_df.loc[trip_df.index[-1], 'timepoint']= 1.0\n",
    "#     trip_res_arr.append(trip_df)\n",
    "    \n",
    "# trip_res_df = pd.concat(trip_res_arr)\n",
    "\n",
    "# # fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# # trip_res_df.to_pickle(fp)\n",
    "# trip_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vehicle assignments here...\n",
    "* Trying to limit to a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2021-10-18': ['121',\n",
       "  '124',\n",
       "  '140',\n",
       "  '1824',\n",
       "  '1830',\n",
       "  '1900',\n",
       "  '1920',\n",
       "  '2013',\n",
       "  '2015',\n",
       "  '715',\n",
       "  '720'],\n",
       " '2021-11-23': ['1812',\n",
       "  '1817',\n",
       "  '1820',\n",
       "  '1825',\n",
       "  '1827',\n",
       "  '1900',\n",
       "  '1906',\n",
       "  '1907',\n",
       "  '2006',\n",
       "  '2009'],\n",
       " '2021-12-15': ['121',\n",
       "  '134',\n",
       "  '1800',\n",
       "  '1804',\n",
       "  '1812',\n",
       "  '1821',\n",
       "  '1826',\n",
       "  '1830',\n",
       "  '1904',\n",
       "  '1908'],\n",
       " '2022-01-27': ['129',\n",
       "  '137',\n",
       "  '1811',\n",
       "  '1815',\n",
       "  '1819',\n",
       "  '1824',\n",
       "  '1906',\n",
       "  '1920',\n",
       "  '2003',\n",
       "  '2004'],\n",
       " '2022-02-25': ['127',\n",
       "  '130',\n",
       "  '139',\n",
       "  '141',\n",
       "  '1814',\n",
       "  '1820',\n",
       "  '1823',\n",
       "  '1901',\n",
       "  '1913',\n",
       "  '1916']}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    '2021-10-18':['121', '124', '140', '1824', '1830', '1900', '1920', '2013', '2015', '715', '720'],\n",
    "    '2021-11-23':['1812', '1817', '1820', '1825', '1827', '1900', '1906', '1907', '2006', '2009'],\n",
    "    '2021-12-15':['121', '134', '1800', '1804', '1812', '1821', '1826', '1830', '1904', '1908'],\n",
    "    '2022-01-27':['129', '137', '1811', '1815', '1819', '1824', '1906', '1920', '2003', '2004'],\n",
    "    '2022-02-25':['127', '130', '139', '141', '1814', '1820', '1823', '1901', '1913', '1916']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '2022-02-25'\n",
    "vehicle_list = ['127', '130', '139', '141', '1814', '1820', '1823', '1901', '1913', '1916']\n",
    "start_time = '08:00:00'\n",
    "end_time = '12:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7feb61900c40>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res_df = pd.read_pickle(fp)\n",
    "trip_res_df['hour'] = trip_res_df.scheduled_time.dt.hour\n",
    "trip_res_df['count'] = 1\n",
    "ax = trip_res_df.groupby('trip_id').agg({\"hour\":\"first\", \"count\":\"count\"}).groupby(\"hour\").count().plot(kind='line')\n",
    "ax.axvline(x=6, ymin=0, ymax=100, ls='--')\n",
    "ax.axvline(x=10, ymin=0, ymax=100, ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159,)\n",
      "(10,)\n",
      "(33,)\n",
      "(9,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>transit_date</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>scheduled_time</th>\n",
       "      <th>block_abbr</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>stop_id_original</th>\n",
       "      <th>route_id_dir</th>\n",
       "      <th>zero_load_at_trip_end</th>\n",
       "      <th>y_pred_classes</th>\n",
       "      <th>y_pred_probs</th>\n",
       "      <th>sampled_loads</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>vehicle_capacity</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>timepoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>680</th>\n",
       "      <td>261228</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022-02-25 08:14:49</td>\n",
       "      <td>2022-02-25 08:15:00</td>\n",
       "      <td>301</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC5_5</td>\n",
       "      <td>3_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1916</td>\n",
       "      <td>40.0</td>\n",
       "      <td>MCC5_5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>261228</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022-02-25 08:20:52</td>\n",
       "      <td>2022-02-25 08:16:28</td>\n",
       "      <td>301</td>\n",
       "      <td>2</td>\n",
       "      <td>6AVDEASN</td>\n",
       "      <td>3_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1916</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6AVDEASN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>261228</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022-02-25 08:22:52</td>\n",
       "      <td>2022-02-25 08:17:25</td>\n",
       "      <td>301</td>\n",
       "      <td>3</td>\n",
       "      <td>6AVCHUSN</td>\n",
       "      <td>3_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1916</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6AVCHUSN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>683</th>\n",
       "      <td>261228</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022-02-25 08:27:32</td>\n",
       "      <td>2022-02-25 08:19:30</td>\n",
       "      <td>301</td>\n",
       "      <td>4</td>\n",
       "      <td>8ABROSN</td>\n",
       "      <td>3_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1916</td>\n",
       "      <td>40.0</td>\n",
       "      <td>8ABROSN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>261228</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022-02-25 08:27:52</td>\n",
       "      <td>2022-02-25 08:20:07</td>\n",
       "      <td>301</td>\n",
       "      <td>5</td>\n",
       "      <td>BRO9AWF</td>\n",
       "      <td>3_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1916</td>\n",
       "      <td>40.0</td>\n",
       "      <td>BRO9AWF</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43589</th>\n",
       "      <td>264427</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022-02-25 09:10:38</td>\n",
       "      <td>2022-02-25 09:08:52</td>\n",
       "      <td>7600</td>\n",
       "      <td>38</td>\n",
       "      <td>DELCUMSN</td>\n",
       "      <td>76_LOOP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.14813484, 0.7656111, 0.0855168, 0.000717444...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1814</td>\n",
       "      <td>40.0</td>\n",
       "      <td>DELCUMSN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43590</th>\n",
       "      <td>264427</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022-02-25 09:11:18</td>\n",
       "      <td>2022-02-25 09:10:21</td>\n",
       "      <td>7600</td>\n",
       "      <td>39</td>\n",
       "      <td>DELSTRSN</td>\n",
       "      <td>76_LOOP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.08654219, 0.89691246, 0.016542338, 3.025093...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1814</td>\n",
       "      <td>40.0</td>\n",
       "      <td>DELSTRSN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43591</th>\n",
       "      <td>264427</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022-02-25 09:12:06</td>\n",
       "      <td>2022-02-25 09:11:25</td>\n",
       "      <td>7600</td>\n",
       "      <td>40</td>\n",
       "      <td>OLD4AVWF</td>\n",
       "      <td>76_LOOP</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.10716917, 0.869284, 0.023528388, 1.8290524e...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1814</td>\n",
       "      <td>40.0</td>\n",
       "      <td>OLD4AVWF</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43592</th>\n",
       "      <td>264427</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022-02-25 09:12:46</td>\n",
       "      <td>2022-02-25 09:12:38</td>\n",
       "      <td>7600</td>\n",
       "      <td>41</td>\n",
       "      <td>OLD3AVWM</td>\n",
       "      <td>76_LOOP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5933823, 0.38343245, 0.023183594, 1.6887274...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1814</td>\n",
       "      <td>40.0</td>\n",
       "      <td>OLD3AVWM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43593</th>\n",
       "      <td>264427</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>2022-02-25 09:13:52</td>\n",
       "      <td>2022-02-25 09:14:13</td>\n",
       "      <td>7600</td>\n",
       "      <td>42</td>\n",
       "      <td>GALMAPSN</td>\n",
       "      <td>76_LOOP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.8235957, 0.14826211, 0.028140657, 1.5388316...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1814</td>\n",
       "      <td>40.0</td>\n",
       "      <td>GALMAPSN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1190 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       trip_id transit_date        arrival_time      scheduled_time  \\\n",
       "680     261228   2022-02-25 2022-02-25 08:14:49 2022-02-25 08:15:00   \n",
       "681     261228   2022-02-25 2022-02-25 08:20:52 2022-02-25 08:16:28   \n",
       "682     261228   2022-02-25 2022-02-25 08:22:52 2022-02-25 08:17:25   \n",
       "683     261228   2022-02-25 2022-02-25 08:27:32 2022-02-25 08:19:30   \n",
       "684     261228   2022-02-25 2022-02-25 08:27:52 2022-02-25 08:20:07   \n",
       "...        ...          ...                 ...                 ...   \n",
       "43589   264427   2022-02-25 2022-02-25 09:10:38 2022-02-25 09:08:52   \n",
       "43590   264427   2022-02-25 2022-02-25 09:11:18 2022-02-25 09:10:21   \n",
       "43591   264427   2022-02-25 2022-02-25 09:12:06 2022-02-25 09:11:25   \n",
       "43592   264427   2022-02-25 2022-02-25 09:12:46 2022-02-25 09:12:38   \n",
       "43593   264427   2022-02-25 2022-02-25 09:13:52 2022-02-25 09:14:13   \n",
       "\n",
       "       block_abbr  stop_sequence stop_id_original     route_id_dir  \\\n",
       "680           301              1           MCC5_5  3_FROM DOWNTOWN   \n",
       "681           301              2         6AVDEASN  3_FROM DOWNTOWN   \n",
       "682           301              3         6AVCHUSN  3_FROM DOWNTOWN   \n",
       "683           301              4          8ABROSN  3_FROM DOWNTOWN   \n",
       "684           301              5          BRO9AWF  3_FROM DOWNTOWN   \n",
       "...           ...            ...              ...              ...   \n",
       "43589        7600             38         DELCUMSN          76_LOOP   \n",
       "43590        7600             39         DELSTRSN          76_LOOP   \n",
       "43591        7600             40         OLD4AVWF          76_LOOP   \n",
       "43592        7600             41         OLD3AVWM          76_LOOP   \n",
       "43593        7600             42         GALMAPSN          76_LOOP   \n",
       "\n",
       "       zero_load_at_trip_end  y_pred_classes  \\\n",
       "680                        0               0   \n",
       "681                        0               0   \n",
       "682                        0               0   \n",
       "683                        0               1   \n",
       "684                        0               1   \n",
       "...                      ...             ...   \n",
       "43589                      0               1   \n",
       "43590                      0               1   \n",
       "43591                      0               1   \n",
       "43592                      0               0   \n",
       "43593                      0               0   \n",
       "\n",
       "                                            y_pred_probs  sampled_loads  \\\n",
       "680                                 [-1, -1, -1, -1, -1]            5.0   \n",
       "681                                 [-1, -1, -1, -1, -1]            5.0   \n",
       "682                                 [-1, -1, -1, -1, -1]            6.0   \n",
       "683                                 [-1, -1, -1, -1, -1]            9.0   \n",
       "684                                 [-1, -1, -1, -1, -1]            9.0   \n",
       "...                                                  ...            ...   \n",
       "43589  [0.14813484, 0.7656111, 0.0855168, 0.000717444...            7.0   \n",
       "43590  [0.08654219, 0.89691246, 0.016542338, 3.025093...            8.0   \n",
       "43591  [0.10716917, 0.869284, 0.023528388, 1.8290524e...            9.0   \n",
       "43592  [0.5933823, 0.38343245, 0.023183594, 1.6887274...            5.0   \n",
       "43593  [0.8235957, 0.14826211, 0.028140657, 1.5388316...            1.0   \n",
       "\n",
       "      vehicle_id  vehicle_capacity   stop_id  timepoint  \n",
       "680         1916              40.0    MCC5_5          1  \n",
       "681         1916              40.0  6AVDEASN          0  \n",
       "682         1916              40.0  6AVCHUSN          0  \n",
       "683         1916              40.0   8ABROSN          0  \n",
       "684         1916              40.0   BRO9AWF          0  \n",
       "...          ...               ...       ...        ...  \n",
       "43589       1814              40.0  DELCUMSN          0  \n",
       "43590       1814              40.0  DELSTRSN          0  \n",
       "43591       1814              40.0  OLD4AVWF          0  \n",
       "43592       1814              40.0  OLD3AVWM          0  \n",
       "43593       1814              40.0  GALMAPSN          1  \n",
       "\n",
       "[1190 rows x 16 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# Vehicle assignments\n",
    "# Each vehicle config is a dict: {vehicle_capacity, blocks}\n",
    "DEFAULT_CAPACITY = 40.0\n",
    "overall_vehicle_plan = {}\n",
    "\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res_df = pd.read_pickle(fp)\n",
    "trip_res_df = trip_res_df[trip_res_df['vehicle_id'].isin(vehicle_list)]\n",
    "print(trip_res_df.trip_id.unique().shape)\n",
    "print(trip_res_df.vehicle_id.unique().shape)\n",
    "\n",
    "start_datetime = dt.datetime.strptime(f\"{DATE} {start_time}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "end_datetime = dt.datetime.strptime(f\"{DATE} {end_time}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "arr = []\n",
    "for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "    if (trip_df.scheduled_time.min() >= start_datetime) and (trip_df.scheduled_time.max() <= end_datetime):\n",
    "        arr.append(trip_df)\n",
    "\n",
    "trip_res_df = pd.concat(arr)\n",
    "print(trip_res_df.trip_id.unique().shape)\n",
    "print(trip_res_df.vehicle_id.unique().shape)\n",
    "trip_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: run again with vehicle_capacity (above)\n",
    "for vehicle_id, vehicle_df in trip_res_df.groupby('vehicle_id'):\n",
    "    vehicle_df = vehicle_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    vehicle_capacity = vehicle_df.iloc[0].vehicle_capacity\n",
    "    # vehicle_capacity = DEFAULT_CAPACITY\n",
    "    if np.isnan(vehicle_capacity):\n",
    "        vehicle_capacity = DEFAULT_CAPACITY\n",
    "    # TODO: This is not the baseline behavior\n",
    "    starting_depot = 'MCC5_1'\n",
    "    service_type = 'regular'\n",
    "    blocks = [block for block in vehicle_df.block_abbr.unique().tolist()]\n",
    "    trips = []\n",
    "    for block in blocks:\n",
    "        block_df = vehicle_df.query(\"block_abbr == @block\")\n",
    "        for trip in block_df.trip_id.unique().tolist():\n",
    "            trips.append((str(block), str(trip)))\n",
    "    overall_vehicle_plan[vehicle_id] = {'vehicle_capacity': vehicle_capacity, 'trips': trips, 'starting_depot': starting_depot, 'service_type': service_type}\n",
    "    \n",
    "len(overall_vehicle_plan)\n",
    "\n",
    "# Number of overload buses\n",
    "#   \"42\": {\n",
    "#     \"service_type\": \"overload\",\n",
    "#     \"starting_depot\": \"MCC5_1\",\n",
    "#     \"trips\": [\n",
    "#     ],\n",
    "#     \"vehicle_capacity\": 55.0\n",
    "#   }\n",
    "OVERLOAD_BUSES = 5\n",
    "for vehicle_id in range(41, 41 + OVERLOAD_BUSES):\n",
    "    overall_vehicle_plan[str(vehicle_id)] = {'vehicle_capacity': 55.0, 'trips': [], \"starting_depot\": \"MCC5_1\", 'service_type': \"overload\"}\n",
    "    \n",
    "with open(f'results/vehicle_plan_{DATE.replace(\"-\", \"\")}_10_limited.json', 'w') as fp:\n",
    "    json.dump(overall_vehicle_plan, fp, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Trip plan (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res_df = pd.read_pickle(fp)\n",
    "\n",
    "# Create a dict of {[block: {trip_ids:[]}, 'block'....]}\n",
    "# trip_id dict = {'route_id', route_direction_name', 'stop_id':[], 'schedule_time':[]}\n",
    "# Use block as grouper in baseline\n",
    "overall_block_plan = {}\n",
    "for block_abbr, block_df in trip_res_df.groupby('block_abbr'):\n",
    "    block_df = block_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    trip_ids = block_df.trip_id.unique().tolist()\n",
    "    start_time = block_df[block_df['trip_id'] == trip_ids[0]].iloc[0]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    end_time = block_df[block_df['trip_id'] == trip_ids[-1]].iloc[-1]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    overall_block_plan[block_abbr] = {'trip_ids': trip_ids,\n",
    "                                      'start_time': start_time,\n",
    "                                      'end_time': end_time}\n",
    "\n",
    "overall_trip_plan = {}\n",
    "for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "    trip_df = trip_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    route_id_dir = trip_df.iloc[0].route_id_dir\n",
    "    route_id = int(route_id_dir.split(\"_\")[0])\n",
    "    route_direction = route_id_dir.split(\"_\")[1]\n",
    "    zero_load_at_trip_end = trip_df.iloc[-1].zero_load_at_trip_end.tolist()\n",
    "    scheduled_time = trip_df.scheduled_time.dt.strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "    stop_sequence = trip_df.stop_sequence.tolist()\n",
    "    stop_sequence = list(range(0, len(stop_sequence)))\n",
    "    # stop_sequence = [ss - 1 for ss in stop_sequence]\n",
    "    stop_id_original = trip_df.stop_id_original.tolist()\n",
    "    \n",
    "    overall_trip_plan[trip_id] = {'route_id': route_id, \n",
    "                                  'route_direction': route_direction, \n",
    "                                  'scheduled_time': scheduled_time, \n",
    "                                  'stop_sequence': stop_sequence, \n",
    "                                  'stop_id_original': stop_id_original,\n",
    "                                  'zero_load_at_trip_end':zero_load_at_trip_end,\n",
    "                                  'last_stop_sequence': stop_sequence[-1],\n",
    "                                  'last_stop_id': stop_id_original[-1]}\n",
    "\n",
    "len(overall_trip_plan), len(overall_block_plan)\n",
    "\n",
    "with open(f'results/trip_plan_{DATE.replace(\"-\", \"\")}_10_limited.json', 'w') as fp:\n",
    "    json.dump(overall_trip_plan, fp, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trip_res_df.query(\"trip_id == 259274\").head())\n",
    "print(trip_res_df.query(\"trip_id == 259274\").shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.query(\"trip_id == '243423'\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_id_dir = \n",
    "# trip_res.query(\"route_id_dir == @route_id_dir and block_abbr == @block and stop_id_original == @stop_id_original[@i] and scheduled_time == @scheduled_time[@i]\").iloc[0]['sampled_loads']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ons and offs from sampled loads\n",
    "* Needs the trip_res generated above\n",
    " ```\n",
    " fp = 'results/sampled_loads.pkl'\n",
    " trip_res.to_pickle(fp)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "    \n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "sampled_ons_offs = []\n",
    "for trip_id, trip_id_df in tqdm(trip_res.groupby(['transit_date', 'trip_id'])):\n",
    "    tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "    tdf['ons'] = 0\n",
    "    tdf['offs'] = 0\n",
    "    tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "    \n",
    "    # Intermediate stops\n",
    "    tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # first and last stops\n",
    "    tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "    tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "    sampled_ons_offs.append(tdf)\n",
    "    \n",
    "sampled_ons_offs = pd.concat(sampled_ons_offs)\n",
    "sampled_ons_offs = sampled_ons_offs.drop('next_load', axis=1)\n",
    "\n",
    "# fp = f'results/sampled_ons_offs_{DATE.replace(\"-\", \"\")}.pkl'\n",
    "# sampled_ons_offs.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a single event chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in trip_res.y_pred_classes]\n",
    "trip_res['sampled_loads'] = loads\n",
    "\n",
    "sampled_ons_offs = []\n",
    "for trip_id, trip_id_df in tqdm(trip_res.groupby(['transit_date', 'trip_id'])):\n",
    "    tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "    tdf['stop_sequence'] = list(range(1, len(tdf) + 1))\n",
    "    tdf['ons'] = 0\n",
    "    tdf['offs'] = 0\n",
    "    tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "    \n",
    "    # Intermediate stops\n",
    "    tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # first and last stops\n",
    "    tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "    tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "    sampled_ons_offs.append(tdf)\n",
    "    \n",
    "df = pd.concat(sampled_ons_offs)\n",
    "df = df.drop('next_load', axis=1)\n",
    "\n",
    "display(df)\n",
    "df['key_pair'] = list(zip(df.route_id_dir, \n",
    "                          df.block_abbr,\n",
    "                          df.stop_sequence,\n",
    "                          df.stop_id_original, \n",
    "                          df.scheduled_time))\n",
    "df = df.set_index('key_pair')\n",
    "drop_cols = ['trip_id', 'route_id_dir', 'block_abbr', 'stop_id_original', 'stop_id', 'scheduled_time', \n",
    "                'transit_date', 'arrival_time', 'zero_load_at_trip_end', 'y_pred_classes', 'y_pred_probs',\n",
    "                'vehicle_capacity', 'vehicle_id', 'stop_sequence']\n",
    "drop_cols = [dc for dc in drop_cols if dc in df.columns]\n",
    "df = df.drop(drop_cols, axis=1)\n",
    "sampled_ons_offs_dict = df.to_dict('index')\n",
    "\n",
    "import pickle \n",
    "\n",
    "# with open(f'results/sampled_ons_offs_dict_{DATE.replace(\"-\", \"\")}.pkl', 'wb') as handle:\n",
    "#     pickle.dump(sampled_ons_offs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[('23_FROM DOWNTOWN', 2310, 'DWMRT', pd.Timestamp('2021-08-23 05:41:00'))]\n",
    "df.query(\"route_id_dir == '23_FROM DOWNTOWN' and block_abbr == 2310 and stop_id_original == 'DWMRT' and scheduled_time == '2021-08-23 05:41:00'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = list(sampled_ons_offs_dict.keys())[0]\n",
    "print(key)\n",
    "sampled_ons_offs_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.query(\"route_id_dir == '7_TO DOWNTOWN' and block_abbr == 5692 and stop_sequence == 20 and stop_id_original == 'MCC5_9'\")\n",
    "# df.query(\"route_id_dir == '7_TO DOWNTOWN' and block_abbr == 5692\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating multiple event chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "\n",
    "CHAINS = 5\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "for chain in tqdm(range(CHAINS)):\n",
    "    loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in trip_res.y_pred_classes]\n",
    "    trip_res['sampled_loads'] = loads\n",
    "\n",
    "    sampled_ons_offs = []\n",
    "    for trip_id, trip_id_df in trip_res.groupby(['transit_date', 'trip_id']):\n",
    "        tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "        tdf['stop_sequence'] = list(range(1, len(tdf) + 1))\n",
    "        tdf['ons'] = 0\n",
    "        tdf['offs'] = 0\n",
    "        tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "        \n",
    "        # Intermediate stops\n",
    "        tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "        \n",
    "        # first and last stops\n",
    "        tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "        tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "        sampled_ons_offs.append(tdf)\n",
    "        \n",
    "    df = pd.concat(sampled_ons_offs)\n",
    "    df['key_pair'] = list(zip(df.route_id_dir, \n",
    "                            df.block_abbr,\n",
    "                            df.stop_sequence,\n",
    "                            df.stop_id_original, \n",
    "                            df.scheduled_time))\n",
    "    df = df.set_index('key_pair')\n",
    "    drop_cols = ['trip_id', 'route_id_dir', 'block_abbr', 'stop_id_original', 'stop_id', 'scheduled_time', \n",
    "                 'transit_date', 'arrival_time', 'zero_load_at_trip_end', 'y_pred_classes', 'y_pred_probs',\n",
    "                 'vehicle_capacity', 'vehicle_id', 'stop_sequence']\n",
    "    drop_cols = [dc for dc in drop_cols if dc in df.columns]\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    sampled_ons_offs_dict = df.to_dict('index')\n",
    "\n",
    "    with open(f'results/chains/ons_offs_dict_chain_{DATE.replace(\"-\",\"\")}_{chain}.pkl', 'wb') as handle:\n",
    "        pickle.dump(sampled_ons_offs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create timepoint dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times_df.iloc[0].arrival_time[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(x):\n",
    "    if x[0:2] == '24':\n",
    "        return '00'+x[2:]\n",
    "    if x[0:2] == '25':\n",
    "        return '01'+x[2:]\n",
    "    return x\n",
    "    \n",
    "stop_times_fp = 'data/GTFS/OCT2021/stop_times.txt'\n",
    "stop_times_df = pd.read_csv(stop_times_fp)\n",
    "# stop_times_df.query(\"trip_id == 264733\")\n",
    "stop_times_df['date'] = DATE\n",
    "stop_times_df['arrival_time'] = stop_times_df['arrival_time'].apply(lambda x: fix_time(x))\n",
    "stop_times_df['scheduled_time'] = pd.to_datetime(stop_times_df['date'] + ' ' + stop_times_df['arrival_time'])\n",
    "\n",
    "stop_times_df['key_pair'] = list(zip(stop_times_df.trip_id, stop_times_df.stop_id, stop_times_df.scheduled_time))\n",
    "stop_times_df = stop_times_df.set_index('key_pair')\n",
    "\n",
    "time_point_dict = stop_times_df.drop(['arrival_time', 'departure_time', 'stop_id', 'stop_sequence', 'stop_headsign', 'trip_id',\n",
    "                                      'pickup_type', 'drop_off_type', 'shape_dist_traveled', 'scheduled_time', 'date'], axis=1).to_dict('index')\n",
    "with open(f'results/time_point_dict_{DATE.replace(\"-\", \"\")}.pkl', 'wb') as handle:\n",
    "    pickle.dump(time_point_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# time_point_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, tp) for k, tp in time_point_dict.items() if k[0] == 263558][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_point_dict[(263558, 'GALBERNN', pd.Timestamp('2021-10-18 05:47:59'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT: \n",
    "* Copy these to `scenarios/baselines/data`\n",
    "    * results/sampled_ons_offs_dict\n",
    "    * results/chains/ons_offs_dict_chain_{chain}.pkl\n",
    "    * results/trip_plan.json\n",
    "    * results/vehicle_plan.json\n",
    "    * results/time_point_dict.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure:\n",
    "```\n",
    "{key:val}\n",
    "key: (tuple) (route_id_dir, block_abbr, stop_sequene, stop_id, scheduled_arrival_time)\n",
    "val: (dict) {'sampled_loads': A, 'ons': B, 'offs': C}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity check\n",
    "CHAINS = 5\n",
    "for chain in range(CHAINS):\n",
    "    with open(f'results/chains/ons_offs_dict_chain_{chain}.pkl', 'rb') as handle:\n",
    "        sampled_ons_offs_dict = pickle.load(handle)\n",
    "    # res = sampled_ons_offs_dict[('7_TO DOWNTOWN', 5692, 20, 'MCC5_9', pd.Timestamp('2021-08-23 14:39:00'))]\n",
    "    res = sampled_ons_offs_dict[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', pd.Timestamp('2021-10-18 14:15:00'))]\n",
    "    print(f\"chain {chain}: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sampled_ons_offs_dict.keys())[0], list(sampled_ons_offs_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "with open(f'results/chains/ons_offs_dict_chain_0.pkl', 'rb') as handle:\n",
    "    sampled_ons_offs_dict = pickle.load(handle)\n",
    "# ('7_TO DOWNTOWN', 5692, 1, 'HBHS', datetime.datetime(2021, 8, 23, 14, 9))\n",
    "# sampled_ons_offs_dict[('7_TO DOWNTOWN', 5692, 1, 'HBHS', dt.datetime(2021, 8, 23, 14, 9))]\n",
    "search_key = ('7_TO DOWNTOWN', 5692, 5)\n",
    "values = [value for key, value in sampled_ons_offs_dict.items() if search_key == key[:len(search_key)]]\n",
    "keys = [key for key, value in sampled_ons_offs_dict.items() if search_key == key[:len(search_key)]]\n",
    "values, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sampled_ons_offs_dict.keys())[0], list(sampled_ons_offs_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sampled_ons_offs_dict[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))]\n",
    "('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))\n",
    "('14_FROM DOWNTOWN', '1400', 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))\n",
    "# ('14_FROM DOWNTOWN', '1400', 1, 'MCC4_20', datetime.datetime(2021, 8, 23, 14, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 263159\n",
    "import pandas as pd\n",
    "\n",
    "fp = 'results/sampled_ons_offs_dict_20211018.pkl'\n",
    "df = pd.read_pickle(fp)\n",
    "list(df.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = '55_FROM DOWNTOWN'\n",
    "sid = 'MCC4_15'\n",
    "time = '2021-10-18 15:35:00'\n",
    "\n",
    "df[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', pd.Timestamp('2021-10-18 14:15:00'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, v)for k, v in df.items() if k[0] == rid and k[3] == sid and k[4] == pd.Timestamp(time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = '/home/jptalusan/gits/mta_simulator_redo/data_generation/results/sampled_ons_offs_dict_20220305.pkl'\n",
    "\n",
    "with open(fp, 'rb') as handle:\n",
    "    sampled_ons_offs_dict = pickle.load(handle)\n",
    "sampled_ons_offs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
