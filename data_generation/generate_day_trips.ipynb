{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation\n",
    "* Generates data for a desired date based on the available APC data and passed through the model for load prediction.\n",
    "* Will provide a distribution of bins which can be used for stochasticity\n",
    "## Generates the following files:\n",
    "* `trip_plan.json`\n",
    "* `vehicle_plan.json`\n",
    "* `sampled_loads.pkl`\n",
    "* `chains.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import sys\n",
    "import datetime as dt\n",
    "import importlib\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "import IPython\n",
    "from copy import deepcopy\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import swifter\n",
    "pd.set_option('display.max_columns', None)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '80g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '40g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "        .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "        .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apc_data_for_date(filter_date):\n",
    "    print(\"Running this...\")\n",
    "    filepath = '/home/jptalusan/mta_stationing_problem/data/processed/apc_weather_gtfs_20220921.parquet'\n",
    "    apcdata = spark.read.load(filepath)\n",
    "    apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "    plot_date = filter_date.strftime('%Y-%m-%d')\n",
    "    get_columns = ['trip_id', 'transit_date', 'arrival_time', 'scheduled_time',\n",
    "                'block_abbr', 'stop_sequence', 'stop_id_original',\n",
    "                'vehicle_id', 'vehicle_capacity',\n",
    "                'load', \n",
    "                'darksky_temperature', \n",
    "                'darksky_humidity', \n",
    "                'darksky_precipitation_probability', \n",
    "                'route_direction_name', 'route_id', 'overload_id',\n",
    "                'dayofweek',  'year', 'month', 'hour', 'zero_load_at_trip_end',\n",
    "                'sched_hdwy']\n",
    "    get_str = \", \".join([c for c in get_columns])\n",
    "    query = f\"\"\"\n",
    "    SELECT {get_str}\n",
    "    FROM apc\n",
    "    WHERE (transit_date == '{plot_date}')\n",
    "    ORDER BY arrival_time\n",
    "    \"\"\"\n",
    "    apcdata = spark.sql(query)\n",
    "    apcdata = apcdata.withColumn(\"route_id_dir\", F.concat_ws(\"_\", apcdata.route_id, apcdata.route_direction_name))\n",
    "    apcdata = apcdata.withColumn(\"day\", F.dayofmonth(apcdata.arrival_time))\n",
    "    apcdata = apcdata.drop(\"route_direction_name\")\n",
    "    apcdata = apcdata.withColumn(\"load\", F.when(apcdata.load < 0, 0).otherwise(apcdata.load))\n",
    "    apcdata = apcdata.na.fill(value=0,subset=[\"zero_load_at_trip_end\"])\n",
    "    return apcdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(input_df, ohe_encoder, label_encoders, num_scaler, columns, keep_columns=[], target='y_class'):\n",
    "    num_columns = ['darksky_temperature', 'darksky_humidity', 'darksky_precipitation_probability', 'sched_hdwy']\n",
    "    cat_columns = ['month', 'hour', 'day', 'stop_sequence', 'stop_id_original', 'year', 'time_window']\n",
    "    ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end']\n",
    "\n",
    "    # OHE\n",
    "    input_df[ohe_encoder.get_feature_names_out()] = ohe_encoder.transform(input_df[ohe_columns]).toarray()\n",
    "    # input_df = input_df.drop(columns=ohe_columns)\n",
    "\n",
    "    # Label encoder\n",
    "    for cat in cat_columns:\n",
    "        print(cat)\n",
    "        encoder = label_encoders[cat]\n",
    "        input_df[cat] = encoder.transform(input_df[cat])\n",
    "    \n",
    "    # Num scaler\n",
    "    input_df[num_columns] = num_scaler.transform(input_df[num_columns])\n",
    "    input_df['y_class']  = input_df.y_class.astype('int')\n",
    "\n",
    "    if keep_columns:\n",
    "        columns = keep_columns + columns\n",
    "    # Rearrange columns\n",
    "    input_df = input_df[columns]\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def assign_data_to_bins(df, TARGET='load'):\n",
    "    bins = pd.IntervalIndex.from_tuples([(-1, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)])\n",
    "    mycut = pd.cut(df[TARGET].tolist(), bins=bins)\n",
    "    df['y_class'] = mycut.codes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEWINDOW = 15\n",
    "def add_features(df):\n",
    "    df = df[df.arrival_time.notna()]\n",
    "    df = df.fillna(method=\"bfill\")\n",
    "\n",
    "    df['day'] = df[\"arrival_time\"].dt.day\n",
    "    df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "    # Adding extra features\n",
    "    # Holidays\n",
    "    fp = os.path.join('data', 'US Holiday Dates (2004-2021).csv')\n",
    "    holidays_df = pd.read_csv(fp)\n",
    "    holidays_df['Date'] = pd.to_datetime(holidays_df['Date'])\n",
    "    holidays_df['is_holiday'] = True\n",
    "    df = df.merge(holidays_df[['Date', 'is_holiday']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_holiday'] = df['is_holiday'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "        \n",
    "    # School breaks\n",
    "    fp = os.path.join('data', 'School Breaks (2019-2022).pkl')\n",
    "    school_break_df = pd.read_pickle(fp)\n",
    "    school_break_df['is_school_break'] = True\n",
    "    df = df.merge(school_break_df[['Date', 'is_school_break']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_school_break'] = df['is_school_break'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "\n",
    "    df['minute'] = df['arrival_time'].dt.minute\n",
    "    df['minuteByWindow'] = df['minute'] // TIMEWINDOW\n",
    "    df['temp'] = df['minuteByWindow'] + (df['hour'] * 60 / TIMEWINDOW)\n",
    "    df['time_window'] = np.floor(df['temp']).astype('int')\n",
    "    df = df.drop(columns=['minute', 'minuteByWindow', 'temp'])\n",
    "\n",
    "    # HACK\n",
    "    # df = df[df['hour'] != 3]\n",
    "    # df = df[df['stop_sequence'] != 0]\n",
    "\n",
    "    df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "    df = assign_data_to_bins(df, TARGET='load')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_simple_lstm_generator(num_features, num_classes, learning_rate=1e-4):\n",
    "    # define model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "\n",
    "    input_shape = (None, None, num_features)\n",
    "    model.build(input_shape)\n",
    "    return model\n",
    "\n",
    "def generate_simple_lstm_predictions(input_df, model, past, future):\n",
    "    past_df = input_df[0:past]\n",
    "    future_df = input_df[past:]\n",
    "    predictions = []\n",
    "    pred_probs = []\n",
    "    if future == None:\n",
    "        future = len(future_df)\n",
    "    for f in range(future):\n",
    "        pred = model.predict(past_df.to_numpy().reshape(1, *past_df.shape))\n",
    "        pred_probs.append(pred)\n",
    "        y_pred = np.argmax(pred)\n",
    "        predictions.append(y_pred)\n",
    "        \n",
    "        # Add information from future\n",
    "        last_row = future_df.iloc[[0]]\n",
    "        last_row['y_class'] = y_pred\n",
    "        past_df = pd.concat([past_df[1:], last_row])\n",
    "        \n",
    "        # Move future to remove used row\n",
    "        future_df = future_df[1:]\n",
    "    return predictions, pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overload_regular_bus_trips(regular, overload):\n",
    "    m = regular.merge(overload, how='left', on=['trip_id', 'transit_date', 'scheduled_time', 'block_abbr', 'stop_sequence', 'stop_id_original', 'route_id_dir', 'route_id'])\n",
    "    \n",
    "    m['arrival_time'] = np.max(m[['arrival_time_x', 'arrival_time_y']], axis=1)\n",
    "    \n",
    "    m['zero_load_at_trip_end'] = m['zero_load_at_trip_end_x']\n",
    "    \n",
    "    m.loc[~m['arrival_time_x'].isnull(), \"load\"] = m['load_x']\n",
    "    # m.loc[~m['arrival_time_x'].isnull(), \"ons\"] = m['ons_x']\n",
    "    # m.loc[~m['arrival_time_x'].isnull(), \"offs\"] = m['offs_x']\n",
    "    \n",
    "    m.loc[~m['arrival_time_y'].isnull(), \"load\"] = m['load_y']\n",
    "    # m.loc[~m['arrival_time_y'].isnull(), \"ons\"] = m['ons_y']\n",
    "    # m.loc[~m['arrival_time_y'].isnull(), \"offs\"] = m['offs_y']\n",
    "    \n",
    "    m['vehicle_id'] = m['vehicle_id_x']\n",
    "    m['vehicle_capacity'] = m['vehicle_capacity_x']\n",
    "    m['overload_id'] = m['overload_id_x']\n",
    "    m = m[m.columns.drop(list(m.filter(regex='_x')))]\n",
    "    m = m[m.columns.drop(list(m.filter(regex='_y')))]\n",
    "    # m = m[regular.columns]\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "latest = tf.train.latest_checkpoint('models/no_speed')\n",
    "columns = joblib.load('models/LL_X_columns.joblib')\n",
    "label_encoders = joblib.load('models/LL_Label_encoders.joblib')\n",
    "ohe_encoder = joblib.load('models/LL_OHE_encoder.joblib')\n",
    "num_scaler = joblib.load('models/LL_Num_scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '2021-03-05'\n",
    "start_time = '08:00:00'\n",
    "end_time = '12:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running this...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 02:41:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2021-10-18, 2021-11-23, 2021-12-15, 2022-01-\n",
    "date_to_predict = dt.datetime.strptime(DATE, '%Y-%m-%d')\n",
    "apcdata = get_apc_data_for_date(date_to_predict)\n",
    "df = apcdata.toPandas()\n",
    "\n",
    "# HACK\n",
    "# a = df.query(\"trip_id == '233300' and vehicle_id == '722'\").sort_values('stop_sequence')\n",
    "# b = df.query(\"trip_id == '233300' and vehicle_id == '1830'\").sort_values('stop_sequence')\n",
    "# m1 = merge_overload_regular_bus_trips(a, b)\n",
    "\n",
    "# a = df.query(\"trip_id == '259635' and vehicle_id == '2019'\").sort_values('stop_sequence')\n",
    "# b = df.query(\"trip_id == '259635' and vehicle_id == '1914'\").sort_values('stop_sequence')\n",
    "# m2 = merge_overload_regular_bus_trips(a, b)\n",
    "\n",
    "df = df.query(\"overload_id == 0\")\n",
    "# overload_trips = df.query(\"overload_id > 0\").trip_id.unique()\n",
    "# df = df[~df['trip_id'].isin(overload_trips)]\n",
    "# df = pd.concat([tdf, m1])\n",
    "df = df.dropna(subset=['arrival_time'])\n",
    "# df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# HACK\n",
    "# df = df.query(\"route_id != 95\")\n",
    "# df = df[~df['stop_id_original'].isin(['PEARL', 'JOHASHEN', 'ROS10AEN'])]\n",
    "\n",
    "df = add_features(df)\n",
    "raw_df = deepcopy(df)\n",
    "\n",
    "# HACK\n",
    "# df.loc[df['time_window'].isin([6, 7, 8]), 'time_window'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month\n",
      "hour\n",
      "day\n",
      "stop_sequence\n",
      "stop_id_original\n",
      "year\n",
      "time_window\n"
     ]
    }
   ],
   "source": [
    "input_df = prepare_input_data(df, ohe_encoder, label_encoders, num_scaler, columns, target='y_class')\n",
    "ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end']\n",
    "input_df = input_df.drop(columns=ohe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 20:05:23.424372: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 20:05:23.878143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11402 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:0b:00.0, compute capability: 6.1\n",
      "  0%|          | 0/1005 [00:00<?, ?it/s]2022-11-30 20:05:25.237817: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8401\n",
      "100%|██████████| 1005/1005 [14:42<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "tf.keras.backend.clear_session()\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "FUTURE = None\n",
    "PAST = 5\n",
    "\n",
    "NUM_TRIPS = None\n",
    "if NUM_TRIPS == None:\n",
    "    rand_trips = df.trip_id.unique().tolist()\n",
    "else:\n",
    "    rand_trips = random.sample(df.trip_id.unique().tolist(), NUM_TRIPS)\n",
    "\n",
    "model = setup_simple_lstm_generator(input_df.shape[1], NUM_CLASSES)\n",
    "model.load_weights(latest)\n",
    "\n",
    "trip_res = []\n",
    "load_arr = []\n",
    "for trip_id in tqdm(rand_trips):\n",
    "    _df = df.query(\"trip_id == @trip_id\")\n",
    "    try:\n",
    "        _input_df = input_df.loc[_df.index]\n",
    "        _y_pred, y_pred_probs = generate_simple_lstm_predictions(_input_df, model, PAST, FUTURE)\n",
    "        \n",
    "        # Introducing stochasticity\n",
    "        y_pred = [np.random.choice(len(ypp.flatten()), size=1, p=ypp.flatten())[0] for ypp in y_pred_probs]\n",
    "        loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in y_pred]\n",
    "        \n",
    "        _raw_df = raw_df.loc[_df.index]\n",
    "        y_true = _raw_df[0:PAST]['load'].tolist()\n",
    "        a = y_true + loads\n",
    "        _raw_df['sampled_loads'] = a\n",
    "        \n",
    "        y_true_classes = _raw_df[0:PAST]['y_class'].tolist()\n",
    "        _raw_df['y_pred_classes'] = y_true_classes + y_pred\n",
    "        _raw_df['y_pred_probs'] = [[-1] * NUM_CLASSES]*len(y_true_classes) + [ypp[0] for ypp in y_pred_probs]\n",
    "        \n",
    "        trip_res.append(_raw_df)\n",
    "    except:\n",
    "        print(f\"FAILED:{trip_id}\")\n",
    "        continue\n",
    "\n",
    "trip_res = pd.concat(trip_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_columns = ['trip_id', 'transit_date', 'arrival_time', 'scheduled_time', 'block_abbr', \n",
    "            'stop_sequence', 'stop_id_original', 'route_id_dir', 'zero_load_at_trip_end', \n",
    "            'y_pred_classes', 'y_pred_probs', 'sampled_loads', 'vehicle_id', 'vehicle_capacity']\n",
    "_trip_res = trip_res[_columns]\n",
    "\n",
    "# fp = 'results/sampled_loads.pkl'\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "_trip_res.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching with GTFS time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     trip_df\u001b[39m.\u001b[39mloc[trip_df\u001b[39m.\u001b[39mindex[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mtimepoint\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     trip_res_arr\u001b[39m.\u001b[39mappend(trip_df)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m trip_res_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(trip_res_arr)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# trip_res_df.to_pickle(fp)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m trip_res_df\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/reshape/concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    156\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m    along the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    348\u001b[0m         objs,\n\u001b[1;32m    349\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    350\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    351\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    352\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    353\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    354\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    355\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    356\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    357\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    360\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/reshape/concat.py:404\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    401\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[1;32m    403\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# # fp = 'results/sampled_loads.pkl'\n",
    "# # trip_res_df = pd.read_pickle(fp)\n",
    "# # trip_res_df['trip_id'] = trip_res_df['trip_id'].astype('int')\n",
    "# trip_res_df = _trip_res\n",
    "\n",
    "# trip_res_df = pd.merge(trip_res_df, raw_df[['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original']], \n",
    "#                        left_on=['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original'], \n",
    "#                        right_on=['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original'], how='left')\n",
    "# trip_res_df['trip_id'] = trip_res_df['trip_id'].astype('int')\n",
    "\n",
    "# # print(trip_res_df.shape)\n",
    "# stop_times_fp = 'data/GTFS/OCT2021/stop_times.txt'\n",
    "# stop_times_df = pd.read_csv(stop_times_fp)\n",
    "# # stop_times_df.query(\"trip_id == 264733\")\n",
    "\n",
    "# trip_res_df = pd.merge(trip_res_df, stop_times_df[['trip_id', 'stop_id', 'timepoint']], left_on=['trip_id', 'stop_id_original'], right_on=['trip_id', 'stop_id'])\n",
    "# trip_res_df.query(\"trip_id == 264733\")\n",
    "# trip_res_df = trip_res_df.drop_duplicates(subset=['trip_id', 'stop_id_original', 'arrival_time', 'scheduled_time'])\n",
    "\n",
    "# trip_res_arr = []\n",
    "# for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "#     trip_df.loc[trip_df.index[-1], 'timepoint']= 1.0\n",
    "#     trip_res_arr.append(trip_df)\n",
    "    \n",
    "# trip_res_df = pd.concat(trip_res_arr)\n",
    "\n",
    "# # fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# # trip_res_df.to_pickle(fp)\n",
    "# trip_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vehicle assignments here...\n",
    "* Trying to limit to a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '2021-03-05'\n",
    "start_time = '08:00:00'\n",
    "end_time = '12:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7fc5ba7b9040>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res_df = pd.read_pickle(fp)\n",
    "trip_res_df['hour'] = trip_res_df.scheduled_time.dt.hour\n",
    "trip_res_df['count'] = 1\n",
    "ax = trip_res_df.groupby('trip_id').agg({\"hour\":\"first\", \"count\":\"count\"}).groupby(\"hour\").count().plot(kind='line')\n",
    "ax.axvline(x=8, ymin=0, ymax=100, ls='--')\n",
    "ax.axvline(x=12, ymin=0, ymax=100, ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1005,)\n",
      "(71,)\n",
      "(242,)\n",
      "(54,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>transit_date</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>scheduled_time</th>\n",
       "      <th>block_abbr</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>stop_id_original</th>\n",
       "      <th>route_id_dir</th>\n",
       "      <th>zero_load_at_trip_end</th>\n",
       "      <th>y_pred_classes</th>\n",
       "      <th>y_pred_probs</th>\n",
       "      <th>sampled_loads</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>vehicle_capacity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7657</th>\n",
       "      <td>229634</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 08:05:24</td>\n",
       "      <td>2021-03-05 08:15:00</td>\n",
       "      <td>1701</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC5_11</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1825</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7658</th>\n",
       "      <td>229634</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 08:18:40</td>\n",
       "      <td>2021-03-05 08:18:23</td>\n",
       "      <td>1701</td>\n",
       "      <td>3</td>\n",
       "      <td>11ACHASF</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1825</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7659</th>\n",
       "      <td>229634</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 08:21:38</td>\n",
       "      <td>2021-03-05 08:19:27</td>\n",
       "      <td>1701</td>\n",
       "      <td>5</td>\n",
       "      <td>11APORSF</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1825</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7660</th>\n",
       "      <td>229634</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 08:22:12</td>\n",
       "      <td>2021-03-05 08:20:11</td>\n",
       "      <td>1701</td>\n",
       "      <td>6</td>\n",
       "      <td>11ADEMSF</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1825</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7661</th>\n",
       "      <td>229634</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 08:22:30</td>\n",
       "      <td>2021-03-05 08:20:32</td>\n",
       "      <td>1701</td>\n",
       "      <td>7</td>\n",
       "      <td>11ALAUSF</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1825</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4210</th>\n",
       "      <td>233485</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 10:02:38</td>\n",
       "      <td>2021-03-05 09:54:53</td>\n",
       "      <td>601</td>\n",
       "      <td>60</td>\n",
       "      <td>ANDDESSN</td>\n",
       "      <td>6_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.61943567, 0.31251433, 0.06793563, 0.0001134...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>233485</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 10:03:02</td>\n",
       "      <td>2021-03-05 09:55:56</td>\n",
       "      <td>601</td>\n",
       "      <td>61</td>\n",
       "      <td>ANDHIGSN</td>\n",
       "      <td>6_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9844844, 0.011235767, 0.004279903, 1.146209...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4212</th>\n",
       "      <td>233485</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 10:03:24</td>\n",
       "      <td>2021-03-05 09:56:52</td>\n",
       "      <td>601</td>\n",
       "      <td>62</td>\n",
       "      <td>ANDRVISF</td>\n",
       "      <td>6_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.94585764, 0.039730974, 0.01441083, 6.237122...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4213</th>\n",
       "      <td>233485</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 10:03:58</td>\n",
       "      <td>2021-03-05 09:58:27</td>\n",
       "      <td>601</td>\n",
       "      <td>63</td>\n",
       "      <td>ANDTYLSN</td>\n",
       "      <td>6_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9308591, 0.052289255, 0.01685098, 6.748076e...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4214</th>\n",
       "      <td>233485</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 10:05:00</td>\n",
       "      <td>2021-03-05 10:01:00</td>\n",
       "      <td>601</td>\n",
       "      <td>64</td>\n",
       "      <td>MCSHERM</td>\n",
       "      <td>6_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.8555168, 0.10746812, 0.03700433, 1.0843737e...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6980 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     trip_id transit_date        arrival_time      scheduled_time  block_abbr  \\\n",
       "7657  229634   2021-03-05 2021-03-05 08:05:24 2021-03-05 08:15:00        1701   \n",
       "7658  229634   2021-03-05 2021-03-05 08:18:40 2021-03-05 08:18:23        1701   \n",
       "7659  229634   2021-03-05 2021-03-05 08:21:38 2021-03-05 08:19:27        1701   \n",
       "7660  229634   2021-03-05 2021-03-05 08:22:12 2021-03-05 08:20:11        1701   \n",
       "7661  229634   2021-03-05 2021-03-05 08:22:30 2021-03-05 08:20:32        1701   \n",
       "...      ...          ...                 ...                 ...         ...   \n",
       "4210  233485   2021-03-05 2021-03-05 10:02:38 2021-03-05 09:54:53         601   \n",
       "4211  233485   2021-03-05 2021-03-05 10:03:02 2021-03-05 09:55:56         601   \n",
       "4212  233485   2021-03-05 2021-03-05 10:03:24 2021-03-05 09:56:52         601   \n",
       "4213  233485   2021-03-05 2021-03-05 10:03:58 2021-03-05 09:58:27         601   \n",
       "4214  233485   2021-03-05 2021-03-05 10:05:00 2021-03-05 10:01:00         601   \n",
       "\n",
       "      stop_sequence stop_id_original      route_id_dir  zero_load_at_trip_end  \\\n",
       "7657              1          MCC5_11  17_FROM DOWNTOWN                      0   \n",
       "7658              3         11ACHASF  17_FROM DOWNTOWN                      0   \n",
       "7659              5         11APORSF  17_FROM DOWNTOWN                      0   \n",
       "7660              6         11ADEMSF  17_FROM DOWNTOWN                      0   \n",
       "7661              7         11ALAUSF  17_FROM DOWNTOWN                      0   \n",
       "...             ...              ...               ...                    ...   \n",
       "4210             60         ANDDESSN   6_FROM DOWNTOWN                      0   \n",
       "4211             61         ANDHIGSN   6_FROM DOWNTOWN                      0   \n",
       "4212             62         ANDRVISF   6_FROM DOWNTOWN                      0   \n",
       "4213             63         ANDTYLSN   6_FROM DOWNTOWN                      0   \n",
       "4214             64          MCSHERM   6_FROM DOWNTOWN                      0   \n",
       "\n",
       "      y_pred_classes                                       y_pred_probs  \\\n",
       "7657               0                               [-1, -1, -1, -1, -1]   \n",
       "7658               0                               [-1, -1, -1, -1, -1]   \n",
       "7659               0                               [-1, -1, -1, -1, -1]   \n",
       "7660               0                               [-1, -1, -1, -1, -1]   \n",
       "7661               0                               [-1, -1, -1, -1, -1]   \n",
       "...              ...                                                ...   \n",
       "4210               2  [0.61943567, 0.31251433, 0.06793563, 0.0001134...   \n",
       "4211               0  [0.9844844, 0.011235767, 0.004279903, 1.146209...   \n",
       "4212               0  [0.94585764, 0.039730974, 0.01441083, 6.237122...   \n",
       "4213               0  [0.9308591, 0.052289255, 0.01685098, 6.748076e...   \n",
       "4214               0  [0.8555168, 0.10746812, 0.03700433, 1.0843737e...   \n",
       "\n",
       "      sampled_loads vehicle_id  vehicle_capacity  \n",
       "7657            2.0       1825              40.0  \n",
       "7658            2.0       1825              40.0  \n",
       "7659            1.0       1825              40.0  \n",
       "7660            1.0       1825              40.0  \n",
       "7661            1.0       1825              40.0  \n",
       "...             ...        ...               ...  \n",
       "4210           37.0       2018              40.0  \n",
       "4211            1.0       2018              40.0  \n",
       "4212            2.0       2018              40.0  \n",
       "4213            2.0       2018              40.0  \n",
       "4214            4.0       2018              40.0  \n",
       "\n",
       "[6980 rows x 14 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# Vehicle assignments\n",
    "# Each vehicle config is a dict: {vehicle_capacity, blocks}\n",
    "DEFAULT_CAPACITY = 40.0\n",
    "overall_vehicle_plan = {}\n",
    "\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res_df = pd.read_pickle(fp)\n",
    "print(trip_res_df.trip_id.unique().shape)\n",
    "print(trip_res_df.vehicle_id.unique().shape)\n",
    "\n",
    "start_datetime = dt.datetime.strptime(f\"{DATE} {start_time}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "end_datetime = dt.datetime.strptime(f\"{DATE} {end_time}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "arr = []\n",
    "for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "    if (trip_df.scheduled_time.min() >= start_datetime) and (trip_df.scheduled_time.max() <= end_datetime):\n",
    "        arr.append(trip_df)\n",
    "\n",
    "trip_res_df = pd.concat(arr)\n",
    "print(trip_res_df.trip_id.unique().shape)\n",
    "print(trip_res_df.vehicle_id.unique().shape)\n",
    "trip_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: run again with vehicle_capacity (above)\n",
    "for vehicle_id, vehicle_df in trip_res_df.groupby('vehicle_id'):\n",
    "    vehicle_df = vehicle_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    vehicle_capacity = vehicle_df.iloc[0].vehicle_capacity\n",
    "    # vehicle_capacity = DEFAULT_CAPACITY\n",
    "    if np.isnan(vehicle_capacity):\n",
    "        vehicle_capacity = DEFAULT_CAPACITY\n",
    "    # TODO: This is not the baseline behavior\n",
    "    starting_depot = 'MCC5_1'\n",
    "    service_type = 'regular'\n",
    "    blocks = [block for block in vehicle_df.block_abbr.unique().tolist()]\n",
    "    trips = []\n",
    "    for block in blocks:\n",
    "        block_df = vehicle_df.query(\"block_abbr == @block\")\n",
    "        for trip in block_df.trip_id.unique().tolist():\n",
    "            trips.append((str(block), str(trip)))\n",
    "    overall_vehicle_plan[vehicle_id] = {'vehicle_capacity': vehicle_capacity, 'trips': trips, 'starting_depot': starting_depot, 'service_type': service_type}\n",
    "    \n",
    "len(overall_vehicle_plan)\n",
    "\n",
    "# Number of overload buses\n",
    "#   \"42\": {\n",
    "#     \"service_type\": \"overload\",\n",
    "#     \"starting_depot\": \"MCC5_1\",\n",
    "#     \"trips\": [\n",
    "#     ],\n",
    "#     \"vehicle_capacity\": 55.0\n",
    "#   }\n",
    "OVERLOAD_BUSES = 5\n",
    "for vehicle_id in range(41, 41 + OVERLOAD_BUSES):\n",
    "    overall_vehicle_plan[str(vehicle_id)] = {'vehicle_capacity': 55.0, 'trips': [], \"starting_depot\": \"MCC5_1\", 'service_type': \"overload\"}\n",
    "    \n",
    "with open(f'results/vehicle_plan_{DATE.replace(\"-\", \"\")}_limited.json', 'w') as fp:\n",
    "    json.dump(overall_vehicle_plan, fp, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Trip plan (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res_df = pd.read_pickle(fp)\n",
    "\n",
    "# Create a dict of {[block: {trip_ids:[]}, 'block'....]}\n",
    "# trip_id dict = {'route_id', route_direction_name', 'stop_id':[], 'schedule_time':[]}\n",
    "# Use block as grouper in baseline\n",
    "overall_block_plan = {}\n",
    "for block_abbr, block_df in trip_res_df.groupby('block_abbr'):\n",
    "    block_df = block_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    trip_ids = block_df.trip_id.unique().tolist()\n",
    "    start_time = block_df[block_df['trip_id'] == trip_ids[0]].iloc[0]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    end_time = block_df[block_df['trip_id'] == trip_ids[-1]].iloc[-1]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    overall_block_plan[block_abbr] = {'trip_ids': trip_ids,\n",
    "                                      'start_time': start_time,\n",
    "                                      'end_time': end_time}\n",
    "\n",
    "overall_trip_plan = {}\n",
    "for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "    trip_df = trip_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    route_id_dir = trip_df.iloc[0].route_id_dir\n",
    "    route_id = int(route_id_dir.split(\"_\")[0])\n",
    "    route_direction = route_id_dir.split(\"_\")[1]\n",
    "    zero_load_at_trip_end = trip_df.iloc[-1].zero_load_at_trip_end.tolist()\n",
    "    scheduled_time = trip_df.scheduled_time.dt.strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "    stop_sequence = trip_df.stop_sequence.tolist()\n",
    "    stop_sequence = list(range(0, len(stop_sequence)))\n",
    "    # stop_sequence = [ss - 1 for ss in stop_sequence]\n",
    "    stop_id_original = trip_df.stop_id_original.tolist()\n",
    "    \n",
    "    overall_trip_plan[trip_id] = {'route_id': route_id, \n",
    "                                  'route_direction': route_direction, \n",
    "                                  'scheduled_time': scheduled_time, \n",
    "                                  'stop_sequence': stop_sequence, \n",
    "                                  'stop_id_original': stop_id_original,\n",
    "                                  'zero_load_at_trip_end':zero_load_at_trip_end,\n",
    "                                  'last_stop_sequence': stop_sequence[-1],\n",
    "                                  'last_stop_id': stop_id_original[-1]}\n",
    "\n",
    "len(overall_trip_plan), len(overall_block_plan)\n",
    "\n",
    "with open(f'results/trip_plan_{DATE.replace(\"-\", \"\")}_limited.json', 'w') as fp:\n",
    "    json.dump(overall_trip_plan, fp, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trip_res_df.query(\"trip_id == 259274\").head())\n",
    "print(trip_res_df.query(\"trip_id == 259274\").shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.query(\"trip_id == '243423'\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_id_dir = \n",
    "# trip_res.query(\"route_id_dir == @route_id_dir and block_abbr == @block and stop_id_original == @stop_id_original[@i] and scheduled_time == @scheduled_time[@i]\").iloc[0]['sampled_loads']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ons and offs from sampled loads\n",
    "* Needs the trip_res generated above\n",
    " ```\n",
    " fp = 'results/sampled_loads.pkl'\n",
    " trip_res.to_pickle(fp)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "    \n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "sampled_ons_offs = []\n",
    "for trip_id, trip_id_df in tqdm(trip_res.groupby(['transit_date', 'trip_id'])):\n",
    "    tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "    tdf['ons'] = 0\n",
    "    tdf['offs'] = 0\n",
    "    tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "    \n",
    "    # Intermediate stops\n",
    "    tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # first and last stops\n",
    "    tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "    tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "    sampled_ons_offs.append(tdf)\n",
    "    \n",
    "sampled_ons_offs = pd.concat(sampled_ons_offs)\n",
    "sampled_ons_offs = sampled_ons_offs.drop('next_load', axis=1)\n",
    "\n",
    "# fp = f'results/sampled_ons_offs_{DATE.replace(\"-\", \"\")}.pkl'\n",
    "# sampled_ons_offs.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a single event chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in trip_res.y_pred_classes]\n",
    "trip_res['sampled_loads'] = loads\n",
    "\n",
    "sampled_ons_offs = []\n",
    "for trip_id, trip_id_df in tqdm(trip_res.groupby(['transit_date', 'trip_id'])):\n",
    "    tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "    tdf['stop_sequence'] = list(range(1, len(tdf) + 1))\n",
    "    tdf['ons'] = 0\n",
    "    tdf['offs'] = 0\n",
    "    tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "    \n",
    "    # Intermediate stops\n",
    "    tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # first and last stops\n",
    "    tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "    tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "    sampled_ons_offs.append(tdf)\n",
    "    \n",
    "df = pd.concat(sampled_ons_offs)\n",
    "df = df.drop('next_load', axis=1)\n",
    "\n",
    "display(df)\n",
    "df['key_pair'] = list(zip(df.route_id_dir, \n",
    "                          df.block_abbr,\n",
    "                          df.stop_sequence,\n",
    "                          df.stop_id_original, \n",
    "                          df.scheduled_time))\n",
    "df = df.set_index('key_pair')\n",
    "drop_cols = ['trip_id', 'route_id_dir', 'block_abbr', 'stop_id_original', 'stop_id', 'scheduled_time', \n",
    "                'transit_date', 'arrival_time', 'zero_load_at_trip_end', 'y_pred_classes', 'y_pred_probs',\n",
    "                'vehicle_capacity', 'vehicle_id', 'stop_sequence']\n",
    "drop_cols = [dc for dc in drop_cols if dc in df.columns]\n",
    "df = df.drop(drop_cols, axis=1)\n",
    "sampled_ons_offs_dict = df.to_dict('index')\n",
    "\n",
    "import pickle \n",
    "\n",
    "# with open(f'results/sampled_ons_offs_dict_{DATE.replace(\"-\", \"\")}.pkl', 'wb') as handle:\n",
    "#     pickle.dump(sampled_ons_offs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[('23_FROM DOWNTOWN', 2310, 'DWMRT', pd.Timestamp('2021-08-23 05:41:00'))]\n",
    "df.query(\"route_id_dir == '23_FROM DOWNTOWN' and block_abbr == 2310 and stop_id_original == 'DWMRT' and scheduled_time == '2021-08-23 05:41:00'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = list(sampled_ons_offs_dict.keys())[0]\n",
    "print(key)\n",
    "sampled_ons_offs_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.query(\"route_id_dir == '7_TO DOWNTOWN' and block_abbr == 5692 and stop_sequence == 20 and stop_id_original == 'MCC5_9'\")\n",
    "# df.query(\"route_id_dir == '7_TO DOWNTOWN' and block_abbr == 5692\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating multiple event chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "\n",
    "CHAINS = 5\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "for chain in tqdm(range(CHAINS)):\n",
    "    loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in trip_res.y_pred_classes]\n",
    "    trip_res['sampled_loads'] = loads\n",
    "\n",
    "    sampled_ons_offs = []\n",
    "    for trip_id, trip_id_df in trip_res.groupby(['transit_date', 'trip_id']):\n",
    "        tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "        tdf['stop_sequence'] = list(range(1, len(tdf) + 1))\n",
    "        tdf['ons'] = 0\n",
    "        tdf['offs'] = 0\n",
    "        tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "        \n",
    "        # Intermediate stops\n",
    "        tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "        \n",
    "        # first and last stops\n",
    "        tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "        tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "        sampled_ons_offs.append(tdf)\n",
    "        \n",
    "    df = pd.concat(sampled_ons_offs)\n",
    "    df['key_pair'] = list(zip(df.route_id_dir, \n",
    "                            df.block_abbr,\n",
    "                            df.stop_sequence,\n",
    "                            df.stop_id_original, \n",
    "                            df.scheduled_time))\n",
    "    df = df.set_index('key_pair')\n",
    "    drop_cols = ['trip_id', 'route_id_dir', 'block_abbr', 'stop_id_original', 'stop_id', 'scheduled_time', \n",
    "                 'transit_date', 'arrival_time', 'zero_load_at_trip_end', 'y_pred_classes', 'y_pred_probs',\n",
    "                 'vehicle_capacity', 'vehicle_id', 'stop_sequence']\n",
    "    drop_cols = [dc for dc in drop_cols if dc in df.columns]\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    sampled_ons_offs_dict = df.to_dict('index')\n",
    "\n",
    "    with open(f'results/chains/ons_offs_dict_chain_{DATE.replace(\"-\",\"\")}_{chain}.pkl', 'wb') as handle:\n",
    "        pickle.dump(sampled_ons_offs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create timepoint dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times_df.iloc[0].arrival_time[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(x):\n",
    "    if x[0:2] == '24':\n",
    "        return '00'+x[2:]\n",
    "    if x[0:2] == '25':\n",
    "        return '01'+x[2:]\n",
    "    return x\n",
    "    \n",
    "stop_times_fp = 'data/GTFS/OCT2021/stop_times.txt'\n",
    "stop_times_df = pd.read_csv(stop_times_fp)\n",
    "# stop_times_df.query(\"trip_id == 264733\")\n",
    "stop_times_df['date'] = DATE\n",
    "stop_times_df['arrival_time'] = stop_times_df['arrival_time'].apply(lambda x: fix_time(x))\n",
    "stop_times_df['scheduled_time'] = pd.to_datetime(stop_times_df['date'] + ' ' + stop_times_df['arrival_time'])\n",
    "\n",
    "stop_times_df['key_pair'] = list(zip(stop_times_df.trip_id, stop_times_df.stop_id, stop_times_df.scheduled_time))\n",
    "stop_times_df = stop_times_df.set_index('key_pair')\n",
    "\n",
    "time_point_dict = stop_times_df.drop(['arrival_time', 'departure_time', 'stop_id', 'stop_sequence', 'stop_headsign', 'trip_id',\n",
    "                                      'pickup_type', 'drop_off_type', 'shape_dist_traveled', 'scheduled_time', 'date'], axis=1).to_dict('index')\n",
    "with open(f'results/time_point_dict_{DATE.replace(\"-\", \"\")}.pkl', 'wb') as handle:\n",
    "    pickle.dump(time_point_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# time_point_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, tp) for k, tp in time_point_dict.items() if k[0] == 263558][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_point_dict[(263558, 'GALBERNN', pd.Timestamp('2021-10-18 05:47:59'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT: \n",
    "* Copy these to `scenarios/baselines/data`\n",
    "    * results/sampled_ons_offs_dict\n",
    "    * results/chains/ons_offs_dict_chain_{chain}.pkl\n",
    "    * results/trip_plan.json\n",
    "    * results/vehicle_plan.json\n",
    "    * results/time_point_dict.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure:\n",
    "```\n",
    "{key:val}\n",
    "key: (tuple) (route_id_dir, block_abbr, stop_sequene, stop_id, scheduled_arrival_time)\n",
    "val: (dict) {'sampled_loads': A, 'ons': B, 'offs': C}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity check\n",
    "CHAINS = 5\n",
    "for chain in range(CHAINS):\n",
    "    with open(f'results/chains/ons_offs_dict_chain_{chain}.pkl', 'rb') as handle:\n",
    "        sampled_ons_offs_dict = pickle.load(handle)\n",
    "    # res = sampled_ons_offs_dict[('7_TO DOWNTOWN', 5692, 20, 'MCC5_9', pd.Timestamp('2021-08-23 14:39:00'))]\n",
    "    res = sampled_ons_offs_dict[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', pd.Timestamp('2021-10-18 14:15:00'))]\n",
    "    print(f\"chain {chain}: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sampled_ons_offs_dict.keys())[0], list(sampled_ons_offs_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "with open(f'results/chains/ons_offs_dict_chain_0.pkl', 'rb') as handle:\n",
    "    sampled_ons_offs_dict = pickle.load(handle)\n",
    "# ('7_TO DOWNTOWN', 5692, 1, 'HBHS', datetime.datetime(2021, 8, 23, 14, 9))\n",
    "# sampled_ons_offs_dict[('7_TO DOWNTOWN', 5692, 1, 'HBHS', dt.datetime(2021, 8, 23, 14, 9))]\n",
    "search_key = ('7_TO DOWNTOWN', 5692, 5)\n",
    "values = [value for key, value in sampled_ons_offs_dict.items() if search_key == key[:len(search_key)]]\n",
    "keys = [key for key, value in sampled_ons_offs_dict.items() if search_key == key[:len(search_key)]]\n",
    "values, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sampled_ons_offs_dict.keys())[0], list(sampled_ons_offs_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sampled_ons_offs_dict[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))]\n",
    "('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))\n",
    "('14_FROM DOWNTOWN', '1400', 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))\n",
    "# ('14_FROM DOWNTOWN', '1400', 1, 'MCC4_20', datetime.datetime(2021, 8, 23, 14, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 263159\n",
    "import pandas as pd\n",
    "\n",
    "fp = 'results/sampled_ons_offs_dict_20211018.pkl'\n",
    "df = pd.read_pickle(fp)\n",
    "list(df.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = '55_FROM DOWNTOWN'\n",
    "sid = 'MCC4_15'\n",
    "time = '2021-10-18 15:35:00'\n",
    "\n",
    "df[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', pd.Timestamp('2021-10-18 14:15:00'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, v)for k, v in df.items() if k[0] == rid and k[3] == sid and k[4] == pd.Timestamp(time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = '/home/jptalusan/gits/mta_simulator_redo/data_generation/results/sampled_ons_offs_dict_20220305.pkl'\n",
    "\n",
    "with open(fp, 'rb') as handle:\n",
    "    sampled_ons_offs_dict = pickle.load(handle)\n",
    "sampled_ons_offs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
