{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation\n",
    "* Generates data for a desired date based on the available APC data and passed through the model for load prediction.\n",
    "* Will provide a distribution of bins which can be used for stochasticity\n",
    "## Generates the following files:\n",
    "* `trip_plan.json`\n",
    "* `vehicle_plan.json`\n",
    "* `sampled_loads.pkl`\n",
    "* `chains.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n",
      "22/12/08 03:48:21 WARN Utils: Your hostname, scope-vanderbilt resolves to a loopback address: 127.0.1.1; using 10.2.218.69 instead (on interface enp8s0)\n",
      "22/12/08 03:48:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 03:48:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import sys\n",
    "import datetime as dt\n",
    "import importlib\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "import IPython\n",
    "from copy import deepcopy\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import swifter\n",
    "pd.set_option('display.max_columns', None)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '80g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '40g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "        .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "        .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apc_data_for_date(filter_date):\n",
    "    print(\"Running this...\")\n",
    "    filepath = '/home/jptalusan/mta_stationing_problem/data/processed/apc_weather_gtfs_20220921.parquet'\n",
    "    apcdata = spark.read.load(filepath)\n",
    "    apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "    plot_date = filter_date.strftime('%Y-%m-%d')\n",
    "    get_columns = ['trip_id', 'transit_date', 'arrival_time', 'scheduled_time',\n",
    "                'block_abbr', 'stop_sequence', 'stop_id_original',\n",
    "                'vehicle_id', 'vehicle_capacity',\n",
    "                'load', \n",
    "                'darksky_temperature', \n",
    "                'darksky_humidity', \n",
    "                'darksky_precipitation_probability', \n",
    "                'route_direction_name', 'route_id', 'overload_id',\n",
    "                'dayofweek',  'year', 'month', 'hour', 'zero_load_at_trip_end',\n",
    "                'sched_hdwy']\n",
    "    get_str = \", \".join([c for c in get_columns])\n",
    "    query = f\"\"\"\n",
    "    SELECT {get_str}\n",
    "    FROM apc\n",
    "    WHERE (transit_date == '{plot_date}')\n",
    "    ORDER BY arrival_time\n",
    "    \"\"\"\n",
    "    apcdata = spark.sql(query)\n",
    "    apcdata = apcdata.withColumn(\"route_id_dir\", F.concat_ws(\"_\", apcdata.route_id, apcdata.route_direction_name))\n",
    "    apcdata = apcdata.withColumn(\"day\", F.dayofmonth(apcdata.arrival_time))\n",
    "    apcdata = apcdata.drop(\"route_direction_name\")\n",
    "    apcdata = apcdata.withColumn(\"load\", F.when(apcdata.load < 0, 0).otherwise(apcdata.load))\n",
    "    apcdata = apcdata.na.fill(value=0,subset=[\"zero_load_at_trip_end\"])\n",
    "    return apcdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(input_df, ohe_encoder, label_encoders, num_scaler, columns, keep_columns=[], target='y_class'):\n",
    "    num_columns = ['darksky_temperature', 'darksky_humidity', 'darksky_precipitation_probability', 'sched_hdwy']\n",
    "    cat_columns = ['month', 'hour', 'day', 'stop_sequence', 'stop_id_original', 'year', 'time_window']\n",
    "    ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end']\n",
    "\n",
    "    # OHE\n",
    "    input_df[ohe_encoder.get_feature_names_out()] = ohe_encoder.transform(input_df[ohe_columns]).toarray()\n",
    "    # input_df = input_df.drop(columns=ohe_columns)\n",
    "\n",
    "    # Label encoder\n",
    "    for cat in cat_columns:\n",
    "        print(cat)\n",
    "        encoder = label_encoders[cat]\n",
    "        input_df[cat] = encoder.transform(input_df[cat])\n",
    "    \n",
    "    # Num scaler\n",
    "    input_df[num_columns] = num_scaler.transform(input_df[num_columns])\n",
    "    input_df['y_class']  = input_df.y_class.astype('int')\n",
    "\n",
    "    if keep_columns:\n",
    "        columns = keep_columns + columns\n",
    "    # Rearrange columns\n",
    "    input_df = input_df[columns]\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def assign_data_to_bins(df, TARGET='load'):\n",
    "    bins = pd.IntervalIndex.from_tuples([(-1, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)])\n",
    "    mycut = pd.cut(df[TARGET].tolist(), bins=bins)\n",
    "    df['y_class'] = mycut.codes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEWINDOW = 15\n",
    "def add_features(df):\n",
    "    df = df[df.arrival_time.notna()]\n",
    "    df = df.fillna(method=\"bfill\")\n",
    "\n",
    "    df['day'] = df[\"arrival_time\"].dt.day\n",
    "    df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "    # Adding extra features\n",
    "    # Holidays\n",
    "    fp = os.path.join('data', 'US Holiday Dates (2004-2021).csv')\n",
    "    holidays_df = pd.read_csv(fp)\n",
    "    holidays_df['Date'] = pd.to_datetime(holidays_df['Date'])\n",
    "    holidays_df['is_holiday'] = True\n",
    "    df = df.merge(holidays_df[['Date', 'is_holiday']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_holiday'] = df['is_holiday'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "        \n",
    "    # School breaks\n",
    "    fp = os.path.join('data', 'School Breaks (2019-2022).pkl')\n",
    "    school_break_df = pd.read_pickle(fp)\n",
    "    school_break_df['is_school_break'] = True\n",
    "    df = df.merge(school_break_df[['Date', 'is_school_break']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_school_break'] = df['is_school_break'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "\n",
    "    df['minute'] = df['arrival_time'].dt.minute\n",
    "    df['minuteByWindow'] = df['minute'] // TIMEWINDOW\n",
    "    df['temp'] = df['minuteByWindow'] + (df['hour'] * 60 / TIMEWINDOW)\n",
    "    df['time_window'] = np.floor(df['temp']).astype('int')\n",
    "    df = df.drop(columns=['minute', 'minuteByWindow', 'temp'])\n",
    "\n",
    "    # HACK\n",
    "    # df = df[df['hour'] != 3]\n",
    "    # df = df[df['stop_sequence'] != 0]\n",
    "\n",
    "    df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "    df = assign_data_to_bins(df, TARGET='load')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_simple_lstm_generator(num_features, num_classes, learning_rate=1e-4):\n",
    "    # define model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "\n",
    "    input_shape = (None, None, num_features)\n",
    "    model.build(input_shape)\n",
    "    return model\n",
    "\n",
    "def generate_simple_lstm_predictions(input_df, model, past, future):\n",
    "    past_df = input_df[0:past]\n",
    "    future_df = input_df[past:]\n",
    "    predictions = []\n",
    "    pred_probs = []\n",
    "    if future == None:\n",
    "        future = len(future_df)\n",
    "    for f in range(future):\n",
    "        pred = model.predict(past_df.to_numpy().reshape(1, *past_df.shape))\n",
    "        pred_probs.append(pred)\n",
    "        y_pred = np.argmax(pred)\n",
    "        predictions.append(y_pred)\n",
    "        \n",
    "        # Add information from future\n",
    "        last_row = future_df.iloc[[0]]\n",
    "        last_row['y_class'] = y_pred\n",
    "        past_df = pd.concat([past_df[1:], last_row])\n",
    "        \n",
    "        # Move future to remove used row\n",
    "        future_df = future_df[1:]\n",
    "    return predictions, pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overload_regular_bus_trips(regular, overload):\n",
    "    m = regular.merge(overload, how='left', on=['trip_id', 'transit_date', 'scheduled_time', 'block_abbr', 'stop_sequence', 'stop_id_original', 'route_id_dir', 'route_id'])\n",
    "    \n",
    "    m['arrival_time'] = np.max(m[['arrival_time_x', 'arrival_time_y']], axis=1)\n",
    "    \n",
    "    m['zero_load_at_trip_end'] = m['zero_load_at_trip_end_x']\n",
    "    \n",
    "    m.loc[~m['arrival_time_x'].isnull(), \"load\"] = m['load_x']\n",
    "    # m.loc[~m['arrival_time_x'].isnull(), \"ons\"] = m['ons_x']\n",
    "    # m.loc[~m['arrival_time_x'].isnull(), \"offs\"] = m['offs_x']\n",
    "    \n",
    "    m.loc[~m['arrival_time_y'].isnull(), \"load\"] = m['load_y']\n",
    "    # m.loc[~m['arrival_time_y'].isnull(), \"ons\"] = m['ons_y']\n",
    "    # m.loc[~m['arrival_time_y'].isnull(), \"offs\"] = m['offs_y']\n",
    "    \n",
    "    m['vehicle_id'] = m['vehicle_id_x']\n",
    "    m['vehicle_capacity'] = m['vehicle_capacity_x']\n",
    "    m['overload_id'] = m['overload_id_x']\n",
    "    m = m[m.columns.drop(list(m.filter(regex='_x')))]\n",
    "    m = m[m.columns.drop(list(m.filter(regex='_y')))]\n",
    "    # m = m[regular.columns]\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "latest = tf.train.latest_checkpoint('models/no_speed')\n",
    "columns = joblib.load('models/LL_X_columns.joblib')\n",
    "label_encoders = joblib.load('models/LL_Label_encoders.joblib')\n",
    "ohe_encoder = joblib.load('models/LL_OHE_encoder.joblib')\n",
    "num_scaler = joblib.load('models/LL_Num_scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '2021-03-05'\n",
    "start_time = '08:00:00'\n",
    "end_time = '12:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running this...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 02:41:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2021-10-18, 2021-11-23, 2021-12-15, 2022-01-\n",
    "date_to_predict = dt.datetime.strptime(DATE, '%Y-%m-%d')\n",
    "apcdata = get_apc_data_for_date(date_to_predict)\n",
    "df = apcdata.toPandas()\n",
    "\n",
    "# HACK\n",
    "# a = df.query(\"trip_id == '233300' and vehicle_id == '722'\").sort_values('stop_sequence')\n",
    "# b = df.query(\"trip_id == '233300' and vehicle_id == '1830'\").sort_values('stop_sequence')\n",
    "# m1 = merge_overload_regular_bus_trips(a, b)\n",
    "\n",
    "# a = df.query(\"trip_id == '259635' and vehicle_id == '2019'\").sort_values('stop_sequence')\n",
    "# b = df.query(\"trip_id == '259635' and vehicle_id == '1914'\").sort_values('stop_sequence')\n",
    "# m2 = merge_overload_regular_bus_trips(a, b)\n",
    "\n",
    "df = df.query(\"overload_id == 0\")\n",
    "# overload_trips = df.query(\"overload_id > 0\").trip_id.unique()\n",
    "# df = df[~df['trip_id'].isin(overload_trips)]\n",
    "# df = pd.concat([tdf, m1])\n",
    "df = df.dropna(subset=['arrival_time'])\n",
    "# df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# HACK\n",
    "# df = df.query(\"route_id != 95\")\n",
    "# df = df[~df['stop_id_original'].isin(['PEARL', 'JOHASHEN', 'ROS10AEN'])]\n",
    "\n",
    "df = add_features(df)\n",
    "raw_df = deepcopy(df)\n",
    "\n",
    "# HACK\n",
    "# df.loc[df['time_window'].isin([6, 7, 8]), 'time_window'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month\n",
      "hour\n",
      "day\n",
      "stop_sequence\n",
      "stop_id_original\n",
      "year\n",
      "time_window\n"
     ]
    }
   ],
   "source": [
    "input_df = prepare_input_data(df, ohe_encoder, label_encoders, num_scaler, columns, target='y_class')\n",
    "ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end']\n",
    "input_df = input_df.drop(columns=ohe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 20:05:23.424372: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 20:05:23.878143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11402 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:0b:00.0, compute capability: 6.1\n",
      "  0%|          | 0/1005 [00:00<?, ?it/s]2022-11-30 20:05:25.237817: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8401\n",
      "100%|██████████| 1005/1005 [14:42<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "tf.keras.backend.clear_session()\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "FUTURE = None\n",
    "PAST = 5\n",
    "\n",
    "NUM_TRIPS = None\n",
    "if NUM_TRIPS == None:\n",
    "    rand_trips = df.trip_id.unique().tolist()\n",
    "else:\n",
    "    rand_trips = random.sample(df.trip_id.unique().tolist(), NUM_TRIPS)\n",
    "\n",
    "model = setup_simple_lstm_generator(input_df.shape[1], NUM_CLASSES)\n",
    "model.load_weights(latest)\n",
    "\n",
    "trip_res = []\n",
    "load_arr = []\n",
    "for trip_id in tqdm(rand_trips):\n",
    "    _df = df.query(\"trip_id == @trip_id\")\n",
    "    try:\n",
    "        _input_df = input_df.loc[_df.index]\n",
    "        _y_pred, y_pred_probs = generate_simple_lstm_predictions(_input_df, model, PAST, FUTURE)\n",
    "        \n",
    "        # Introducing stochasticity\n",
    "        y_pred = [np.random.choice(len(ypp.flatten()), size=1, p=ypp.flatten())[0] for ypp in y_pred_probs]\n",
    "        loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in y_pred]\n",
    "        \n",
    "        _raw_df = raw_df.loc[_df.index]\n",
    "        y_true = _raw_df[0:PAST]['load'].tolist()\n",
    "        a = y_true + loads\n",
    "        _raw_df['sampled_loads'] = a\n",
    "        \n",
    "        y_true_classes = _raw_df[0:PAST]['y_class'].tolist()\n",
    "        _raw_df['y_pred_classes'] = y_true_classes + y_pred\n",
    "        _raw_df['y_pred_probs'] = [[-1] * NUM_CLASSES]*len(y_true_classes) + [ypp[0] for ypp in y_pred_probs]\n",
    "        \n",
    "        trip_res.append(_raw_df)\n",
    "    except:\n",
    "        print(f\"FAILED:{trip_id}\")\n",
    "        continue\n",
    "\n",
    "trip_res = pd.concat(trip_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_columns = ['trip_id', 'transit_date', 'arrival_time', 'scheduled_time', 'block_abbr', \n",
    "            'stop_sequence', 'stop_id_original', 'route_id_dir', 'zero_load_at_trip_end', \n",
    "            'y_pred_classes', 'y_pred_probs', 'sampled_loads', 'vehicle_id', 'vehicle_capacity']\n",
    "_trip_res = trip_res[_columns]\n",
    "\n",
    "# fp = 'results/sampled_loads.pkl'\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "_trip_res.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching with GTFS time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     trip_df\u001b[39m.\u001b[39mloc[trip_df\u001b[39m.\u001b[39mindex[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mtimepoint\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     trip_res_arr\u001b[39m.\u001b[39mappend(trip_df)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m trip_res_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(trip_res_arr)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# trip_res_df.to_pickle(fp)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m trip_res_df\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/reshape/concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    156\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m    along the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    348\u001b[0m         objs,\n\u001b[1;32m    349\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    350\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    351\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    352\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    353\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    354\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    355\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    356\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    357\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    360\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/reshape/concat.py:404\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    401\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[1;32m    403\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# # fp = 'results/sampled_loads.pkl'\n",
    "# # trip_res_df = pd.read_pickle(fp)\n",
    "# # trip_res_df['trip_id'] = trip_res_df['trip_id'].astype('int')\n",
    "# trip_res_df = _trip_res\n",
    "\n",
    "# trip_res_df = pd.merge(trip_res_df, raw_df[['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original']], \n",
    "#                        left_on=['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original'], \n",
    "#                        right_on=['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original'], how='left')\n",
    "# trip_res_df['trip_id'] = trip_res_df['trip_id'].astype('int')\n",
    "\n",
    "# # print(trip_res_df.shape)\n",
    "# stop_times_fp = 'data/GTFS/OCT2021/stop_times.txt'\n",
    "# stop_times_df = pd.read_csv(stop_times_fp)\n",
    "# # stop_times_df.query(\"trip_id == 264733\")\n",
    "\n",
    "# trip_res_df = pd.merge(trip_res_df, stop_times_df[['trip_id', 'stop_id', 'timepoint']], left_on=['trip_id', 'stop_id_original'], right_on=['trip_id', 'stop_id'])\n",
    "# trip_res_df.query(\"trip_id == 264733\")\n",
    "# trip_res_df = trip_res_df.drop_duplicates(subset=['trip_id', 'stop_id_original', 'arrival_time', 'scheduled_time'])\n",
    "\n",
    "# trip_res_arr = []\n",
    "# for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "#     trip_df.loc[trip_df.index[-1], 'timepoint']= 1.0\n",
    "#     trip_res_arr.append(trip_df)\n",
    "    \n",
    "# trip_res_df = pd.concat(trip_res_arr)\n",
    "\n",
    "# # fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# # trip_res_df.to_pickle(fp)\n",
    "# trip_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vehicle assignments here...\n",
    "* Trying to limit to a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2021-10-18': ['121',\n",
       "  '124',\n",
       "  '140',\n",
       "  '1824',\n",
       "  '1830',\n",
       "  '1900',\n",
       "  '1920',\n",
       "  '2013',\n",
       "  '2015',\n",
       "  '715',\n",
       "  '720'],\n",
       " '2021-11-23': ['1812',\n",
       "  '1817',\n",
       "  '1820',\n",
       "  '1825',\n",
       "  '1827',\n",
       "  '1900',\n",
       "  '1906',\n",
       "  '1907',\n",
       "  '2006',\n",
       "  '2009'],\n",
       " '2021-12-15': ['121',\n",
       "  '134',\n",
       "  '1800',\n",
       "  '1804',\n",
       "  '1812',\n",
       "  '1821',\n",
       "  '1826',\n",
       "  '1830',\n",
       "  '1904',\n",
       "  '1908'],\n",
       " '2022-01-27': ['129',\n",
       "  '137',\n",
       "  '1811',\n",
       "  '1815',\n",
       "  '1819',\n",
       "  '1824',\n",
       "  '1906',\n",
       "  '1920',\n",
       "  '2003',\n",
       "  '2004'],\n",
       " '2022-02-25': ['127',\n",
       "  '130',\n",
       "  '139',\n",
       "  '141',\n",
       "  '1814',\n",
       "  '1820',\n",
       "  '1823',\n",
       "  '1901',\n",
       "  '1913',\n",
       "  '1916']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    '2021-10-18':['121', '124', '140', '1824', '1830', '1900', '1920', '2013', '2015', '715', '720'],\n",
    "    '2021-11-23':['1812', '1817', '1820', '1825', '1827', '1900', '1906', '1907', '2006', '2009'],\n",
    "    '2021-12-15':['121', '134', '1800', '1804', '1812', '1821', '1826', '1830', '1904', '1908'],\n",
    "    '2022-01-27':['129', '137', '1811', '1815', '1819', '1824', '1906', '1920', '2003', '2004'],\n",
    "    '2022-02-25':['127', '130', '139', '141', '1814', '1820', '1823', '1901', '1913', '1916']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '2021-12-15'\n",
    "vehicle_list = ['121', '134', '1800', '1804', '1812', '1821', '1826', '1830', '1904', '1908', '2007']\n",
    "start_time = '08:00:00'\n",
    "end_time = '12:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7f99ad863730>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res_df = pd.read_pickle(fp)\n",
    "trip_res_df['hour'] = trip_res_df.scheduled_time.dt.hour\n",
    "trip_res_df['count'] = 1\n",
    "ax = trip_res_df.groupby('trip_id').agg({\"hour\":\"first\", \"count\":\"count\"}).groupby(\"hour\").count().plot(kind='line')\n",
    "ax.axvline(x=6, ymin=0, ymax=100, ls='--')\n",
    "ax.axvline(x=10, ymin=0, ymax=100, ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(219,)\n",
      "(11,)\n",
      "(45,)\n",
      "(10,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>transit_date</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>scheduled_time</th>\n",
       "      <th>block_abbr</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>stop_id_original</th>\n",
       "      <th>route_id_dir</th>\n",
       "      <th>zero_load_at_trip_end</th>\n",
       "      <th>y_pred_classes</th>\n",
       "      <th>y_pred_probs</th>\n",
       "      <th>sampled_loads</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>vehicle_capacity</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>timepoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10809</th>\n",
       "      <td>259285</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15 08:07:16</td>\n",
       "      <td>2021-12-15 08:15:00</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_20</td>\n",
       "      <td>14_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1821</td>\n",
       "      <td>40.0</td>\n",
       "      <td>MCC4_20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10810</th>\n",
       "      <td>259285</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15 08:19:12</td>\n",
       "      <td>2021-12-15 08:16:19</td>\n",
       "      <td>1400</td>\n",
       "      <td>2</td>\n",
       "      <td>UNI2AEF</td>\n",
       "      <td>14_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1821</td>\n",
       "      <td>40.0</td>\n",
       "      <td>UNI2AEF</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10811</th>\n",
       "      <td>259285</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15 08:20:38</td>\n",
       "      <td>2021-12-15 08:17:52</td>\n",
       "      <td>1400</td>\n",
       "      <td>3</td>\n",
       "      <td>1SWOONM</td>\n",
       "      <td>14_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1821</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1SWOONM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10812</th>\n",
       "      <td>259285</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15 08:20:58</td>\n",
       "      <td>2021-12-15 08:18:41</td>\n",
       "      <td>1400</td>\n",
       "      <td>4</td>\n",
       "      <td>1SJAMNM</td>\n",
       "      <td>14_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1821</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1SJAMNM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10813</th>\n",
       "      <td>259285</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15 08:21:16</td>\n",
       "      <td>2021-12-15 08:19:24</td>\n",
       "      <td>1400</td>\n",
       "      <td>5</td>\n",
       "      <td>N1SOLDNM</td>\n",
       "      <td>14_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1821</td>\n",
       "      <td>40.0</td>\n",
       "      <td>N1SOLDNM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6150</th>\n",
       "      <td>264060</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15 10:38:32</td>\n",
       "      <td>2021-12-15 10:34:53</td>\n",
       "      <td>600</td>\n",
       "      <td>60</td>\n",
       "      <td>ANDDESSN</td>\n",
       "      <td>6_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.5961582, 0.32973012, 0.07393855, 0.00017165...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1904</td>\n",
       "      <td>40.0</td>\n",
       "      <td>ANDDESSN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>264060</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15 10:39:04</td>\n",
       "      <td>2021-12-15 10:35:56</td>\n",
       "      <td>600</td>\n",
       "      <td>61</td>\n",
       "      <td>ANDHIGSN</td>\n",
       "      <td>6_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9862316, 0.0101123005, 0.0036561803, 6.3828...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1904</td>\n",
       "      <td>40.0</td>\n",
       "      <td>ANDHIGSN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>264060</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15 10:39:50</td>\n",
       "      <td>2021-12-15 10:36:52</td>\n",
       "      <td>600</td>\n",
       "      <td>62</td>\n",
       "      <td>ANDRVISF</td>\n",
       "      <td>6_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9708015, 0.021678776, 0.007519682, 7.727087...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1904</td>\n",
       "      <td>40.0</td>\n",
       "      <td>ANDRVISF</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>264060</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15 10:40:28</td>\n",
       "      <td>2021-12-15 10:38:27</td>\n",
       "      <td>600</td>\n",
       "      <td>63</td>\n",
       "      <td>ANDTYLSN</td>\n",
       "      <td>6_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9622068, 0.02951225, 0.008280826, 7.070647e...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1904</td>\n",
       "      <td>40.0</td>\n",
       "      <td>ANDTYLSN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6154</th>\n",
       "      <td>264060</td>\n",
       "      <td>2021-12-15</td>\n",
       "      <td>2021-12-15 10:41:28</td>\n",
       "      <td>2021-12-15 10:41:00</td>\n",
       "      <td>600</td>\n",
       "      <td>64</td>\n",
       "      <td>MCSHERM</td>\n",
       "      <td>6_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.88259476, 0.089706704, 0.027694432, 4.04968...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1904</td>\n",
       "      <td>40.0</td>\n",
       "      <td>MCSHERM</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1629 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       trip_id transit_date        arrival_time      scheduled_time  \\\n",
       "10809   259285   2021-12-15 2021-12-15 08:07:16 2021-12-15 08:15:00   \n",
       "10810   259285   2021-12-15 2021-12-15 08:19:12 2021-12-15 08:16:19   \n",
       "10811   259285   2021-12-15 2021-12-15 08:20:38 2021-12-15 08:17:52   \n",
       "10812   259285   2021-12-15 2021-12-15 08:20:58 2021-12-15 08:18:41   \n",
       "10813   259285   2021-12-15 2021-12-15 08:21:16 2021-12-15 08:19:24   \n",
       "...        ...          ...                 ...                 ...   \n",
       "6150    264060   2021-12-15 2021-12-15 10:38:32 2021-12-15 10:34:53   \n",
       "6151    264060   2021-12-15 2021-12-15 10:39:04 2021-12-15 10:35:56   \n",
       "6152    264060   2021-12-15 2021-12-15 10:39:50 2021-12-15 10:36:52   \n",
       "6153    264060   2021-12-15 2021-12-15 10:40:28 2021-12-15 10:38:27   \n",
       "6154    264060   2021-12-15 2021-12-15 10:41:28 2021-12-15 10:41:00   \n",
       "\n",
       "       block_abbr  stop_sequence stop_id_original      route_id_dir  \\\n",
       "10809        1400              1          MCC4_20  14_FROM DOWNTOWN   \n",
       "10810        1400              2          UNI2AEF  14_FROM DOWNTOWN   \n",
       "10811        1400              3          1SWOONM  14_FROM DOWNTOWN   \n",
       "10812        1400              4          1SJAMNM  14_FROM DOWNTOWN   \n",
       "10813        1400              5         N1SOLDNM  14_FROM DOWNTOWN   \n",
       "...           ...            ...              ...               ...   \n",
       "6150          600             60         ANDDESSN   6_FROM DOWNTOWN   \n",
       "6151          600             61         ANDHIGSN   6_FROM DOWNTOWN   \n",
       "6152          600             62         ANDRVISF   6_FROM DOWNTOWN   \n",
       "6153          600             63         ANDTYLSN   6_FROM DOWNTOWN   \n",
       "6154          600             64          MCSHERM   6_FROM DOWNTOWN   \n",
       "\n",
       "       zero_load_at_trip_end  y_pred_classes  \\\n",
       "10809                      0               1   \n",
       "10810                      0               1   \n",
       "10811                      0               1   \n",
       "10812                      0               1   \n",
       "10813                      0               0   \n",
       "...                      ...             ...   \n",
       "6150                       0               0   \n",
       "6151                       0               0   \n",
       "6152                       0               0   \n",
       "6153                       0               0   \n",
       "6154                       0               0   \n",
       "\n",
       "                                            y_pred_probs  sampled_loads  \\\n",
       "10809                               [-1, -1, -1, -1, -1]            8.0   \n",
       "10810                               [-1, -1, -1, -1, -1]            8.0   \n",
       "10811                               [-1, -1, -1, -1, -1]            8.0   \n",
       "10812                               [-1, -1, -1, -1, -1]            8.0   \n",
       "10813                               [-1, -1, -1, -1, -1]            6.0   \n",
       "...                                                  ...            ...   \n",
       "6150   [0.5961582, 0.32973012, 0.07393855, 0.00017165...            4.0   \n",
       "6151   [0.9862316, 0.0101123005, 0.0036561803, 6.3828...            3.0   \n",
       "6152   [0.9708015, 0.021678776, 0.007519682, 7.727087...            6.0   \n",
       "6153   [0.9622068, 0.02951225, 0.008280826, 7.070647e...            5.0   \n",
       "6154   [0.88259476, 0.089706704, 0.027694432, 4.04968...            0.0   \n",
       "\n",
       "      vehicle_id  vehicle_capacity   stop_id  timepoint  \n",
       "10809       1821              40.0   MCC4_20          1  \n",
       "10810       1821              40.0   UNI2AEF          0  \n",
       "10811       1821              40.0   1SWOONM          0  \n",
       "10812       1821              40.0   1SJAMNM          0  \n",
       "10813       1821              40.0  N1SOLDNM          0  \n",
       "...          ...               ...       ...        ...  \n",
       "6150        1904              40.0  ANDDESSN          0  \n",
       "6151        1904              40.0  ANDHIGSN          0  \n",
       "6152        1904              40.0  ANDRVISF          0  \n",
       "6153        1904              40.0  ANDTYLSN          0  \n",
       "6154        1904              40.0   MCSHERM          1  \n",
       "\n",
       "[1629 rows x 16 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# Vehicle assignments\n",
    "# Each vehicle config is a dict: {vehicle_capacity, blocks}\n",
    "DEFAULT_CAPACITY = 40.0\n",
    "overall_vehicle_plan = {}\n",
    "\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res_df = pd.read_pickle(fp)\n",
    "trip_res_df = trip_res_df[trip_res_df['vehicle_id'].isin(vehicle_list)]\n",
    "print(trip_res_df.trip_id.unique().shape)\n",
    "print(trip_res_df.vehicle_id.unique().shape)\n",
    "\n",
    "start_datetime = dt.datetime.strptime(f\"{DATE} {start_time}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "end_datetime = dt.datetime.strptime(f\"{DATE} {end_time}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "arr = []\n",
    "for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "    if (trip_df.scheduled_time.min() >= start_datetime) and (trip_df.scheduled_time.max() <= end_datetime):\n",
    "        arr.append(trip_df)\n",
    "\n",
    "trip_res_df = pd.concat(arr)\n",
    "print(trip_res_df.trip_id.unique().shape)\n",
    "print(trip_res_df.vehicle_id.unique().shape)\n",
    "trip_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: run again with vehicle_capacity (above)\n",
    "for vehicle_id, vehicle_df in trip_res_df.groupby('vehicle_id'):\n",
    "    vehicle_df = vehicle_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    vehicle_capacity = vehicle_df.iloc[0].vehicle_capacity\n",
    "    # vehicle_capacity = DEFAULT_CAPACITY\n",
    "    if np.isnan(vehicle_capacity):\n",
    "        vehicle_capacity = DEFAULT_CAPACITY\n",
    "    # TODO: This is not the baseline behavior\n",
    "    starting_depot = 'MCC5_1'\n",
    "    service_type = 'regular'\n",
    "    blocks = [block for block in vehicle_df.block_abbr.unique().tolist()]\n",
    "    trips = []\n",
    "    for block in blocks:\n",
    "        block_df = vehicle_df.query(\"block_abbr == @block\")\n",
    "        for trip in block_df.trip_id.unique().tolist():\n",
    "            trips.append((str(block), str(trip)))\n",
    "    overall_vehicle_plan[vehicle_id] = {'vehicle_capacity': vehicle_capacity, 'trips': trips, 'starting_depot': starting_depot, 'service_type': service_type}\n",
    "    \n",
    "len(overall_vehicle_plan)\n",
    "\n",
    "# Number of overload buses\n",
    "#   \"42\": {\n",
    "#     \"service_type\": \"overload\",\n",
    "#     \"starting_depot\": \"MCC5_1\",\n",
    "#     \"trips\": [\n",
    "#     ],\n",
    "#     \"vehicle_capacity\": 55.0\n",
    "#   }\n",
    "OVERLOAD_BUSES = 5\n",
    "for vehicle_id in range(41, 41 + OVERLOAD_BUSES):\n",
    "    overall_vehicle_plan[str(vehicle_id)] = {'vehicle_capacity': 55.0, 'trips': [], \"starting_depot\": \"MCC5_1\", 'service_type': \"overload\"}\n",
    "    \n",
    "with open(f'results/vehicle_plan_{DATE.replace(\"-\", \"\")}_10_limited.json', 'w') as fp:\n",
    "    json.dump(overall_vehicle_plan, fp, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Trip plan (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res_df = pd.read_pickle(fp)\n",
    "\n",
    "# Create a dict of {[block: {trip_ids:[]}, 'block'....]}\n",
    "# trip_id dict = {'route_id', route_direction_name', 'stop_id':[], 'schedule_time':[]}\n",
    "# Use block as grouper in baseline\n",
    "overall_block_plan = {}\n",
    "for block_abbr, block_df in trip_res_df.groupby('block_abbr'):\n",
    "    block_df = block_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    trip_ids = block_df.trip_id.unique().tolist()\n",
    "    start_time = block_df[block_df['trip_id'] == trip_ids[0]].iloc[0]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    end_time = block_df[block_df['trip_id'] == trip_ids[-1]].iloc[-1]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    overall_block_plan[block_abbr] = {'trip_ids': trip_ids,\n",
    "                                      'start_time': start_time,\n",
    "                                      'end_time': end_time}\n",
    "\n",
    "overall_trip_plan = {}\n",
    "for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "    trip_df = trip_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    route_id_dir = trip_df.iloc[0].route_id_dir\n",
    "    route_id = int(route_id_dir.split(\"_\")[0])\n",
    "    route_direction = route_id_dir.split(\"_\")[1]\n",
    "    zero_load_at_trip_end = trip_df.iloc[-1].zero_load_at_trip_end.tolist()\n",
    "    scheduled_time = trip_df.scheduled_time.dt.strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "    stop_sequence = trip_df.stop_sequence.tolist()\n",
    "    stop_sequence = list(range(0, len(stop_sequence)))\n",
    "    # stop_sequence = [ss - 1 for ss in stop_sequence]\n",
    "    stop_id_original = trip_df.stop_id_original.tolist()\n",
    "    \n",
    "    overall_trip_plan[trip_id] = {'route_id': route_id, \n",
    "                                  'route_direction': route_direction, \n",
    "                                  'scheduled_time': scheduled_time, \n",
    "                                  'stop_sequence': stop_sequence, \n",
    "                                  'stop_id_original': stop_id_original,\n",
    "                                  'zero_load_at_trip_end':zero_load_at_trip_end,\n",
    "                                  'last_stop_sequence': stop_sequence[-1],\n",
    "                                  'last_stop_id': stop_id_original[-1]}\n",
    "\n",
    "len(overall_trip_plan), len(overall_block_plan)\n",
    "\n",
    "with open(f'results/trip_plan_{DATE.replace(\"-\", \"\")}_10_limited.json', 'w') as fp:\n",
    "    json.dump(overall_trip_plan, fp, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trip_res_df.query(\"trip_id == 259274\").head())\n",
    "print(trip_res_df.query(\"trip_id == 259274\").shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.query(\"trip_id == '243423'\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_id_dir = \n",
    "# trip_res.query(\"route_id_dir == @route_id_dir and block_abbr == @block and stop_id_original == @stop_id_original[@i] and scheduled_time == @scheduled_time[@i]\").iloc[0]['sampled_loads']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trip and Vehicle plan loop\n",
    "* All buses and all day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dates = ['2021-06-07', '2021-07-13', '2021-08-25', '2021-05-07']\n",
    "\n",
    "for date in dates:\n",
    "    # Vehicle assignments\n",
    "    # Each vehicle config is a dict: {vehicle_capacity, blocks}\n",
    "    DEFAULT_CAPACITY = 10.0\n",
    "    overall_vehicle_plan = {}\n",
    "\n",
    "    fp = f'results/sampled_loads_{date.replace(\"-\",\"\")}.pkl'\n",
    "    trip_res_df = pd.read_pickle(fp)\n",
    "    \n",
    "    for vehicle_id, vehicle_df in trip_res_df.groupby('vehicle_id'):\n",
    "        vehicle_df = vehicle_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "        vehicle_capacity = vehicle_df.iloc[0].vehicle_capacity\n",
    "        vehicle_capacity = DEFAULT_CAPACITY\n",
    "        # if np.isnan(vehicle_capacity):\n",
    "        #     vehicle_capacity = DEFAULT_CAPACITY\n",
    "        # TODO: This is not the baseline behavior\n",
    "        starting_depot = 'MCC5_1'\n",
    "        service_type = 'regular'\n",
    "        blocks = [block for block in vehicle_df.block_abbr.unique().tolist()]\n",
    "        trips = []\n",
    "        for block in blocks:\n",
    "            block_df = vehicle_df.query(\"block_abbr == @block\")\n",
    "            for trip in block_df.trip_id.unique().tolist():\n",
    "                trips.append((str(block), str(trip)))\n",
    "        overall_vehicle_plan[vehicle_id] = {'vehicle_capacity': vehicle_capacity, 'trips': trips, 'starting_depot': starting_depot, 'service_type': service_type}\n",
    "        \n",
    "    OVERLOAD_BUSES = 5\n",
    "    for vehicle_id in range(41, 41 + OVERLOAD_BUSES):\n",
    "        overall_vehicle_plan[str(vehicle_id)] = {'vehicle_capacity': 55.0, 'trips': [], \"starting_depot\": \"MCC5_1\", 'service_type': \"overload\"}\n",
    "        \n",
    "    with open(f'results/vehicle_plan_{date.replace(\"-\", \"\")}.json', 'w') as fp:\n",
    "        json.dump(overall_vehicle_plan, fp, sort_keys=True, indent=2)\n",
    "        \n",
    "    overall_block_plan = {}\n",
    "    for block_abbr, block_df in trip_res_df.groupby('block_abbr'):\n",
    "        block_df = block_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "        trip_ids = block_df.trip_id.unique().tolist()\n",
    "        start_time = block_df[block_df['trip_id'] == trip_ids[0]].iloc[0]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        end_time = block_df[block_df['trip_id'] == trip_ids[-1]].iloc[-1]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "        overall_block_plan[block_abbr] = {'trip_ids': trip_ids,\n",
    "                                        'start_time': start_time,\n",
    "                                        'end_time': end_time}\n",
    "\n",
    "    overall_trip_plan = {}\n",
    "    for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "        trip_df = trip_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "        route_id_dir = trip_df.iloc[0].route_id_dir\n",
    "        route_id = int(route_id_dir.split(\"_\")[0])\n",
    "        route_direction = route_id_dir.split(\"_\")[1]\n",
    "        zero_load_at_trip_end = trip_df.iloc[-1].zero_load_at_trip_end.tolist()\n",
    "        scheduled_time = trip_df.scheduled_time.dt.strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "        stop_sequence = trip_df.stop_sequence.tolist()\n",
    "        stop_sequence = list(range(0, len(stop_sequence)))\n",
    "        # stop_sequence = [ss - 1 for ss in stop_sequence]\n",
    "        stop_id_original = trip_df.stop_id_original.tolist()\n",
    "        \n",
    "        overall_trip_plan[trip_id] = {'route_id': route_id, \n",
    "                                    'route_direction': route_direction, \n",
    "                                    'scheduled_time': scheduled_time, \n",
    "                                    'stop_sequence': stop_sequence, \n",
    "                                    'stop_id_original': stop_id_original,\n",
    "                                    'zero_load_at_trip_end':zero_load_at_trip_end,\n",
    "                                    'last_stop_sequence': stop_sequence[-1],\n",
    "                                    'last_stop_id': stop_id_original[-1]}\n",
    "\n",
    "    len(overall_trip_plan), len(overall_block_plan)\n",
    "\n",
    "    with open(f'results/trip_plan_{date.replace(\"-\", \"\")}.json', 'w') as fp:\n",
    "        json.dump(overall_trip_plan, fp, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ons and offs from sampled loads\n",
    "* Needs the trip_res generated above\n",
    " ```\n",
    " fp = 'results/sampled_loads.pkl'\n",
    " trip_res.to_pickle(fp)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "    \n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "sampled_ons_offs = []\n",
    "for trip_id, trip_id_df in tqdm(trip_res.groupby(['transit_date', 'trip_id'])):\n",
    "    tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "    tdf['ons'] = 0\n",
    "    tdf['offs'] = 0\n",
    "    tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "    \n",
    "    # Intermediate stops\n",
    "    tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # first and last stops\n",
    "    tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "    tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "    sampled_ons_offs.append(tdf)\n",
    "    \n",
    "sampled_ons_offs = pd.concat(sampled_ons_offs)\n",
    "sampled_ons_offs = sampled_ons_offs.drop('next_load', axis=1)\n",
    "\n",
    "# fp = f'results/sampled_ons_offs_{DATE.replace(\"-\", \"\")}.pkl'\n",
    "# sampled_ons_offs.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a single event chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in trip_res.y_pred_classes]\n",
    "trip_res['sampled_loads'] = loads\n",
    "\n",
    "sampled_ons_offs = []\n",
    "for trip_id, trip_id_df in tqdm(trip_res.groupby(['transit_date', 'trip_id'])):\n",
    "    tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "    tdf['stop_sequence'] = list(range(1, len(tdf) + 1))\n",
    "    tdf['ons'] = 0\n",
    "    tdf['offs'] = 0\n",
    "    tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "    \n",
    "    # Intermediate stops\n",
    "    tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # first and last stops\n",
    "    tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "    tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "    sampled_ons_offs.append(tdf)\n",
    "    \n",
    "df = pd.concat(sampled_ons_offs)\n",
    "df = df.drop('next_load', axis=1)\n",
    "\n",
    "display(df)\n",
    "df['key_pair'] = list(zip(df.route_id_dir, \n",
    "                          df.block_abbr,\n",
    "                          df.stop_sequence,\n",
    "                          df.stop_id_original, \n",
    "                          df.scheduled_time))\n",
    "df = df.set_index('key_pair')\n",
    "drop_cols = ['trip_id', 'route_id_dir', 'block_abbr', 'stop_id_original', 'stop_id', 'scheduled_time', \n",
    "                'transit_date', 'arrival_time', 'zero_load_at_trip_end', 'y_pred_classes', 'y_pred_probs',\n",
    "                'vehicle_capacity', 'vehicle_id', 'stop_sequence']\n",
    "drop_cols = [dc for dc in drop_cols if dc in df.columns]\n",
    "df = df.drop(drop_cols, axis=1)\n",
    "sampled_ons_offs_dict = df.to_dict('index')\n",
    "\n",
    "import pickle \n",
    "\n",
    "# with open(f'results/sampled_ons_offs_dict_{DATE.replace(\"-\", \"\")}.pkl', 'wb') as handle:\n",
    "#     pickle.dump(sampled_ons_offs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[('23_FROM DOWNTOWN', 2310, 'DWMRT', pd.Timestamp('2021-08-23 05:41:00'))]\n",
    "df.query(\"route_id_dir == '23_FROM DOWNTOWN' and block_abbr == 2310 and stop_id_original == 'DWMRT' and scheduled_time == '2021-08-23 05:41:00'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = list(sampled_ons_offs_dict.keys())[0]\n",
    "print(key)\n",
    "sampled_ons_offs_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.query(\"route_id_dir == '7_TO DOWNTOWN' and block_abbr == 5692 and stop_sequence == 20 and stop_id_original == 'MCC5_9'\")\n",
    "# df.query(\"route_id_dir == '7_TO DOWNTOWN' and block_abbr == 5692\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating multiple event chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "\n",
    "CHAINS = 5\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "for chain in tqdm(range(CHAINS)):\n",
    "    loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in trip_res.y_pred_classes]\n",
    "    trip_res['sampled_loads'] = loads\n",
    "\n",
    "    sampled_ons_offs = []\n",
    "    for trip_id, trip_id_df in trip_res.groupby(['transit_date', 'trip_id']):\n",
    "        tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "        tdf['stop_sequence'] = list(range(1, len(tdf) + 1))\n",
    "        tdf['ons'] = 0\n",
    "        tdf['offs'] = 0\n",
    "        tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "        \n",
    "        # Intermediate stops\n",
    "        tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "        \n",
    "        # first and last stops\n",
    "        tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "        tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "        sampled_ons_offs.append(tdf)\n",
    "        \n",
    "    df = pd.concat(sampled_ons_offs)\n",
    "    df['key_pair'] = list(zip(df.route_id_dir, \n",
    "                            df.block_abbr,\n",
    "                            df.stop_sequence,\n",
    "                            df.stop_id_original, \n",
    "                            df.scheduled_time))\n",
    "    df = df.set_index('key_pair')\n",
    "    drop_cols = ['trip_id', 'route_id_dir', 'block_abbr', 'stop_id_original', 'stop_id', 'scheduled_time', \n",
    "                 'transit_date', 'arrival_time', 'zero_load_at_trip_end', 'y_pred_classes', 'y_pred_probs',\n",
    "                 'vehicle_capacity', 'vehicle_id', 'stop_sequence']\n",
    "    drop_cols = [dc for dc in drop_cols if dc in df.columns]\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    sampled_ons_offs_dict = df.to_dict('index')\n",
    "\n",
    "    with open(f'results/chains/ons_offs_dict_chain_{DATE.replace(\"-\",\"\")}_{chain}.pkl', 'wb') as handle:\n",
    "        pickle.dump(sampled_ons_offs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create timepoint dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times_df.iloc[0].arrival_time[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(x):\n",
    "    if x[0:2] == '24':\n",
    "        return '00'+x[2:]\n",
    "    if x[0:2] == '25':\n",
    "        return '01'+x[2:]\n",
    "    return x\n",
    "    \n",
    "stop_times_fp = 'data/GTFS/OCT2021/stop_times.txt'\n",
    "stop_times_df = pd.read_csv(stop_times_fp)\n",
    "# stop_times_df.query(\"trip_id == 264733\")\n",
    "stop_times_df['date'] = DATE\n",
    "stop_times_df['arrival_time'] = stop_times_df['arrival_time'].apply(lambda x: fix_time(x))\n",
    "stop_times_df['scheduled_time'] = pd.to_datetime(stop_times_df['date'] + ' ' + stop_times_df['arrival_time'])\n",
    "\n",
    "stop_times_df['key_pair'] = list(zip(stop_times_df.trip_id, stop_times_df.stop_id, stop_times_df.scheduled_time))\n",
    "stop_times_df = stop_times_df.set_index('key_pair')\n",
    "\n",
    "time_point_dict = stop_times_df.drop(['arrival_time', 'departure_time', 'stop_id', 'stop_sequence', 'stop_headsign', 'trip_id',\n",
    "                                      'pickup_type', 'drop_off_type', 'shape_dist_traveled', 'scheduled_time', 'date'], axis=1).to_dict('index')\n",
    "with open(f'results/time_point_dict_{DATE.replace(\"-\", \"\")}.pkl', 'wb') as handle:\n",
    "    pickle.dump(time_point_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# time_point_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, tp) for k, tp in time_point_dict.items() if k[0] == 263558][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_point_dict[(263558, 'GALBERNN', pd.Timestamp('2021-10-18 05:47:59'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT: \n",
    "* Copy these to `scenarios/baselines/data`\n",
    "    * results/sampled_ons_offs_dict\n",
    "    * results/chains/ons_offs_dict_chain_{chain}.pkl\n",
    "    * results/trip_plan.json\n",
    "    * results/vehicle_plan.json\n",
    "    * results/time_point_dict.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure:\n",
    "```\n",
    "{key:val}\n",
    "key: (tuple) (route_id_dir, block_abbr, stop_sequene, stop_id, scheduled_arrival_time)\n",
    "val: (dict) {'sampled_loads': A, 'ons': B, 'offs': C}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity check\n",
    "CHAINS = 5\n",
    "for chain in range(CHAINS):\n",
    "    with open(f'results/chains/ons_offs_dict_chain_{chain}.pkl', 'rb') as handle:\n",
    "        sampled_ons_offs_dict = pickle.load(handle)\n",
    "    # res = sampled_ons_offs_dict[('7_TO DOWNTOWN', 5692, 20, 'MCC5_9', pd.Timestamp('2021-08-23 14:39:00'))]\n",
    "    res = sampled_ons_offs_dict[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', pd.Timestamp('2021-10-18 14:15:00'))]\n",
    "    print(f\"chain {chain}: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sampled_ons_offs_dict.keys())[0], list(sampled_ons_offs_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "with open(f'results/chains/ons_offs_dict_chain_0.pkl', 'rb') as handle:\n",
    "    sampled_ons_offs_dict = pickle.load(handle)\n",
    "# ('7_TO DOWNTOWN', 5692, 1, 'HBHS', datetime.datetime(2021, 8, 23, 14, 9))\n",
    "# sampled_ons_offs_dict[('7_TO DOWNTOWN', 5692, 1, 'HBHS', dt.datetime(2021, 8, 23, 14, 9))]\n",
    "search_key = ('7_TO DOWNTOWN', 5692, 5)\n",
    "values = [value for key, value in sampled_ons_offs_dict.items() if search_key == key[:len(search_key)]]\n",
    "keys = [key for key, value in sampled_ons_offs_dict.items() if search_key == key[:len(search_key)]]\n",
    "values, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sampled_ons_offs_dict.keys())[0], list(sampled_ons_offs_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sampled_ons_offs_dict[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))]\n",
    "('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))\n",
    "('14_FROM DOWNTOWN', '1400', 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))\n",
    "# ('14_FROM DOWNTOWN', '1400', 1, 'MCC4_20', datetime.datetime(2021, 8, 23, 14, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 263159\n",
    "import pandas as pd\n",
    "\n",
    "fp = 'results/sampled_ons_offs_dict_20211018.pkl'\n",
    "df = pd.read_pickle(fp)\n",
    "list(df.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = '55_FROM DOWNTOWN'\n",
    "sid = 'MCC4_15'\n",
    "time = '2021-10-18 15:35:00'\n",
    "\n",
    "df[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', pd.Timestamp('2021-10-18 14:15:00'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, v)for k, v in df.items() if k[0] == rid and k[3] == sid and k[4] == pd.Timestamp(time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = '/home/jptalusan/gits/mta_simulator_redo/data_generation/results/sampled_ons_offs_dict_20220305.pkl'\n",
    "\n",
    "with open(fp, 'rb') as handle:\n",
    "    sampled_ons_offs_dict = pickle.load(handle)\n",
    "sampled_ons_offs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
