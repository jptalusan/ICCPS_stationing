{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation\n",
    "* Generates data for a desired date based on the available APC data and passed through the model for load prediction.\n",
    "* Will provide a distribution of bins which can be used for stochasticity\n",
    "## Generates the following files:\n",
    "* `trip_plan.json`\n",
    "* `vehicle_plan.json`\n",
    "* `sampled_loads.pkl`\n",
    "* `chains.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n",
      "22/12/06 17:50:20 WARN Utils: Your hostname, scope-vanderbilt resolves to a loopback address: 127.0.1.1; using 10.2.218.69 instead (on interface enp8s0)\n",
      "22/12/06 17:50:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/06 17:50:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/06 17:50:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import sys\n",
    "import datetime as dt\n",
    "import importlib\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "import IPython\n",
    "from copy import deepcopy\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import swifter\n",
    "pd.set_option('display.max_columns', None)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '80g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '40g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "        .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "        .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apc_data_for_date(filter_date):\n",
    "    print(\"Running this...\")\n",
    "    filepath = '/home/jptalusan/mta_stationing_problem/data/processed/apc_weather_gtfs_20220921.parquet'\n",
    "    apcdata = spark.read.load(filepath)\n",
    "    apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "    plot_date = filter_date.strftime('%Y-%m-%d')\n",
    "    get_columns = ['trip_id', 'transit_date', 'arrival_time', 'scheduled_time',\n",
    "                'block_abbr', 'stop_sequence', 'stop_id_original',\n",
    "                'vehicle_id', 'vehicle_capacity',\n",
    "                'load', \n",
    "                'darksky_temperature', \n",
    "                'darksky_humidity', \n",
    "                'darksky_precipitation_probability', \n",
    "                'route_direction_name', 'route_id', 'overload_id',\n",
    "                'dayofweek',  'year', 'month', 'hour', 'zero_load_at_trip_end',\n",
    "                'sched_hdwy']\n",
    "    get_str = \", \".join([c for c in get_columns])\n",
    "    query = f\"\"\"\n",
    "    SELECT {get_str}\n",
    "    FROM apc\n",
    "    WHERE (transit_date == '{plot_date}')\n",
    "    ORDER BY arrival_time\n",
    "    \"\"\"\n",
    "    apcdata = spark.sql(query)\n",
    "    apcdata = apcdata.withColumn(\"route_id_dir\", F.concat_ws(\"_\", apcdata.route_id, apcdata.route_direction_name))\n",
    "    apcdata = apcdata.withColumn(\"day\", F.dayofmonth(apcdata.arrival_time))\n",
    "    apcdata = apcdata.drop(\"route_direction_name\")\n",
    "    apcdata = apcdata.withColumn(\"load\", F.when(apcdata.load < 0, 0).otherwise(apcdata.load))\n",
    "    apcdata = apcdata.na.fill(value=0,subset=[\"zero_load_at_trip_end\"])\n",
    "    return apcdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(input_df, ohe_encoder, label_encoders, num_scaler, columns, keep_columns=[], target='y_class'):\n",
    "    num_columns = ['darksky_temperature', 'darksky_humidity', 'darksky_precipitation_probability', 'sched_hdwy']\n",
    "    cat_columns = ['month', 'hour', 'day', 'stop_sequence', 'stop_id_original', 'year', 'time_window']\n",
    "    ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end']\n",
    "\n",
    "    # OHE\n",
    "    input_df[ohe_encoder.get_feature_names_out()] = ohe_encoder.transform(input_df[ohe_columns]).toarray()\n",
    "    # input_df = input_df.drop(columns=ohe_columns)\n",
    "\n",
    "    # Label encoder\n",
    "    for cat in cat_columns:\n",
    "        print(cat)\n",
    "        encoder = label_encoders[cat]\n",
    "        input_df[cat] = encoder.transform(input_df[cat])\n",
    "    \n",
    "    # Num scaler\n",
    "    input_df[num_columns] = num_scaler.transform(input_df[num_columns])\n",
    "    input_df['y_class']  = input_df.y_class.astype('int')\n",
    "\n",
    "    if keep_columns:\n",
    "        columns = keep_columns + columns\n",
    "    # Rearrange columns\n",
    "    input_df = input_df[columns]\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def assign_data_to_bins(df, TARGET='load'):\n",
    "    bins = pd.IntervalIndex.from_tuples([(-1, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)])\n",
    "    mycut = pd.cut(df[TARGET].tolist(), bins=bins)\n",
    "    df['y_class'] = mycut.codes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEWINDOW = 15\n",
    "def add_features(df):\n",
    "    df = df[df.arrival_time.notna()]\n",
    "    df = df.fillna(method=\"bfill\")\n",
    "\n",
    "    df['day'] = df[\"arrival_time\"].dt.day\n",
    "    df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "    # Adding extra features\n",
    "    # Holidays\n",
    "    fp = os.path.join('data', 'US Holiday Dates (2004-2021).csv')\n",
    "    holidays_df = pd.read_csv(fp)\n",
    "    holidays_df['Date'] = pd.to_datetime(holidays_df['Date'])\n",
    "    holidays_df['is_holiday'] = True\n",
    "    df = df.merge(holidays_df[['Date', 'is_holiday']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_holiday'] = df['is_holiday'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "        \n",
    "    # School breaks\n",
    "    fp = os.path.join('data', 'School Breaks (2019-2022).pkl')\n",
    "    school_break_df = pd.read_pickle(fp)\n",
    "    school_break_df['is_school_break'] = True\n",
    "    df = df.merge(school_break_df[['Date', 'is_school_break']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_school_break'] = df['is_school_break'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "\n",
    "    df['minute'] = df['arrival_time'].dt.minute\n",
    "    df['minuteByWindow'] = df['minute'] // TIMEWINDOW\n",
    "    df['temp'] = df['minuteByWindow'] + (df['hour'] * 60 / TIMEWINDOW)\n",
    "    df['time_window'] = np.floor(df['temp']).astype('int')\n",
    "    df = df.drop(columns=['minute', 'minuteByWindow', 'temp'])\n",
    "\n",
    "    # HACK\n",
    "    # df = df[df['hour'] != 3]\n",
    "    # df = df[df['stop_sequence'] != 0]\n",
    "\n",
    "    df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "    df = assign_data_to_bins(df, TARGET='load')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_simple_lstm_generator(num_features, num_classes, learning_rate=1e-4):\n",
    "    # define model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "\n",
    "    input_shape = (None, None, num_features)\n",
    "    model.build(input_shape)\n",
    "    return model\n",
    "\n",
    "def generate_simple_lstm_predictions(input_df, model, past, future):\n",
    "    past_df = input_df[0:past]\n",
    "    future_df = input_df[past:]\n",
    "    predictions = []\n",
    "    pred_probs = []\n",
    "    if future == None:\n",
    "        future = len(future_df)\n",
    "    for f in range(future):\n",
    "        pred = model.predict(past_df.to_numpy().reshape(1, *past_df.shape))\n",
    "        pred_probs.append(pred)\n",
    "        y_pred = np.argmax(pred)\n",
    "        predictions.append(y_pred)\n",
    "        \n",
    "        # Add information from future\n",
    "        last_row = future_df.iloc[[0]]\n",
    "        last_row['y_class'] = y_pred\n",
    "        past_df = pd.concat([past_df[1:], last_row])\n",
    "        \n",
    "        # Move future to remove used row\n",
    "        future_df = future_df[1:]\n",
    "    return predictions, pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overload_regular_bus_trips(regular, overload):\n",
    "    m = regular.merge(overload, how='left', on=['trip_id', 'transit_date', 'scheduled_time', 'block_abbr', 'stop_sequence', 'stop_id_original', 'route_id_dir', 'route_id'])\n",
    "    \n",
    "    m['arrival_time'] = np.max(m[['arrival_time_x', 'arrival_time_y']], axis=1)\n",
    "    \n",
    "    m['zero_load_at_trip_end'] = m['zero_load_at_trip_end_x']\n",
    "    \n",
    "    m.loc[~m['arrival_time_x'].isnull(), \"load\"] = m['load_x']\n",
    "    # m.loc[~m['arrival_time_x'].isnull(), \"ons\"] = m['ons_x']\n",
    "    # m.loc[~m['arrival_time_x'].isnull(), \"offs\"] = m['offs_x']\n",
    "    \n",
    "    m.loc[~m['arrival_time_y'].isnull(), \"load\"] = m['load_y']\n",
    "    # m.loc[~m['arrival_time_y'].isnull(), \"ons\"] = m['ons_y']\n",
    "    # m.loc[~m['arrival_time_y'].isnull(), \"offs\"] = m['offs_y']\n",
    "    \n",
    "    m['vehicle_id'] = m['vehicle_id_x']\n",
    "    m['vehicle_capacity'] = m['vehicle_capacity_x']\n",
    "    m['overload_id'] = m['overload_id_x']\n",
    "    m = m[m.columns.drop(list(m.filter(regex='_x')))]\n",
    "    m = m[m.columns.drop(list(m.filter(regex='_y')))]\n",
    "    # m = m[regular.columns]\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "latest = tf.train.latest_checkpoint('models/no_speed')\n",
    "columns = joblib.load('models/LL_X_columns.joblib')\n",
    "label_encoders = joblib.load('models/LL_Label_encoders.joblib')\n",
    "ohe_encoder = joblib.load('models/LL_OHE_encoder.joblib')\n",
    "num_scaler = joblib.load('models/LL_Num_scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '2021-03-05'\n",
    "start_time = '08:00:00'\n",
    "end_time = '12:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running this...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 02:41:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2021-10-18, 2021-11-23, 2021-12-15, 2022-01-\n",
    "date_to_predict = dt.datetime.strptime(DATE, '%Y-%m-%d')\n",
    "apcdata = get_apc_data_for_date(date_to_predict)\n",
    "df = apcdata.toPandas()\n",
    "\n",
    "# HACK\n",
    "# a = df.query(\"trip_id == '233300' and vehicle_id == '722'\").sort_values('stop_sequence')\n",
    "# b = df.query(\"trip_id == '233300' and vehicle_id == '1830'\").sort_values('stop_sequence')\n",
    "# m1 = merge_overload_regular_bus_trips(a, b)\n",
    "\n",
    "# a = df.query(\"trip_id == '259635' and vehicle_id == '2019'\").sort_values('stop_sequence')\n",
    "# b = df.query(\"trip_id == '259635' and vehicle_id == '1914'\").sort_values('stop_sequence')\n",
    "# m2 = merge_overload_regular_bus_trips(a, b)\n",
    "\n",
    "df = df.query(\"overload_id == 0\")\n",
    "# overload_trips = df.query(\"overload_id > 0\").trip_id.unique()\n",
    "# df = df[~df['trip_id'].isin(overload_trips)]\n",
    "# df = pd.concat([tdf, m1])\n",
    "df = df.dropna(subset=['arrival_time'])\n",
    "# df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# HACK\n",
    "# df = df.query(\"route_id != 95\")\n",
    "# df = df[~df['stop_id_original'].isin(['PEARL', 'JOHASHEN', 'ROS10AEN'])]\n",
    "\n",
    "df = add_features(df)\n",
    "raw_df = deepcopy(df)\n",
    "\n",
    "# HACK\n",
    "# df.loc[df['time_window'].isin([6, 7, 8]), 'time_window'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month\n",
      "hour\n",
      "day\n",
      "stop_sequence\n",
      "stop_id_original\n",
      "year\n",
      "time_window\n"
     ]
    }
   ],
   "source": [
    "input_df = prepare_input_data(df, ohe_encoder, label_encoders, num_scaler, columns, target='y_class')\n",
    "ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end']\n",
    "input_df = input_df.drop(columns=ohe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 20:05:23.424372: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 20:05:23.878143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11402 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:0b:00.0, compute capability: 6.1\n",
      "  0%|          | 0/1005 [00:00<?, ?it/s]2022-11-30 20:05:25.237817: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8401\n",
      "100%|██████████| 1005/1005 [14:42<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "tf.keras.backend.clear_session()\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "FUTURE = None\n",
    "PAST = 5\n",
    "\n",
    "NUM_TRIPS = None\n",
    "if NUM_TRIPS == None:\n",
    "    rand_trips = df.trip_id.unique().tolist()\n",
    "else:\n",
    "    rand_trips = random.sample(df.trip_id.unique().tolist(), NUM_TRIPS)\n",
    "\n",
    "model = setup_simple_lstm_generator(input_df.shape[1], NUM_CLASSES)\n",
    "model.load_weights(latest)\n",
    "\n",
    "trip_res = []\n",
    "load_arr = []\n",
    "for trip_id in tqdm(rand_trips):\n",
    "    _df = df.query(\"trip_id == @trip_id\")\n",
    "    try:\n",
    "        _input_df = input_df.loc[_df.index]\n",
    "        _y_pred, y_pred_probs = generate_simple_lstm_predictions(_input_df, model, PAST, FUTURE)\n",
    "        \n",
    "        # Introducing stochasticity\n",
    "        y_pred = [np.random.choice(len(ypp.flatten()), size=1, p=ypp.flatten())[0] for ypp in y_pred_probs]\n",
    "        loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in y_pred]\n",
    "        \n",
    "        _raw_df = raw_df.loc[_df.index]\n",
    "        y_true = _raw_df[0:PAST]['load'].tolist()\n",
    "        a = y_true + loads\n",
    "        _raw_df['sampled_loads'] = a\n",
    "        \n",
    "        y_true_classes = _raw_df[0:PAST]['y_class'].tolist()\n",
    "        _raw_df['y_pred_classes'] = y_true_classes + y_pred\n",
    "        _raw_df['y_pred_probs'] = [[-1] * NUM_CLASSES]*len(y_true_classes) + [ypp[0] for ypp in y_pred_probs]\n",
    "        \n",
    "        trip_res.append(_raw_df)\n",
    "    except:\n",
    "        print(f\"FAILED:{trip_id}\")\n",
    "        continue\n",
    "\n",
    "trip_res = pd.concat(trip_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_columns = ['trip_id', 'transit_date', 'arrival_time', 'scheduled_time', 'block_abbr', \n",
    "            'stop_sequence', 'stop_id_original', 'route_id_dir', 'zero_load_at_trip_end', \n",
    "            'y_pred_classes', 'y_pred_probs', 'sampled_loads', 'vehicle_id', 'vehicle_capacity']\n",
    "_trip_res = trip_res[_columns]\n",
    "\n",
    "# fp = 'results/sampled_loads.pkl'\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "_trip_res.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching with GTFS time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     trip_df\u001b[39m.\u001b[39mloc[trip_df\u001b[39m.\u001b[39mindex[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mtimepoint\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     trip_res_arr\u001b[39m.\u001b[39mappend(trip_df)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m trip_res_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(trip_res_arr)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# trip_res_df.to_pickle(fp)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m trip_res_df\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/reshape/concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    156\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m    along the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    348\u001b[0m         objs,\n\u001b[1;32m    349\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    350\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    351\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    352\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    353\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    354\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    355\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    356\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    357\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    360\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/reshape/concat.py:404\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    401\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[1;32m    403\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# # fp = 'results/sampled_loads.pkl'\n",
    "# # trip_res_df = pd.read_pickle(fp)\n",
    "# # trip_res_df['trip_id'] = trip_res_df['trip_id'].astype('int')\n",
    "# trip_res_df = _trip_res\n",
    "\n",
    "# trip_res_df = pd.merge(trip_res_df, raw_df[['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original']], \n",
    "#                        left_on=['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original'], \n",
    "#                        right_on=['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original'], how='left')\n",
    "# trip_res_df['trip_id'] = trip_res_df['trip_id'].astype('int')\n",
    "\n",
    "# # print(trip_res_df.shape)\n",
    "# stop_times_fp = 'data/GTFS/OCT2021/stop_times.txt'\n",
    "# stop_times_df = pd.read_csv(stop_times_fp)\n",
    "# # stop_times_df.query(\"trip_id == 264733\")\n",
    "\n",
    "# trip_res_df = pd.merge(trip_res_df, stop_times_df[['trip_id', 'stop_id', 'timepoint']], left_on=['trip_id', 'stop_id_original'], right_on=['trip_id', 'stop_id'])\n",
    "# trip_res_df.query(\"trip_id == 264733\")\n",
    "# trip_res_df = trip_res_df.drop_duplicates(subset=['trip_id', 'stop_id_original', 'arrival_time', 'scheduled_time'])\n",
    "\n",
    "# trip_res_arr = []\n",
    "# for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "#     trip_df.loc[trip_df.index[-1], 'timepoint']= 1.0\n",
    "#     trip_res_arr.append(trip_df)\n",
    "    \n",
    "# trip_res_df = pd.concat(trip_res_arr)\n",
    "\n",
    "# # fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# # trip_res_df.to_pickle(fp)\n",
    "# trip_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vehicle assignments here...\n",
    "* Trying to limit to a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '2021-03-05'\n",
    "vehicle_list = ['1761', '1804', '1807', '1825', '1826', '1906', '2004', '2008', '2011', '722']\n",
    "start_time = '06:00:00'\n",
    "end_time = '10:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7fdd08a9ee20>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res_df = pd.read_pickle(fp)\n",
    "trip_res_df['hour'] = trip_res_df.scheduled_time.dt.hour\n",
    "trip_res_df['count'] = 1\n",
    "ax = trip_res_df.groupby('trip_id').agg({\"hour\":\"first\", \"count\":\"count\"}).groupby(\"hour\").count().plot(kind='line')\n",
    "ax.axvline(x=6, ymin=0, ymax=100, ls='--')\n",
    "ax.axvline(x=10, ymin=0, ymax=100, ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(247,)\n",
      "(10,)\n",
      "(58,)\n",
      "(10,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>transit_date</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>scheduled_time</th>\n",
       "      <th>block_abbr</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>stop_id_original</th>\n",
       "      <th>route_id_dir</th>\n",
       "      <th>zero_load_at_trip_end</th>\n",
       "      <th>y_pred_classes</th>\n",
       "      <th>y_pred_probs</th>\n",
       "      <th>sampled_loads</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>vehicle_capacity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7558</th>\n",
       "      <td>229632</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 06:04:54</td>\n",
       "      <td>2021-03-05 06:15:00</td>\n",
       "      <td>1701</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC5_11</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1825</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7559</th>\n",
       "      <td>229632</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 06:18:08</td>\n",
       "      <td>2021-03-05 06:18:02</td>\n",
       "      <td>1701</td>\n",
       "      <td>3</td>\n",
       "      <td>11ACHASF</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1825</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7560</th>\n",
       "      <td>229632</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 06:19:56</td>\n",
       "      <td>2021-03-05 06:19:00</td>\n",
       "      <td>1701</td>\n",
       "      <td>5</td>\n",
       "      <td>11APORSF</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1825</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7561</th>\n",
       "      <td>229632</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 06:20:40</td>\n",
       "      <td>2021-03-05 06:19:40</td>\n",
       "      <td>1701</td>\n",
       "      <td>6</td>\n",
       "      <td>11ADEMSF</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1825</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7562</th>\n",
       "      <td>229632</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 06:21:20</td>\n",
       "      <td>2021-03-05 06:19:59</td>\n",
       "      <td>1701</td>\n",
       "      <td>7</td>\n",
       "      <td>11ALAUSF</td>\n",
       "      <td>17_FROM DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1825</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5204</th>\n",
       "      <td>233337</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 07:03:24</td>\n",
       "      <td>2021-03-05 06:59:35</td>\n",
       "      <td>800</td>\n",
       "      <td>22</td>\n",
       "      <td>8ABRONN</td>\n",
       "      <td>8_TO DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.942109, 0.04840789, 0.009483038, 1.13967715...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>722</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5205</th>\n",
       "      <td>233337</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 07:03:49</td>\n",
       "      <td>2021-03-05 07:00:00</td>\n",
       "      <td>800</td>\n",
       "      <td>23</td>\n",
       "      <td>8ABRONM</td>\n",
       "      <td>8_TO DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.94541323, 0.045744333, 0.008842383, 8.72595...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>722</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5206</th>\n",
       "      <td>233337</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 07:04:12</td>\n",
       "      <td>2021-03-05 07:02:40</td>\n",
       "      <td>800</td>\n",
       "      <td>24</td>\n",
       "      <td>8AVUNINN</td>\n",
       "      <td>8_TO DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9446957, 0.046234503, 0.009069684, 1.005406...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>722</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5207</th>\n",
       "      <td>233337</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 07:04:58</td>\n",
       "      <td>2021-03-05 07:04:39</td>\n",
       "      <td>800</td>\n",
       "      <td>25</td>\n",
       "      <td>CHA7AEN</td>\n",
       "      <td>8_TO DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.92541903, 0.062254336, 0.012326358, 2.77728...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>722</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5209</th>\n",
       "      <td>233337</td>\n",
       "      <td>2021-03-05</td>\n",
       "      <td>2021-03-05 07:06:28</td>\n",
       "      <td>2021-03-05 07:07:00</td>\n",
       "      <td>800</td>\n",
       "      <td>26</td>\n",
       "      <td>MCC5_3</td>\n",
       "      <td>8_TO DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.7319436, 0.21221358, 0.055806562, 3.6283313...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>722</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1526 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     trip_id transit_date        arrival_time      scheduled_time  block_abbr  \\\n",
       "7558  229632   2021-03-05 2021-03-05 06:04:54 2021-03-05 06:15:00        1701   \n",
       "7559  229632   2021-03-05 2021-03-05 06:18:08 2021-03-05 06:18:02        1701   \n",
       "7560  229632   2021-03-05 2021-03-05 06:19:56 2021-03-05 06:19:00        1701   \n",
       "7561  229632   2021-03-05 2021-03-05 06:20:40 2021-03-05 06:19:40        1701   \n",
       "7562  229632   2021-03-05 2021-03-05 06:21:20 2021-03-05 06:19:59        1701   \n",
       "...      ...          ...                 ...                 ...         ...   \n",
       "5204  233337   2021-03-05 2021-03-05 07:03:24 2021-03-05 06:59:35         800   \n",
       "5205  233337   2021-03-05 2021-03-05 07:03:49 2021-03-05 07:00:00         800   \n",
       "5206  233337   2021-03-05 2021-03-05 07:04:12 2021-03-05 07:02:40         800   \n",
       "5207  233337   2021-03-05 2021-03-05 07:04:58 2021-03-05 07:04:39         800   \n",
       "5209  233337   2021-03-05 2021-03-05 07:06:28 2021-03-05 07:07:00         800   \n",
       "\n",
       "      stop_sequence stop_id_original      route_id_dir  zero_load_at_trip_end  \\\n",
       "7558              1          MCC5_11  17_FROM DOWNTOWN                      0   \n",
       "7559              3         11ACHASF  17_FROM DOWNTOWN                      0   \n",
       "7560              5         11APORSF  17_FROM DOWNTOWN                      0   \n",
       "7561              6         11ADEMSF  17_FROM DOWNTOWN                      0   \n",
       "7562              7         11ALAUSF  17_FROM DOWNTOWN                      0   \n",
       "...             ...              ...               ...                    ...   \n",
       "5204             22          8ABRONN     8_TO DOWNTOWN                      0   \n",
       "5205             23          8ABRONM     8_TO DOWNTOWN                      0   \n",
       "5206             24         8AVUNINN     8_TO DOWNTOWN                      0   \n",
       "5207             25          CHA7AEN     8_TO DOWNTOWN                      0   \n",
       "5209             26           MCC5_3     8_TO DOWNTOWN                      0   \n",
       "\n",
       "      y_pred_classes                                       y_pred_probs  \\\n",
       "7558               0                               [-1, -1, -1, -1, -1]   \n",
       "7559               0                               [-1, -1, -1, -1, -1]   \n",
       "7560               0                               [-1, -1, -1, -1, -1]   \n",
       "7561               0                               [-1, -1, -1, -1, -1]   \n",
       "7562               0                               [-1, -1, -1, -1, -1]   \n",
       "...              ...                                                ...   \n",
       "5204               1  [0.942109, 0.04840789, 0.009483038, 1.13967715...   \n",
       "5205               0  [0.94541323, 0.045744333, 0.008842383, 8.72595...   \n",
       "5206               0  [0.9446957, 0.046234503, 0.009069684, 1.005406...   \n",
       "5207               0  [0.92541903, 0.062254336, 0.012326358, 2.77728...   \n",
       "5209               0  [0.7319436, 0.21221358, 0.055806562, 3.6283313...   \n",
       "\n",
       "      sampled_loads vehicle_id  vehicle_capacity  \n",
       "7558            4.0       1825              40.0  \n",
       "7559            4.0       1825              40.0  \n",
       "7560            4.0       1825              40.0  \n",
       "7561            5.0       1825              40.0  \n",
       "7562            5.0       1825              40.0  \n",
       "...             ...        ...               ...  \n",
       "5204            7.0        722              40.0  \n",
       "5205            6.0        722              40.0  \n",
       "5206            3.0        722              45.0  \n",
       "5207            4.0        722              40.0  \n",
       "5209            2.0        722              40.0  \n",
       "\n",
       "[1526 rows x 14 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# Vehicle assignments\n",
    "# Each vehicle config is a dict: {vehicle_capacity, blocks}\n",
    "DEFAULT_CAPACITY = 40.0\n",
    "overall_vehicle_plan = {}\n",
    "\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res_df = pd.read_pickle(fp)\n",
    "trip_res_df = trip_res_df[trip_res_df['vehicle_id'].isin(vehicle_list)]\n",
    "print(trip_res_df.trip_id.unique().shape)\n",
    "print(trip_res_df.vehicle_id.unique().shape)\n",
    "\n",
    "start_datetime = dt.datetime.strptime(f\"{DATE} {start_time}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "end_datetime = dt.datetime.strptime(f\"{DATE} {end_time}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "arr = []\n",
    "for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "    if (trip_df.scheduled_time.min() >= start_datetime) and (trip_df.scheduled_time.max() <= end_datetime):\n",
    "        arr.append(trip_df)\n",
    "\n",
    "trip_res_df = pd.concat(arr)\n",
    "print(trip_res_df.trip_id.unique().shape)\n",
    "print(trip_res_df.vehicle_id.unique().shape)\n",
    "trip_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: run again with vehicle_capacity (above)\n",
    "for vehicle_id, vehicle_df in trip_res_df.groupby('vehicle_id'):\n",
    "    vehicle_df = vehicle_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    vehicle_capacity = vehicle_df.iloc[0].vehicle_capacity\n",
    "    # vehicle_capacity = DEFAULT_CAPACITY\n",
    "    if np.isnan(vehicle_capacity):\n",
    "        vehicle_capacity = DEFAULT_CAPACITY\n",
    "    # TODO: This is not the baseline behavior\n",
    "    starting_depot = 'MCC5_1'\n",
    "    service_type = 'regular'\n",
    "    blocks = [block for block in vehicle_df.block_abbr.unique().tolist()]\n",
    "    trips = []\n",
    "    for block in blocks:\n",
    "        block_df = vehicle_df.query(\"block_abbr == @block\")\n",
    "        for trip in block_df.trip_id.unique().tolist():\n",
    "            trips.append((str(block), str(trip)))\n",
    "    overall_vehicle_plan[vehicle_id] = {'vehicle_capacity': vehicle_capacity, 'trips': trips, 'starting_depot': starting_depot, 'service_type': service_type}\n",
    "    \n",
    "len(overall_vehicle_plan)\n",
    "\n",
    "# Number of overload buses\n",
    "#   \"42\": {\n",
    "#     \"service_type\": \"overload\",\n",
    "#     \"starting_depot\": \"MCC5_1\",\n",
    "#     \"trips\": [\n",
    "#     ],\n",
    "#     \"vehicle_capacity\": 55.0\n",
    "#   }\n",
    "OVERLOAD_BUSES = 5\n",
    "for vehicle_id in range(41, 41 + OVERLOAD_BUSES):\n",
    "    overall_vehicle_plan[str(vehicle_id)] = {'vehicle_capacity': 55.0, 'trips': [], \"starting_depot\": \"MCC5_1\", 'service_type': \"overload\"}\n",
    "    \n",
    "with open(f'results/vehicle_plan_{DATE.replace(\"-\", \"\")}_10_limited.json', 'w') as fp:\n",
    "    json.dump(overall_vehicle_plan, fp, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Trip plan (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res_df = pd.read_pickle(fp)\n",
    "\n",
    "# Create a dict of {[block: {trip_ids:[]}, 'block'....]}\n",
    "# trip_id dict = {'route_id', route_direction_name', 'stop_id':[], 'schedule_time':[]}\n",
    "# Use block as grouper in baseline\n",
    "overall_block_plan = {}\n",
    "for block_abbr, block_df in trip_res_df.groupby('block_abbr'):\n",
    "    block_df = block_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    trip_ids = block_df.trip_id.unique().tolist()\n",
    "    start_time = block_df[block_df['trip_id'] == trip_ids[0]].iloc[0]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    end_time = block_df[block_df['trip_id'] == trip_ids[-1]].iloc[-1]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    overall_block_plan[block_abbr] = {'trip_ids': trip_ids,\n",
    "                                      'start_time': start_time,\n",
    "                                      'end_time': end_time}\n",
    "\n",
    "overall_trip_plan = {}\n",
    "for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "    trip_df = trip_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    route_id_dir = trip_df.iloc[0].route_id_dir\n",
    "    route_id = int(route_id_dir.split(\"_\")[0])\n",
    "    route_direction = route_id_dir.split(\"_\")[1]\n",
    "    zero_load_at_trip_end = trip_df.iloc[-1].zero_load_at_trip_end.tolist()\n",
    "    scheduled_time = trip_df.scheduled_time.dt.strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "    stop_sequence = trip_df.stop_sequence.tolist()\n",
    "    stop_sequence = list(range(0, len(stop_sequence)))\n",
    "    # stop_sequence = [ss - 1 for ss in stop_sequence]\n",
    "    stop_id_original = trip_df.stop_id_original.tolist()\n",
    "    \n",
    "    overall_trip_plan[trip_id] = {'route_id': route_id, \n",
    "                                  'route_direction': route_direction, \n",
    "                                  'scheduled_time': scheduled_time, \n",
    "                                  'stop_sequence': stop_sequence, \n",
    "                                  'stop_id_original': stop_id_original,\n",
    "                                  'zero_load_at_trip_end':zero_load_at_trip_end,\n",
    "                                  'last_stop_sequence': stop_sequence[-1],\n",
    "                                  'last_stop_id': stop_id_original[-1]}\n",
    "\n",
    "len(overall_trip_plan), len(overall_block_plan)\n",
    "\n",
    "with open(f'results/trip_plan_{DATE.replace(\"-\", \"\")}_10_limited.json', 'w') as fp:\n",
    "    json.dump(overall_trip_plan, fp, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trip_res_df.query(\"trip_id == 259274\").head())\n",
    "print(trip_res_df.query(\"trip_id == 259274\").shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.query(\"trip_id == '243423'\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_id_dir = \n",
    "# trip_res.query(\"route_id_dir == @route_id_dir and block_abbr == @block and stop_id_original == @stop_id_original[@i] and scheduled_time == @scheduled_time[@i]\").iloc[0]['sampled_loads']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ons and offs from sampled loads\n",
    "* Needs the trip_res generated above\n",
    " ```\n",
    " fp = 'results/sampled_loads.pkl'\n",
    " trip_res.to_pickle(fp)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "    \n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "sampled_ons_offs = []\n",
    "for trip_id, trip_id_df in tqdm(trip_res.groupby(['transit_date', 'trip_id'])):\n",
    "    tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "    tdf['ons'] = 0\n",
    "    tdf['offs'] = 0\n",
    "    tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "    \n",
    "    # Intermediate stops\n",
    "    tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # first and last stops\n",
    "    tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "    tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "    sampled_ons_offs.append(tdf)\n",
    "    \n",
    "sampled_ons_offs = pd.concat(sampled_ons_offs)\n",
    "sampled_ons_offs = sampled_ons_offs.drop('next_load', axis=1)\n",
    "\n",
    "# fp = f'results/sampled_ons_offs_{DATE.replace(\"-\", \"\")}.pkl'\n",
    "# sampled_ons_offs.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a single event chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in trip_res.y_pred_classes]\n",
    "trip_res['sampled_loads'] = loads\n",
    "\n",
    "sampled_ons_offs = []\n",
    "for trip_id, trip_id_df in tqdm(trip_res.groupby(['transit_date', 'trip_id'])):\n",
    "    tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "    tdf['stop_sequence'] = list(range(1, len(tdf) + 1))\n",
    "    tdf['ons'] = 0\n",
    "    tdf['offs'] = 0\n",
    "    tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "    \n",
    "    # Intermediate stops\n",
    "    tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # first and last stops\n",
    "    tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "    tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "    sampled_ons_offs.append(tdf)\n",
    "    \n",
    "df = pd.concat(sampled_ons_offs)\n",
    "df = df.drop('next_load', axis=1)\n",
    "\n",
    "display(df)\n",
    "df['key_pair'] = list(zip(df.route_id_dir, \n",
    "                          df.block_abbr,\n",
    "                          df.stop_sequence,\n",
    "                          df.stop_id_original, \n",
    "                          df.scheduled_time))\n",
    "df = df.set_index('key_pair')\n",
    "drop_cols = ['trip_id', 'route_id_dir', 'block_abbr', 'stop_id_original', 'stop_id', 'scheduled_time', \n",
    "                'transit_date', 'arrival_time', 'zero_load_at_trip_end', 'y_pred_classes', 'y_pred_probs',\n",
    "                'vehicle_capacity', 'vehicle_id', 'stop_sequence']\n",
    "drop_cols = [dc for dc in drop_cols if dc in df.columns]\n",
    "df = df.drop(drop_cols, axis=1)\n",
    "sampled_ons_offs_dict = df.to_dict('index')\n",
    "\n",
    "import pickle \n",
    "\n",
    "# with open(f'results/sampled_ons_offs_dict_{DATE.replace(\"-\", \"\")}.pkl', 'wb') as handle:\n",
    "#     pickle.dump(sampled_ons_offs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[('23_FROM DOWNTOWN', 2310, 'DWMRT', pd.Timestamp('2021-08-23 05:41:00'))]\n",
    "df.query(\"route_id_dir == '23_FROM DOWNTOWN' and block_abbr == 2310 and stop_id_original == 'DWMRT' and scheduled_time == '2021-08-23 05:41:00'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = list(sampled_ons_offs_dict.keys())[0]\n",
    "print(key)\n",
    "sampled_ons_offs_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.query(\"route_id_dir == '7_TO DOWNTOWN' and block_abbr == 5692 and stop_sequence == 20 and stop_id_original == 'MCC5_9'\")\n",
    "# df.query(\"route_id_dir == '7_TO DOWNTOWN' and block_abbr == 5692\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating multiple event chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "\n",
    "CHAINS = 5\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "for chain in tqdm(range(CHAINS)):\n",
    "    loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in trip_res.y_pred_classes]\n",
    "    trip_res['sampled_loads'] = loads\n",
    "\n",
    "    sampled_ons_offs = []\n",
    "    for trip_id, trip_id_df in trip_res.groupby(['transit_date', 'trip_id']):\n",
    "        tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "        tdf['stop_sequence'] = list(range(1, len(tdf) + 1))\n",
    "        tdf['ons'] = 0\n",
    "        tdf['offs'] = 0\n",
    "        tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "        \n",
    "        # Intermediate stops\n",
    "        tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "        \n",
    "        # first and last stops\n",
    "        tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "        tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "        sampled_ons_offs.append(tdf)\n",
    "        \n",
    "    df = pd.concat(sampled_ons_offs)\n",
    "    df['key_pair'] = list(zip(df.route_id_dir, \n",
    "                            df.block_abbr,\n",
    "                            df.stop_sequence,\n",
    "                            df.stop_id_original, \n",
    "                            df.scheduled_time))\n",
    "    df = df.set_index('key_pair')\n",
    "    drop_cols = ['trip_id', 'route_id_dir', 'block_abbr', 'stop_id_original', 'stop_id', 'scheduled_time', \n",
    "                 'transit_date', 'arrival_time', 'zero_load_at_trip_end', 'y_pred_classes', 'y_pred_probs',\n",
    "                 'vehicle_capacity', 'vehicle_id', 'stop_sequence']\n",
    "    drop_cols = [dc for dc in drop_cols if dc in df.columns]\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    sampled_ons_offs_dict = df.to_dict('index')\n",
    "\n",
    "    with open(f'results/chains/ons_offs_dict_chain_{DATE.replace(\"-\",\"\")}_{chain}.pkl', 'wb') as handle:\n",
    "        pickle.dump(sampled_ons_offs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create timepoint dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times_df.iloc[0].arrival_time[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(x):\n",
    "    if x[0:2] == '24':\n",
    "        return '00'+x[2:]\n",
    "    if x[0:2] == '25':\n",
    "        return '01'+x[2:]\n",
    "    return x\n",
    "    \n",
    "stop_times_fp = 'data/GTFS/OCT2021/stop_times.txt'\n",
    "stop_times_df = pd.read_csv(stop_times_fp)\n",
    "# stop_times_df.query(\"trip_id == 264733\")\n",
    "stop_times_df['date'] = DATE\n",
    "stop_times_df['arrival_time'] = stop_times_df['arrival_time'].apply(lambda x: fix_time(x))\n",
    "stop_times_df['scheduled_time'] = pd.to_datetime(stop_times_df['date'] + ' ' + stop_times_df['arrival_time'])\n",
    "\n",
    "stop_times_df['key_pair'] = list(zip(stop_times_df.trip_id, stop_times_df.stop_id, stop_times_df.scheduled_time))\n",
    "stop_times_df = stop_times_df.set_index('key_pair')\n",
    "\n",
    "time_point_dict = stop_times_df.drop(['arrival_time', 'departure_time', 'stop_id', 'stop_sequence', 'stop_headsign', 'trip_id',\n",
    "                                      'pickup_type', 'drop_off_type', 'shape_dist_traveled', 'scheduled_time', 'date'], axis=1).to_dict('index')\n",
    "with open(f'results/time_point_dict_{DATE.replace(\"-\", \"\")}.pkl', 'wb') as handle:\n",
    "    pickle.dump(time_point_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# time_point_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, tp) for k, tp in time_point_dict.items() if k[0] == 263558][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_point_dict[(263558, 'GALBERNN', pd.Timestamp('2021-10-18 05:47:59'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT: \n",
    "* Copy these to `scenarios/baselines/data`\n",
    "    * results/sampled_ons_offs_dict\n",
    "    * results/chains/ons_offs_dict_chain_{chain}.pkl\n",
    "    * results/trip_plan.json\n",
    "    * results/vehicle_plan.json\n",
    "    * results/time_point_dict.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure:\n",
    "```\n",
    "{key:val}\n",
    "key: (tuple) (route_id_dir, block_abbr, stop_sequene, stop_id, scheduled_arrival_time)\n",
    "val: (dict) {'sampled_loads': A, 'ons': B, 'offs': C}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity check\n",
    "CHAINS = 5\n",
    "for chain in range(CHAINS):\n",
    "    with open(f'results/chains/ons_offs_dict_chain_{chain}.pkl', 'rb') as handle:\n",
    "        sampled_ons_offs_dict = pickle.load(handle)\n",
    "    # res = sampled_ons_offs_dict[('7_TO DOWNTOWN', 5692, 20, 'MCC5_9', pd.Timestamp('2021-08-23 14:39:00'))]\n",
    "    res = sampled_ons_offs_dict[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', pd.Timestamp('2021-10-18 14:15:00'))]\n",
    "    print(f\"chain {chain}: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sampled_ons_offs_dict.keys())[0], list(sampled_ons_offs_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "with open(f'results/chains/ons_offs_dict_chain_0.pkl', 'rb') as handle:\n",
    "    sampled_ons_offs_dict = pickle.load(handle)\n",
    "# ('7_TO DOWNTOWN', 5692, 1, 'HBHS', datetime.datetime(2021, 8, 23, 14, 9))\n",
    "# sampled_ons_offs_dict[('7_TO DOWNTOWN', 5692, 1, 'HBHS', dt.datetime(2021, 8, 23, 14, 9))]\n",
    "search_key = ('7_TO DOWNTOWN', 5692, 5)\n",
    "values = [value for key, value in sampled_ons_offs_dict.items() if search_key == key[:len(search_key)]]\n",
    "keys = [key for key, value in sampled_ons_offs_dict.items() if search_key == key[:len(search_key)]]\n",
    "values, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sampled_ons_offs_dict.keys())[0], list(sampled_ons_offs_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sampled_ons_offs_dict[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))]\n",
    "('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))\n",
    "('14_FROM DOWNTOWN', '1400', 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))\n",
    "# ('14_FROM DOWNTOWN', '1400', 1, 'MCC4_20', datetime.datetime(2021, 8, 23, 14, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 263159\n",
    "import pandas as pd\n",
    "\n",
    "fp = 'results/sampled_ons_offs_dict_20211018.pkl'\n",
    "df = pd.read_pickle(fp)\n",
    "list(df.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = '55_FROM DOWNTOWN'\n",
    "sid = 'MCC4_15'\n",
    "time = '2021-10-18 15:35:00'\n",
    "\n",
    "df[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', pd.Timestamp('2021-10-18 14:15:00'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, v)for k, v in df.items() if k[0] == rid and k[3] == sid and k[4] == pd.Timestamp(time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = '/home/jptalusan/gits/mta_simulator_redo/data_generation/results/sampled_ons_offs_dict_20220305.pkl'\n",
    "\n",
    "with open(fp, 'rb') as handle:\n",
    "    sampled_ons_offs_dict = pickle.load(handle)\n",
    "sampled_ons_offs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
