{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation\n",
    "* Generates data for a desired date based on the available APC data and passed through the model for load prediction.\n",
    "* Will provide a distribution of bins which can be used for stochasticity\n",
    "## Generates the following files:\n",
    "* `trip_plan.json`\n",
    "* `vehicle_plan.json`\n",
    "* `sampled_loads.pkl`\n",
    "* `chains.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import sys\n",
    "import datetime as dt\n",
    "import importlib\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "import IPython\n",
    "from copy import deepcopy\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import swifter\n",
    "pd.set_option('display.max_columns', None)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '80g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '40g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "        .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "        .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apc_data_for_date(filter_date):\n",
    "    print(\"Running this...\")\n",
    "    filepath = '/home/jptalusan/mta_stationing_problem/data/processed/apc_weather_gtfs_20220921.parquet'\n",
    "    apcdata = spark.read.load(filepath)\n",
    "    apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "    plot_date = filter_date.strftime('%Y-%m-%d')\n",
    "    get_columns = ['trip_id', 'transit_date', 'arrival_time', 'scheduled_time',\n",
    "                'block_abbr', 'stop_sequence', 'stop_id_original',\n",
    "                'vehicle_id', 'vehicle_capacity',\n",
    "                'load', \n",
    "                'darksky_temperature', \n",
    "                'darksky_humidity', \n",
    "                'darksky_precipitation_probability', \n",
    "                'route_direction_name', 'route_id',\n",
    "                'dayofweek',  'year', 'month', 'hour', 'zero_load_at_trip_end',\n",
    "                'sched_hdwy']\n",
    "    get_str = \", \".join([c for c in get_columns])\n",
    "    query = f\"\"\"\n",
    "    SELECT {get_str}\n",
    "    FROM apc\n",
    "    WHERE (transit_date == '{plot_date}')\n",
    "    ORDER BY arrival_time\n",
    "    \"\"\"\n",
    "    apcdata = spark.sql(query)\n",
    "    apcdata = apcdata.withColumn(\"route_id_dir\", F.concat_ws(\"_\", apcdata.route_id, apcdata.route_direction_name))\n",
    "    apcdata = apcdata.withColumn(\"day\", F.dayofmonth(apcdata.arrival_time))\n",
    "    apcdata = apcdata.drop(\"route_direction_name\")\n",
    "    apcdata = apcdata.withColumn(\"load\", F.when(apcdata.load < 0, 0).otherwise(apcdata.load))\n",
    "    apcdata = apcdata.na.fill(value=0,subset=[\"zero_load_at_trip_end\"])\n",
    "    return apcdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(input_df, ohe_encoder, label_encoders, num_scaler, columns, keep_columns=[], target='y_class'):\n",
    "    num_columns = ['darksky_temperature', 'darksky_humidity', 'darksky_precipitation_probability', 'sched_hdwy']\n",
    "    cat_columns = ['month', 'hour', 'day', 'stop_sequence', 'stop_id_original', 'year', 'time_window']\n",
    "    ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end']\n",
    "\n",
    "    # OHE\n",
    "    input_df[ohe_encoder.get_feature_names_out()] = ohe_encoder.transform(input_df[ohe_columns]).toarray()\n",
    "    # input_df = input_df.drop(columns=ohe_columns)\n",
    "\n",
    "    # Label encoder\n",
    "    for cat in cat_columns:\n",
    "        encoder = label_encoders[cat]\n",
    "        input_df[cat] = encoder.transform(input_df[cat])\n",
    "    \n",
    "    # Num scaler\n",
    "    input_df[num_columns] = num_scaler.transform(input_df[num_columns])\n",
    "    input_df['y_class']  = input_df.y_class.astype('int')\n",
    "\n",
    "    if keep_columns:\n",
    "        columns = keep_columns + columns\n",
    "    # Rearrange columns\n",
    "    input_df = input_df[columns]\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def assign_data_to_bins(df, TARGET='load'):\n",
    "    bins = pd.IntervalIndex.from_tuples([(-1, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)])\n",
    "    mycut = pd.cut(df[TARGET].tolist(), bins=bins)\n",
    "    df['y_class'] = mycut.codes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEWINDOW = 15\n",
    "def add_features(df):\n",
    "    df = df[df.arrival_time.notna()]\n",
    "    df = df.fillna(method=\"bfill\")\n",
    "\n",
    "    df['day'] = df[\"arrival_time\"].dt.day\n",
    "    df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "    # Adding extra features\n",
    "    # Holidays\n",
    "    fp = os.path.join('data', 'US Holiday Dates (2004-2021).csv')\n",
    "    holidays_df = pd.read_csv(fp)\n",
    "    holidays_df['Date'] = pd.to_datetime(holidays_df['Date'])\n",
    "    holidays_df['is_holiday'] = True\n",
    "    df = df.merge(holidays_df[['Date', 'is_holiday']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_holiday'] = df['is_holiday'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "        \n",
    "    # School breaks\n",
    "    fp = os.path.join('data', 'School Breaks (2019-2022).pkl')\n",
    "    school_break_df = pd.read_pickle(fp)\n",
    "    school_break_df['is_school_break'] = True\n",
    "    df = df.merge(school_break_df[['Date', 'is_school_break']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_school_break'] = df['is_school_break'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "\n",
    "    df['minute'] = df['arrival_time'].dt.minute\n",
    "    df['minuteByWindow'] = df['minute'] // TIMEWINDOW\n",
    "    df['temp'] = df['minuteByWindow'] + (df['hour'] * 60 / TIMEWINDOW)\n",
    "    df['time_window'] = np.floor(df['temp']).astype('int')\n",
    "    df = df.drop(columns=['minute', 'minuteByWindow', 'temp'])\n",
    "\n",
    "    # HACK\n",
    "    df = df[df['hour'] != 3]\n",
    "    df = df[df['stop_sequence'] != 0]\n",
    "\n",
    "    df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "    df = assign_data_to_bins(df, TARGET='load')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_simple_lstm_generator(num_features, num_classes, learning_rate=1e-4):\n",
    "    # define model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "\n",
    "    input_shape = (None, None, num_features)\n",
    "    model.build(input_shape)\n",
    "    return model\n",
    "\n",
    "def generate_simple_lstm_predictions(input_df, model, past, future):\n",
    "    past_df = input_df[0:past]\n",
    "    future_df = input_df[past:]\n",
    "    predictions = []\n",
    "    pred_probs = []\n",
    "    if future == None:\n",
    "        future = len(future_df)\n",
    "    for f in range(future):\n",
    "        pred = model.predict(past_df.to_numpy().reshape(1, *past_df.shape))\n",
    "        pred_probs.append(pred)\n",
    "        y_pred = np.argmax(pred)\n",
    "        predictions.append(y_pred)\n",
    "        \n",
    "        # Add information from future\n",
    "        last_row = future_df.iloc[[0]]\n",
    "        last_row['y_class'] = y_pred\n",
    "        past_df = pd.concat([past_df[1:], last_row])\n",
    "        \n",
    "        # Move future to remove used row\n",
    "        future_df = future_df[1:]\n",
    "    return predictions, pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "latest = tf.train.latest_checkpoint('models/no_speed')\n",
    "columns = joblib.load('models/LL_X_columns.joblib')\n",
    "label_encoders = joblib.load('models/LL_Label_encoders.joblib')\n",
    "ohe_encoder = joblib.load('models/LL_OHE_encoder.joblib')\n",
    "num_scaler = joblib.load('models/LL_Num_scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2021-10-18, 2021-11-23, 2021-12-15, 2022-01-\n",
    "DATE = '2021-10-18'\n",
    "date_to_predict = dt.datetime.strptime(DATE, '%Y-%m-%d')\n",
    "apcdata = get_apc_data_for_date(date_to_predict)\n",
    "df = apcdata.toPandas()\n",
    "\n",
    "# HACK\n",
    "df = df.query(\"route_id != 95\")\n",
    "df = df[~df['stop_id_original'].isin(['PEARL', 'JOHASHEN', 'ROS10AEN'])]\n",
    "\n",
    "df = add_features(df)\n",
    "raw_df = deepcopy(df)\n",
    "\n",
    "input_df = prepare_input_data(df, ohe_encoder, label_encoders, num_scaler, columns, target='y_class')\n",
    "ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end']\n",
    "input_df = input_df.drop(columns=ohe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "past = 5\n",
    "rand_trips = random.sample(df.trip_id.unique().tolist(), 1)\n",
    "for trip_id in tqdm(rand_trips):\n",
    "    _df = df.query(\"trip_id == @trip_id\")\n",
    "    try:\n",
    "        _input_df = input_df.loc[_df.index]\n",
    "    except:\n",
    "        pass\n",
    "past_df = input_df[0:past]\n",
    "future_df = input_df[past:]\n",
    "\n",
    "display(past_df)\n",
    "model = setup_simple_lstm_generator(input_df.shape[1], 5)\n",
    "model.load_weights(latest)\n",
    "# model.summary()\n",
    "res = model.predict(past_df.to_numpy().reshape(1, *past_df.shape))\n",
    "\n",
    "plt.plot(res.flatten(), marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 145/1277 [02:47<35:27,  1.88s/it]"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "FUTURE = None\n",
    "PAST = 5\n",
    "\n",
    "NUM_TRIPS = None\n",
    "if NUM_TRIPS == None:\n",
    "    rand_trips = df.trip_id.unique().tolist()\n",
    "else:\n",
    "    rand_trips = random.sample(df.trip_id.unique().tolist(), NUM_TRIPS)\n",
    "\n",
    "model = setup_simple_lstm_generator(input_df.shape[1], NUM_CLASSES)\n",
    "model.load_weights(latest)\n",
    "\n",
    "trip_res = []\n",
    "load_arr = []\n",
    "for trip_id in tqdm(rand_trips):\n",
    "    _df = df.query(\"trip_id == @trip_id\")\n",
    "    try:\n",
    "        _input_df = input_df.loc[_df.index]\n",
    "        y_pred, y_pred_probs = generate_simple_lstm_predictions(_input_df, model, PAST, FUTURE)\n",
    "        loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in y_pred]\n",
    "        \n",
    "        _raw_df = raw_df.loc[_df.index]\n",
    "        y_true = _raw_df[0:PAST]['load'].tolist()\n",
    "        a = y_true + loads\n",
    "        _raw_df['sampled_loads'] = a\n",
    "        \n",
    "        y_true_classes = _raw_df[0:PAST]['y_class'].tolist()\n",
    "        _raw_df['y_pred_classes'] = y_true_classes + y_pred\n",
    "        _raw_df['y_pred_probs'] = [[-1] * NUM_CLASSES]*len(y_true_classes) + [ypp[0] for ypp in y_pred_probs]\n",
    "        \n",
    "        trip_res.append(_raw_df)\n",
    "    except:\n",
    "        print(f\"FAILED:{trip_id}\")\n",
    "        continue\n",
    "\n",
    "trip_res = pd.concat(trip_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_columns = ['trip_id', 'transit_date', 'arrival_time', 'scheduled_time', 'block_abbr', \n",
    "            'stop_sequence', 'stop_id_original', 'route_id_dir', 'zero_load_at_trip_end', \n",
    "            'y_pred_classes', 'y_pred_probs', 'sampled_loads', 'vehicle_id', 'vehicle_capacity']\n",
    "_trip_res = trip_res[_columns]\n",
    "\n",
    "fp = 'results/sampled_loads.pkl'\n",
    "_trip_res.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching with GTFS time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = 'results/sampled_loads.pkl'\n",
    "trip_res_df = pd.read_pickle(fp)\n",
    "# trip_res_df['trip_id'] = trip_res_df['trip_id'].astype('int')\n",
    "\n",
    "trip_res_df = pd.merge(trip_res_df, raw_df[['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original']], \n",
    "                       left_on=['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original'], \n",
    "                       right_on=['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original'], how='left')\n",
    "trip_res_df['trip_id'] = trip_res_df['trip_id'].astype('int')\n",
    "\n",
    "# print(trip_res_df.shape)\n",
    "stop_times_fp = 'data/GTFS/OCT2021/stop_times.txt'\n",
    "stop_times_df = pd.read_csv(stop_times_fp)\n",
    "# stop_times_df.query(\"trip_id == 264733\")\n",
    "\n",
    "trip_res_df = pd.merge(trip_res_df, stop_times_df[['trip_id', 'stop_id', 'timepoint']], left_on=['trip_id', 'stop_id_original'], right_on=['trip_id', 'stop_id'])\n",
    "trip_res_df.query(\"trip_id == 264733\")\n",
    "trip_res_df = trip_res_df.drop_duplicates(subset=['trip_id', 'stop_id_original', 'arrival_time', 'scheduled_time'])\n",
    "\n",
    "trip_res_arr = []\n",
    "for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "    trip_df.loc[trip_df.index[-1], 'timepoint']= 1.0\n",
    "    trip_res_arr.append(trip_df)\n",
    "    \n",
    "trip_res_df = pd.concat(trip_res_arr)\n",
    "# trip_res_df.query(\"trip_id == 259274\")\n",
    "\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res_df.to_pickle(fp)\n",
    "trip_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vehicle assignments here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Vehicle assignments\n",
    "# Each vehicle config is a dict: {vehicle_capacity, blocks}\n",
    "DEFAULT_CAPACITY = 40.0\n",
    "overall_vehicle_plan = {}\n",
    "\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res_df = pd.read_pickle(fp)\n",
    "\n",
    "# TODO: run again with vehicle_capacity (above)\n",
    "for vehicle_id, vehicle_df in trip_res_df.groupby('vehicle_id'):\n",
    "    vehicle_df = vehicle_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    vehicle_capacity = vehicle_df.iloc[0].vehicle_capacity\n",
    "    # vehicle_capacity = DEFAULT_CAPACITY\n",
    "    if np.isnan(vehicle_capacity):\n",
    "        vehicle_capacity = DEFAULT_CAPACITY\n",
    "    # TODO: This is not the baseline behavior\n",
    "    starting_depot = 'MCC5_1'\n",
    "    service_type = 'regular'\n",
    "    blocks = [block for block in vehicle_df.block_abbr.unique().tolist()]\n",
    "    trips = []\n",
    "    for block in blocks:\n",
    "        block_df = vehicle_df.query(\"block_abbr == @block\")\n",
    "        for trip in block_df.trip_id.unique().tolist():\n",
    "            trips.append((str(block), str(trip)))\n",
    "    overall_vehicle_plan[vehicle_id] = {'vehicle_capacity': vehicle_capacity, 'trips': trips, 'starting_depot': starting_depot, 'service_type': service_type}\n",
    "    \n",
    "len(overall_vehicle_plan)\n",
    "\n",
    "# Number of overload buses\n",
    "#   \"42\": {\n",
    "#     \"service_type\": \"overload\",\n",
    "#     \"starting_depot\": \"MCC5_1\",\n",
    "#     \"trips\": [\n",
    "#     ],\n",
    "#     \"vehicle_capacity\": 55.0\n",
    "#   }\n",
    "OVERLOAD_BUSES = 2\n",
    "for vehicle_id in range(41, 41 + OVERLOAD_BUSES):\n",
    "    overall_vehicle_plan[str(vehicle_id)] = {'vehicle_capacity': 55.0, 'trips': [], \"starting_depot\": \"MCC5_1\", 'service_type': \"overload\"}\n",
    "    \n",
    "with open(f'results/vehicle_plan_{DATE.replace(\"-\", \"\")}.json', 'w') as fp:\n",
    "    json.dump(overall_vehicle_plan, fp, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Trip plan (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res_df = pd.read_pickle(fp)\n",
    "\n",
    "# Create a dict of {[block: {trip_ids:[]}, 'block'....]}\n",
    "# trip_id dict = {'route_id', route_direction_name', 'stop_id':[], 'schedule_time':[]}\n",
    "# Use block as grouper in baseline\n",
    "overall_block_plan = {}\n",
    "for block_abbr, block_df in trip_res_df.groupby('block_abbr'):\n",
    "    block_df = block_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    trip_ids = block_df.trip_id.unique().tolist()\n",
    "    start_time = block_df[block_df['trip_id'] == trip_ids[0]].iloc[0]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    end_time = block_df[block_df['trip_id'] == trip_ids[-1]].iloc[-1]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    overall_block_plan[block_abbr] = {'trip_ids': trip_ids,\n",
    "                                      'start_time': start_time,\n",
    "                                      'end_time': end_time}\n",
    "\n",
    "overall_trip_plan = {}\n",
    "for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "    trip_df = trip_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    route_id_dir = trip_df.iloc[0].route_id_dir\n",
    "    route_id = int(route_id_dir.split(\"_\")[0])\n",
    "    route_direction = route_id_dir.split(\"_\")[1]\n",
    "    zero_load_at_trip_end = trip_df.iloc[-1].zero_load_at_trip_end.tolist()\n",
    "    scheduled_time = trip_df.scheduled_time.dt.strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "    stop_sequence = trip_df.stop_sequence.tolist()\n",
    "    stop_sequence = list(range(0, len(stop_sequence)))\n",
    "    # stop_sequence = [ss - 1 for ss in stop_sequence]\n",
    "    stop_id_original = trip_df.stop_id_original.tolist()\n",
    "    \n",
    "    overall_trip_plan[trip_id] = {'route_id': route_id, \n",
    "                                  'route_direction': route_direction, \n",
    "                                  'scheduled_time': scheduled_time, \n",
    "                                  'stop_sequence': stop_sequence, \n",
    "                                  'stop_id_original': stop_id_original,\n",
    "                                  'zero_load_at_trip_end':zero_load_at_trip_end,\n",
    "                                  'last_stop_sequence': stop_sequence[-1],\n",
    "                                  'last_stop_id': stop_id_original[-1]}\n",
    "\n",
    "len(overall_trip_plan), len(overall_block_plan)\n",
    "\n",
    "with open(f'results/trip_plan_{DATE.replace(\"-\", \"\")}.json', 'w') as fp:\n",
    "    json.dump(overall_trip_plan, fp, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trip_res_df.query(\"trip_id == 259274\").head())\n",
    "print(trip_res_df.query(\"trip_id == 259274\").shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.query(\"trip_id == '243423'\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_id_dir = \n",
    "# trip_res.query(\"route_id_dir == @route_id_dir and block_abbr == @block and stop_id_original == @stop_id_original[@i] and scheduled_time == @scheduled_time[@i]\").iloc[0]['sampled_loads']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ons and offs from sampled loads\n",
    "* Needs the trip_res generated above\n",
    " ```\n",
    " fp = 'results/sampled_loads.pkl'\n",
    " trip_res.to_pickle(fp)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "    \n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res = pd.read_pickle(fp)\n",
    "sampled_ons_offs = []\n",
    "for trip_id, trip_id_df in tqdm(trip_res.groupby(['transit_date', 'trip_id'])):\n",
    "    tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "    tdf['ons'] = 0\n",
    "    tdf['offs'] = 0\n",
    "    tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "    \n",
    "    # Intermediate stops\n",
    "    tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # first and last stops\n",
    "    tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "    tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "    sampled_ons_offs.append(tdf)\n",
    "    \n",
    "sampled_ons_offs = pd.concat(sampled_ons_offs)\n",
    "sampled_ons_offs = sampled_ons_offs.drop('next_load', axis=1)\n",
    "\n",
    "fp = f'results/sampled_ons_offs_{DATE.replace(\"-\", \"\")}.pkl'\n",
    "sampled_ons_offs.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_ons_offs.query(\"trip_id == '259274'\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a single event chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res = pd.read_pickle(fp)\n",
    "loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in trip_res.y_pred_classes]\n",
    "trip_res['sampled_loads'] = loads\n",
    "\n",
    "sampled_ons_offs = []\n",
    "for trip_id, trip_id_df in tqdm(trip_res.groupby(['transit_date', 'trip_id'])):\n",
    "    tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "    tdf['stop_sequence'] = list(range(1, len(tdf) + 1))\n",
    "    tdf['ons'] = 0\n",
    "    tdf['offs'] = 0\n",
    "    tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "    \n",
    "    # Intermediate stops\n",
    "    tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # first and last stops\n",
    "    tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "    tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "    sampled_ons_offs.append(tdf)\n",
    "    \n",
    "df = pd.concat(sampled_ons_offs)\n",
    "df = df.drop('next_load', axis=1)\n",
    "\n",
    "display(df)\n",
    "df['key_pair'] = list(zip(df.route_id_dir, \n",
    "                          df.block_abbr,\n",
    "                          df.stop_sequence,\n",
    "                          df.stop_id_original, \n",
    "                          df.scheduled_time))\n",
    "df = df.set_index('key_pair')\n",
    "sampled_ons_offs_dict = df.drop(['trip_id', 'route_id_dir', 'block_abbr', 'stop_id_original', 'stop_id', \n",
    "                                 'scheduled_time', 'transit_date', 'arrival_time', \n",
    "                                 'zero_load_at_trip_end', 'y_pred_classes', 'y_pred_probs',\n",
    "                                 'vehicle_capacity', 'vehicle_id',\n",
    "                                 'stop_sequence'], axis=1).to_dict('index')\n",
    "sampled_ons_offs_dict\n",
    "\n",
    "import pickle \n",
    "\n",
    "with open(f'results/sampled_ons_offs_dict_{DATE.replace(\"-\", \"\")}.pkl', 'wb') as handle:\n",
    "    pickle.dump(sampled_ons_offs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[('23_FROM DOWNTOWN', 2310, 'DWMRT', pd.Timestamp('2021-08-23 05:41:00'))]\n",
    "df.query(\"route_id_dir == '23_FROM DOWNTOWN' and block_abbr == 2310 and stop_id_original == 'DWMRT' and scheduled_time == '2021-08-23 05:41:00'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = list(sampled_ons_offs_dict.keys())[0]\n",
    "print(key)\n",
    "sampled_ons_offs_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.query(\"route_id_dir == '7_TO DOWNTOWN' and block_abbr == 5692 and stop_sequence == 20 and stop_id_original == 'MCC5_9'\")\n",
    "# df.query(\"route_id_dir == '7_TO DOWNTOWN' and block_abbr == 5692\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating multiple event chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "\n",
    "CHAINS = 5\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res = pd.read_pickle(fp)\n",
    "\n",
    "for chain in tqdm(range(CHAINS)):\n",
    "    loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in trip_res.y_pred_classes]\n",
    "    trip_res['sampled_loads'] = loads\n",
    "\n",
    "    sampled_ons_offs = []\n",
    "    for trip_id, trip_id_df in trip_res.groupby(['transit_date', 'trip_id']):\n",
    "        tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "        tdf['stop_sequence'] = list(range(1, len(tdf) + 1))\n",
    "        tdf['ons'] = 0\n",
    "        tdf['offs'] = 0\n",
    "        tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "        \n",
    "        # Intermediate stops\n",
    "        tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "        \n",
    "        # first and last stops\n",
    "        tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "        tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "        sampled_ons_offs.append(tdf)\n",
    "        \n",
    "    df = pd.concat(sampled_ons_offs)\n",
    "    df['key_pair'] = list(zip(df.route_id_dir, \n",
    "                            df.block_abbr,\n",
    "                            df.stop_sequence,\n",
    "                            df.stop_id_original, \n",
    "                            df.scheduled_time))\n",
    "    df = df.set_index('key_pair')\n",
    "    sampled_ons_offs_dict = df.drop(['trip_id', 'route_id_dir', 'block_abbr', 'stop_id_original', 'stop_id', \n",
    "                                    'scheduled_time', 'transit_date', 'arrival_time', \n",
    "                                    'zero_load_at_trip_end', 'y_pred_classes', 'y_pred_probs',\n",
    "                                    'vehicle_capacity', 'vehicle_id',\n",
    "                                    'stop_sequence'], axis=1).to_dict('index')\n",
    "\n",
    "    with open(f'results/chains/ons_offs_dict_chain_{DATE.replace(\"-\",\"\")}_{chain}.pkl', 'wb') as handle:\n",
    "        pickle.dump(sampled_ons_offs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create timepoint dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times_df.iloc[0].arrival_time[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(x):\n",
    "    if x[0:2] == '24':\n",
    "        return '00'+x[2:]\n",
    "    if x[0:2] == '25':\n",
    "        return '01'+x[2:]\n",
    "    return x\n",
    "    \n",
    "stop_times_fp = 'data/GTFS/OCT2021/stop_times.txt'\n",
    "stop_times_df = pd.read_csv(stop_times_fp)\n",
    "# stop_times_df.query(\"trip_id == 264733\")\n",
    "stop_times_df['date'] = DATE\n",
    "stop_times_df['arrival_time'] = stop_times_df['arrival_time'].apply(lambda x: fix_time(x))\n",
    "stop_times_df['scheduled_time'] = pd.to_datetime(stop_times_df['date'] + ' ' + stop_times_df['arrival_time'])\n",
    "\n",
    "stop_times_df['key_pair'] = list(zip(stop_times_df.trip_id, stop_times_df.stop_id, stop_times_df.scheduled_time))\n",
    "stop_times_df = stop_times_df.set_index('key_pair')\n",
    "\n",
    "time_point_dict = stop_times_df.drop(['arrival_time', 'departure_time', 'stop_id', 'stop_sequence', 'stop_headsign', 'trip_id',\n",
    "                                      'pickup_type', 'drop_off_type', 'shape_dist_traveled', 'scheduled_time', 'date'], axis=1).to_dict('index')\n",
    "with open(f'results/time_point_dict_{DATE.replace(\"-\", \"\")}.pkl', 'wb') as handle:\n",
    "    pickle.dump(time_point_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# time_point_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, tp) for k, tp in time_point_dict.items() if k[0] == 263558][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_point_dict[(263558, 'GALBERNN', pd.Timestamp('2021-10-18 05:47:59'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT: \n",
    "* Copy these to `scenarios/baselines/data`\n",
    "    * results/sampled_ons_offs_dict\n",
    "    * results/chains/ons_offs_dict_chain_{chain}.pkl\n",
    "    * results/trip_plan.json\n",
    "    * results/vehicle_plan.json\n",
    "    * results/time_point_dict.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure:\n",
    "```\n",
    "{key:val}\n",
    "key: (tuple) (route_id_dir, block_abbr, stop_sequene, stop_id, scheduled_arrival_time)\n",
    "val: (dict) {'sampled_loads': A, 'ons': B, 'offs': C}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity check\n",
    "CHAINS = 5\n",
    "for chain in range(CHAINS):\n",
    "    with open(f'results/chains/ons_offs_dict_chain_{chain}.pkl', 'rb') as handle:\n",
    "        sampled_ons_offs_dict = pickle.load(handle)\n",
    "    # res = sampled_ons_offs_dict[('7_TO DOWNTOWN', 5692, 20, 'MCC5_9', pd.Timestamp('2021-08-23 14:39:00'))]\n",
    "    res = sampled_ons_offs_dict[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', pd.Timestamp('2021-10-18 14:15:00'))]\n",
    "    print(f\"chain {chain}: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sampled_ons_offs_dict.keys())[0], list(sampled_ons_offs_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "with open(f'results/chains/ons_offs_dict_chain_0.pkl', 'rb') as handle:\n",
    "    sampled_ons_offs_dict = pickle.load(handle)\n",
    "# ('7_TO DOWNTOWN', 5692, 1, 'HBHS', datetime.datetime(2021, 8, 23, 14, 9))\n",
    "# sampled_ons_offs_dict[('7_TO DOWNTOWN', 5692, 1, 'HBHS', dt.datetime(2021, 8, 23, 14, 9))]\n",
    "search_key = ('7_TO DOWNTOWN', 5692, 5)\n",
    "values = [value for key, value in sampled_ons_offs_dict.items() if search_key == key[:len(search_key)]]\n",
    "keys = [key for key, value in sampled_ons_offs_dict.items() if search_key == key[:len(search_key)]]\n",
    "values, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sampled_ons_offs_dict.keys())[0], list(sampled_ons_offs_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sampled_ons_offs_dict[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))]\n",
    "('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))\n",
    "('14_FROM DOWNTOWN', '1400', 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))\n",
    "# ('14_FROM DOWNTOWN', '1400', 1, 'MCC4_20', datetime.datetime(2021, 8, 23, 14, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', Timestamp('2021-10-18 14:15:00'))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 263159\n",
    "import pandas as pd\n",
    "\n",
    "fp = 'results/sampled_ons_offs_dict_20211018.pkl'\n",
    "df = pd.read_pickle(fp)\n",
    "list(df.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sampled_loads': 38,\n",
       " 'timepoint': 1,\n",
       " 'ons': 38.0,\n",
       " 'offs': 7.0,\n",
       " 'next_load': 31.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rid = '55_FROM DOWNTOWN'\n",
    "sid = 'MCC4_15'\n",
    "time = '2021-10-18 15:35:00'\n",
    "\n",
    "df[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', pd.Timestamp('2021-10-18 14:15:00'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('55_FROM DOWNTOWN', 5512, 1, 'MCC4_15', Timestamp('2021-10-18 15:35:00')),\n",
       "  {'sampled_loads': 19,\n",
       "   'timepoint': 1,\n",
       "   'ons': 19.0,\n",
       "   'offs': 0.0,\n",
       "   'next_load': 38.0})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, v)for k, v in df.items() if k[0] == rid and k[3] == sid and k[4] == pd.Timestamp(time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
