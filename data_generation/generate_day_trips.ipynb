{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation\n",
    "* Generates data for a desired date based on the available APC data and passed through the model for load prediction.\n",
    "* Will provide a distribution of bins which can be used for stochasticity\n",
    "## Generates the following files:\n",
    "* `trip_plan.json`\n",
    "* `vehicle_plan.json`\n",
    "* `sampled_loads.pkl`\n",
    "* `chains.pkl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n",
      "22/12/07 19:30:20 WARN Utils: Your hostname, scope-vanderbilt resolves to a loopback address: 127.0.1.1; using 10.2.218.69 instead (on interface enp8s0)\n",
      "22/12/07 19:30:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/07 19:30:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "import sys\n",
    "import datetime as dt\n",
    "import importlib\n",
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, concatenate, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "import IPython\n",
    "from copy import deepcopy\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "mpl.rcParams['figure.facecolor'] = 'white'\n",
    "\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import swifter\n",
    "pd.set_option('display.max_columns', None)\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "tf.get_logger().setLevel('INFO')\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '80g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '40g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "        .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "        .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_apc_data_for_date(filter_date):\n",
    "    print(\"Running this...\")\n",
    "    filepath = '/home/jptalusan/mta_stationing_problem/data/processed/apc_weather_gtfs_20220921.parquet'\n",
    "    apcdata = spark.read.load(filepath)\n",
    "    apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "    plot_date = filter_date.strftime('%Y-%m-%d')\n",
    "    get_columns = ['trip_id', 'transit_date', 'arrival_time', 'scheduled_time',\n",
    "                'block_abbr', 'stop_sequence', 'stop_id_original',\n",
    "                'vehicle_id', 'vehicle_capacity',\n",
    "                'load', \n",
    "                'darksky_temperature', \n",
    "                'darksky_humidity', \n",
    "                'darksky_precipitation_probability', \n",
    "                'route_direction_name', 'route_id', 'overload_id',\n",
    "                'dayofweek',  'year', 'month', 'hour', 'zero_load_at_trip_end',\n",
    "                'sched_hdwy']\n",
    "    get_str = \", \".join([c for c in get_columns])\n",
    "    query = f\"\"\"\n",
    "    SELECT {get_str}\n",
    "    FROM apc\n",
    "    WHERE (transit_date == '{plot_date}')\n",
    "    ORDER BY arrival_time\n",
    "    \"\"\"\n",
    "    apcdata = spark.sql(query)\n",
    "    apcdata = apcdata.withColumn(\"route_id_dir\", F.concat_ws(\"_\", apcdata.route_id, apcdata.route_direction_name))\n",
    "    apcdata = apcdata.withColumn(\"day\", F.dayofmonth(apcdata.arrival_time))\n",
    "    apcdata = apcdata.drop(\"route_direction_name\")\n",
    "    apcdata = apcdata.withColumn(\"load\", F.when(apcdata.load < 0, 0).otherwise(apcdata.load))\n",
    "    apcdata = apcdata.na.fill(value=0,subset=[\"zero_load_at_trip_end\"])\n",
    "    return apcdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input_data(input_df, ohe_encoder, label_encoders, num_scaler, columns, keep_columns=[], target='y_class'):\n",
    "    num_columns = ['darksky_temperature', 'darksky_humidity', 'darksky_precipitation_probability', 'sched_hdwy']\n",
    "    cat_columns = ['month', 'hour', 'day', 'stop_sequence', 'stop_id_original', 'year', 'time_window']\n",
    "    ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end']\n",
    "\n",
    "    # OHE\n",
    "    input_df[ohe_encoder.get_feature_names_out()] = ohe_encoder.transform(input_df[ohe_columns]).toarray()\n",
    "    # input_df = input_df.drop(columns=ohe_columns)\n",
    "\n",
    "    # Label encoder\n",
    "    for cat in cat_columns:\n",
    "        print(cat)\n",
    "        encoder = label_encoders[cat]\n",
    "        input_df[cat] = encoder.transform(input_df[cat])\n",
    "    \n",
    "    # Num scaler\n",
    "    input_df[num_columns] = num_scaler.transform(input_df[num_columns])\n",
    "    input_df['y_class']  = input_df.y_class.astype('int')\n",
    "\n",
    "    if keep_columns:\n",
    "        columns = keep_columns + columns\n",
    "    # Rearrange columns\n",
    "    input_df = input_df[columns]\n",
    "    \n",
    "    return input_df\n",
    "\n",
    "def assign_data_to_bins(df, TARGET='load'):\n",
    "    bins = pd.IntervalIndex.from_tuples([(-1, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)])\n",
    "    mycut = pd.cut(df[TARGET].tolist(), bins=bins)\n",
    "    df['y_class'] = mycut.codes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEWINDOW = 15\n",
    "def add_features(df):\n",
    "    df = df[df.arrival_time.notna()]\n",
    "    df = df.fillna(method=\"bfill\")\n",
    "\n",
    "    df['day'] = df[\"arrival_time\"].dt.day\n",
    "    df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "    # Adding extra features\n",
    "    # Holidays\n",
    "    fp = os.path.join('data', 'US Holiday Dates (2004-2021).csv')\n",
    "    holidays_df = pd.read_csv(fp)\n",
    "    holidays_df['Date'] = pd.to_datetime(holidays_df['Date'])\n",
    "    holidays_df['is_holiday'] = True\n",
    "    df = df.merge(holidays_df[['Date', 'is_holiday']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_holiday'] = df['is_holiday'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "        \n",
    "    # School breaks\n",
    "    fp = os.path.join('data', 'School Breaks (2019-2022).pkl')\n",
    "    school_break_df = pd.read_pickle(fp)\n",
    "    school_break_df['is_school_break'] = True\n",
    "    df = df.merge(school_break_df[['Date', 'is_school_break']], left_on='transit_date', right_on='Date', how='left')\n",
    "    df['is_school_break'] = df['is_school_break'].fillna(False)\n",
    "    df = df.drop(columns=['Date'])\n",
    "\n",
    "    df['minute'] = df['arrival_time'].dt.minute\n",
    "    df['minuteByWindow'] = df['minute'] // TIMEWINDOW\n",
    "    df['temp'] = df['minuteByWindow'] + (df['hour'] * 60 / TIMEWINDOW)\n",
    "    df['time_window'] = np.floor(df['temp']).astype('int')\n",
    "    df = df.drop(columns=['minute', 'minuteByWindow', 'temp'])\n",
    "\n",
    "    # HACK\n",
    "    # df = df[df['hour'] != 3]\n",
    "    # df = df[df['stop_sequence'] != 0]\n",
    "\n",
    "    df = df.sort_values(by=['block_abbr', 'arrival_time']).reset_index(drop=True)\n",
    "\n",
    "    df = assign_data_to_bins(df, TARGET='load')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_simple_lstm_generator(num_features, num_classes, learning_rate=1e-4):\n",
    "    # define model\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(LSTM(256, return_sequences=True))\n",
    "    model.add(LSTM(256))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    # compile model\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "\n",
    "    input_shape = (None, None, num_features)\n",
    "    model.build(input_shape)\n",
    "    return model\n",
    "\n",
    "def generate_simple_lstm_predictions(input_df, model, past, future):\n",
    "    past_df = input_df[0:past]\n",
    "    future_df = input_df[past:]\n",
    "    predictions = []\n",
    "    pred_probs = []\n",
    "    if future == None:\n",
    "        future = len(future_df)\n",
    "    for f in range(future):\n",
    "        pred = model.predict(past_df.to_numpy().reshape(1, *past_df.shape))\n",
    "        pred_probs.append(pred)\n",
    "        y_pred = np.argmax(pred)\n",
    "        predictions.append(y_pred)\n",
    "        \n",
    "        # Add information from future\n",
    "        last_row = future_df.iloc[[0]]\n",
    "        last_row['y_class'] = y_pred\n",
    "        past_df = pd.concat([past_df[1:], last_row])\n",
    "        \n",
    "        # Move future to remove used row\n",
    "        future_df = future_df[1:]\n",
    "    return predictions, pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_overload_regular_bus_trips(regular, overload):\n",
    "    m = regular.merge(overload, how='left', on=['trip_id', 'transit_date', 'scheduled_time', 'block_abbr', 'stop_sequence', 'stop_id_original', 'route_id_dir', 'route_id'])\n",
    "    \n",
    "    m['arrival_time'] = np.max(m[['arrival_time_x', 'arrival_time_y']], axis=1)\n",
    "    \n",
    "    m['zero_load_at_trip_end'] = m['zero_load_at_trip_end_x']\n",
    "    \n",
    "    m.loc[~m['arrival_time_x'].isnull(), \"load\"] = m['load_x']\n",
    "    # m.loc[~m['arrival_time_x'].isnull(), \"ons\"] = m['ons_x']\n",
    "    # m.loc[~m['arrival_time_x'].isnull(), \"offs\"] = m['offs_x']\n",
    "    \n",
    "    m.loc[~m['arrival_time_y'].isnull(), \"load\"] = m['load_y']\n",
    "    # m.loc[~m['arrival_time_y'].isnull(), \"ons\"] = m['ons_y']\n",
    "    # m.loc[~m['arrival_time_y'].isnull(), \"offs\"] = m['offs_y']\n",
    "    \n",
    "    m['vehicle_id'] = m['vehicle_id_x']\n",
    "    m['vehicle_capacity'] = m['vehicle_capacity_x']\n",
    "    m['overload_id'] = m['overload_id_x']\n",
    "    m = m[m.columns.drop(list(m.filter(regex='_x')))]\n",
    "    m = m[m.columns.drop(list(m.filter(regex='_y')))]\n",
    "    # m = m[regular.columns]\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "latest = tf.train.latest_checkpoint('models/no_speed')\n",
    "columns = joblib.load('models/LL_X_columns.joblib')\n",
    "label_encoders = joblib.load('models/LL_Label_encoders.joblib')\n",
    "ohe_encoder = joblib.load('models/LL_OHE_encoder.joblib')\n",
    "num_scaler = joblib.load('models/LL_Num_scaler.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '2021-03-05'\n",
    "start_time = '08:00:00'\n",
    "end_time = '12:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running this...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/01 02:41:17 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2021-10-18, 2021-11-23, 2021-12-15, 2022-01-\n",
    "date_to_predict = dt.datetime.strptime(DATE, '%Y-%m-%d')\n",
    "apcdata = get_apc_data_for_date(date_to_predict)\n",
    "df = apcdata.toPandas()\n",
    "\n",
    "# HACK\n",
    "# a = df.query(\"trip_id == '233300' and vehicle_id == '722'\").sort_values('stop_sequence')\n",
    "# b = df.query(\"trip_id == '233300' and vehicle_id == '1830'\").sort_values('stop_sequence')\n",
    "# m1 = merge_overload_regular_bus_trips(a, b)\n",
    "\n",
    "# a = df.query(\"trip_id == '259635' and vehicle_id == '2019'\").sort_values('stop_sequence')\n",
    "# b = df.query(\"trip_id == '259635' and vehicle_id == '1914'\").sort_values('stop_sequence')\n",
    "# m2 = merge_overload_regular_bus_trips(a, b)\n",
    "\n",
    "df = df.query(\"overload_id == 0\")\n",
    "# overload_trips = df.query(\"overload_id > 0\").trip_id.unique()\n",
    "# df = df[~df['trip_id'].isin(overload_trips)]\n",
    "# df = pd.concat([tdf, m1])\n",
    "df = df.dropna(subset=['arrival_time'])\n",
    "# df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# HACK\n",
    "# df = df.query(\"route_id != 95\")\n",
    "# df = df[~df['stop_id_original'].isin(['PEARL', 'JOHASHEN', 'ROS10AEN'])]\n",
    "\n",
    "df = add_features(df)\n",
    "raw_df = deepcopy(df)\n",
    "\n",
    "# HACK\n",
    "# df.loc[df['time_window'].isin([6, 7, 8]), 'time_window'] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month\n",
      "hour\n",
      "day\n",
      "stop_sequence\n",
      "stop_id_original\n",
      "year\n",
      "time_window\n"
     ]
    }
   ],
   "source": [
    "input_df = prepare_input_data(df, ohe_encoder, label_encoders, num_scaler, columns, target='y_class')\n",
    "ohe_columns = ['dayofweek', 'route_id_dir', 'is_holiday', 'is_school_break', 'zero_load_at_trip_end']\n",
    "input_df = input_df.drop(columns=ohe_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-30 20:05:23.424372: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-30 20:05:23.878143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 11402 MB memory:  -> device: 0, name: NVIDIA TITAN Xp, pci bus id: 0000:0b:00.0, compute capability: 6.1\n",
      "  0%|          | 0/1005 [00:00<?, ?it/s]2022-11-30 20:05:25.237817: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8401\n",
      "100%|██████████| 1005/1005 [14:42<00:00,  1.14it/s]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "tf.keras.backend.clear_session()\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "FUTURE = None\n",
    "PAST = 5\n",
    "\n",
    "NUM_TRIPS = None\n",
    "if NUM_TRIPS == None:\n",
    "    rand_trips = df.trip_id.unique().tolist()\n",
    "else:\n",
    "    rand_trips = random.sample(df.trip_id.unique().tolist(), NUM_TRIPS)\n",
    "\n",
    "model = setup_simple_lstm_generator(input_df.shape[1], NUM_CLASSES)\n",
    "model.load_weights(latest)\n",
    "\n",
    "trip_res = []\n",
    "load_arr = []\n",
    "for trip_id in tqdm(rand_trips):\n",
    "    _df = df.query(\"trip_id == @trip_id\")\n",
    "    try:\n",
    "        _input_df = input_df.loc[_df.index]\n",
    "        _y_pred, y_pred_probs = generate_simple_lstm_predictions(_input_df, model, PAST, FUTURE)\n",
    "        \n",
    "        # Introducing stochasticity\n",
    "        y_pred = [np.random.choice(len(ypp.flatten()), size=1, p=ypp.flatten())[0] for ypp in y_pred_probs]\n",
    "        loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in y_pred]\n",
    "        \n",
    "        _raw_df = raw_df.loc[_df.index]\n",
    "        y_true = _raw_df[0:PAST]['load'].tolist()\n",
    "        a = y_true + loads\n",
    "        _raw_df['sampled_loads'] = a\n",
    "        \n",
    "        y_true_classes = _raw_df[0:PAST]['y_class'].tolist()\n",
    "        _raw_df['y_pred_classes'] = y_true_classes + y_pred\n",
    "        _raw_df['y_pred_probs'] = [[-1] * NUM_CLASSES]*len(y_true_classes) + [ypp[0] for ypp in y_pred_probs]\n",
    "        \n",
    "        trip_res.append(_raw_df)\n",
    "    except:\n",
    "        print(f\"FAILED:{trip_id}\")\n",
    "        continue\n",
    "\n",
    "trip_res = pd.concat(trip_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "_columns = ['trip_id', 'transit_date', 'arrival_time', 'scheduled_time', 'block_abbr', \n",
    "            'stop_sequence', 'stop_id_original', 'route_id_dir', 'zero_load_at_trip_end', \n",
    "            'y_pred_classes', 'y_pred_probs', 'sampled_loads', 'vehicle_id', 'vehicle_capacity']\n",
    "_trip_res = trip_res[_columns]\n",
    "\n",
    "# fp = 'results/sampled_loads.pkl'\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "_trip_res.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching with GTFS time points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     trip_df\u001b[39m.\u001b[39mloc[trip_df\u001b[39m.\u001b[39mindex[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mtimepoint\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     trip_res_arr\u001b[39m.\u001b[39mappend(trip_df)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m trip_res_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(trip_res_arr)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# trip_res_df.to_pickle(fp)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdigital-storm-1/home/jptalusan/gits/mta_simulator_redo/data_generation/generate_day_trips.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m trip_res_df\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/reshape/concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    156\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m    along the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    348\u001b[0m         objs,\n\u001b[1;32m    349\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    350\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    351\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    352\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    353\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    354\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    355\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    356\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    357\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    360\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/pandas/core/reshape/concat.py:404\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    401\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[1;32m    403\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 404\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     objs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "# # fp = 'results/sampled_loads.pkl'\n",
    "# # trip_res_df = pd.read_pickle(fp)\n",
    "# # trip_res_df['trip_id'] = trip_res_df['trip_id'].astype('int')\n",
    "# trip_res_df = _trip_res\n",
    "\n",
    "# trip_res_df = pd.merge(trip_res_df, raw_df[['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original']], \n",
    "#                        left_on=['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original'], \n",
    "#                        right_on=['trip_id', 'scheduled_time', 'arrival_time', 'stop_id_original'], how='left')\n",
    "# trip_res_df['trip_id'] = trip_res_df['trip_id'].astype('int')\n",
    "\n",
    "# # print(trip_res_df.shape)\n",
    "# stop_times_fp = 'data/GTFS/OCT2021/stop_times.txt'\n",
    "# stop_times_df = pd.read_csv(stop_times_fp)\n",
    "# # stop_times_df.query(\"trip_id == 264733\")\n",
    "\n",
    "# trip_res_df = pd.merge(trip_res_df, stop_times_df[['trip_id', 'stop_id', 'timepoint']], left_on=['trip_id', 'stop_id_original'], right_on=['trip_id', 'stop_id'])\n",
    "# trip_res_df.query(\"trip_id == 264733\")\n",
    "# trip_res_df = trip_res_df.drop_duplicates(subset=['trip_id', 'stop_id_original', 'arrival_time', 'scheduled_time'])\n",
    "\n",
    "# trip_res_arr = []\n",
    "# for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "#     trip_df.loc[trip_df.index[-1], 'timepoint']= 1.0\n",
    "#     trip_res_arr.append(trip_df)\n",
    "    \n",
    "# trip_res_df = pd.concat(trip_res_arr)\n",
    "\n",
    "# # fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# # trip_res_df.to_pickle(fp)\n",
    "# trip_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate vehicle assignments here...\n",
    "* Trying to limit to a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    '2021-10-18':['121', '124', '140', '1824', '1830', '1900', '1920', '2013', '2015', '715', '720'],\n",
    "    '2021-11-23':['1812', '1817', '1820', '1825', '1827', '1900', '1906', '1907', '2006', '2009'],\n",
    "    '2021-12-15':['121', '134', '1800', '1804', '1812', '1821', '1826', '1830', '1904', '1908'],\n",
    "    '2022-01-27':['129', '137', '1811', '1815', '1819', '1824', '1906', '1920', '2003', '2004'],\n",
    "    '2022-02-25':['127', '130', '139', '141', '1814', '1820', '1823', '1901', '1913', '1916']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATE = '2021-11-23'\n",
    "vehicle_list = ['1812', '1817', '1820', '1825', '1827', '1900', '1906', '1907', '2006', '2009']\n",
    "start_time = '08:00:00'\n",
    "end_time = '12:00:00'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7feb61d84bb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res_df = pd.read_pickle(fp)\n",
    "trip_res_df['hour'] = trip_res_df.scheduled_time.dt.hour\n",
    "trip_res_df['count'] = 1\n",
    "ax = trip_res_df.groupby('trip_id').agg({\"hour\":\"first\", \"count\":\"count\"}).groupby(\"hour\").count().plot(kind='line')\n",
    "ax.axvline(x=6, ymin=0, ymax=100, ls='--')\n",
    "ax.axvline(x=10, ymin=0, ymax=100, ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181,)\n",
      "(10,)\n",
      "(40,)\n",
      "(9,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>transit_date</th>\n",
       "      <th>arrival_time</th>\n",
       "      <th>scheduled_time</th>\n",
       "      <th>block_abbr</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>stop_id_original</th>\n",
       "      <th>route_id_dir</th>\n",
       "      <th>zero_load_at_trip_end</th>\n",
       "      <th>y_pred_classes</th>\n",
       "      <th>y_pred_probs</th>\n",
       "      <th>sampled_loads</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>vehicle_capacity</th>\n",
       "      <th>stop_id</th>\n",
       "      <th>timepoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16372</th>\n",
       "      <td>260005</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>2021-11-23 08:02:01</td>\n",
       "      <td>2021-11-23 08:00:00</td>\n",
       "      <td>2201</td>\n",
       "      <td>1</td>\n",
       "      <td>CLSFCLK</td>\n",
       "      <td>22_TO DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1817</td>\n",
       "      <td>40.0</td>\n",
       "      <td>CLSFCLK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16373</th>\n",
       "      <td>260005</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>2021-11-23 08:02:02</td>\n",
       "      <td>2021-11-23 08:00:15</td>\n",
       "      <td>2201</td>\n",
       "      <td>2</td>\n",
       "      <td>CLICLANM</td>\n",
       "      <td>22_TO DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1817</td>\n",
       "      <td>40.0</td>\n",
       "      <td>CLICLANM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16374</th>\n",
       "      <td>260005</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>2021-11-23 08:02:20</td>\n",
       "      <td>2021-11-23 08:00:41</td>\n",
       "      <td>2201</td>\n",
       "      <td>3</td>\n",
       "      <td>CLIBUENM</td>\n",
       "      <td>22_TO DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1817</td>\n",
       "      <td>40.0</td>\n",
       "      <td>CLIBUENM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16375</th>\n",
       "      <td>260005</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>2021-11-23 08:03:08</td>\n",
       "      <td>2021-11-23 08:01:01</td>\n",
       "      <td>2201</td>\n",
       "      <td>4</td>\n",
       "      <td>CLIBUENN</td>\n",
       "      <td>22_TO DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1817</td>\n",
       "      <td>40.0</td>\n",
       "      <td>CLIBUENN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16376</th>\n",
       "      <td>260005</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>2021-11-23 08:03:46</td>\n",
       "      <td>2021-11-23 08:01:40</td>\n",
       "      <td>2201</td>\n",
       "      <td>5</td>\n",
       "      <td>BUEKIRNF</td>\n",
       "      <td>22_TO DOWNTOWN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[-1, -1, -1, -1, -1]</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1817</td>\n",
       "      <td>40.0</td>\n",
       "      <td>BUEKIRNF</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37430</th>\n",
       "      <td>264427</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>2021-11-23 09:07:46</td>\n",
       "      <td>2021-11-23 09:08:52</td>\n",
       "      <td>7600</td>\n",
       "      <td>38</td>\n",
       "      <td>DELCUMSN</td>\n",
       "      <td>76_LOOP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.88960415, 0.086769514, 0.0236245, 1.8498251...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1812</td>\n",
       "      <td>40.0</td>\n",
       "      <td>DELCUMSN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37431</th>\n",
       "      <td>264427</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>2021-11-23 09:08:26</td>\n",
       "      <td>2021-11-23 09:10:21</td>\n",
       "      <td>7600</td>\n",
       "      <td>39</td>\n",
       "      <td>DELSTRSN</td>\n",
       "      <td>76_LOOP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.98913914, 0.009340478, 0.0015203691, 8.8738...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1812</td>\n",
       "      <td>40.0</td>\n",
       "      <td>DELSTRSN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37432</th>\n",
       "      <td>264427</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>2021-11-23 09:10:14</td>\n",
       "      <td>2021-11-23 09:11:25</td>\n",
       "      <td>7600</td>\n",
       "      <td>40</td>\n",
       "      <td>OLD4AVWF</td>\n",
       "      <td>76_LOOP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.9560493, 0.03614658, 0.007804037, 7.252601e...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1812</td>\n",
       "      <td>40.0</td>\n",
       "      <td>OLD4AVWF</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37433</th>\n",
       "      <td>264427</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>2021-11-23 09:10:36</td>\n",
       "      <td>2021-11-23 09:12:38</td>\n",
       "      <td>7600</td>\n",
       "      <td>41</td>\n",
       "      <td>OLD3AVWM</td>\n",
       "      <td>76_LOOP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.88010883, 0.098540016, 0.021349994, 1.21099...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1812</td>\n",
       "      <td>40.0</td>\n",
       "      <td>OLD3AVWM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37434</th>\n",
       "      <td>264427</td>\n",
       "      <td>2021-11-23</td>\n",
       "      <td>2021-11-23 09:11:36</td>\n",
       "      <td>2021-11-23 09:14:13</td>\n",
       "      <td>7600</td>\n",
       "      <td>42</td>\n",
       "      <td>GALMAPSN</td>\n",
       "      <td>76_LOOP</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.846465, 0.12590528, 0.027625099, 4.6414466e...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1812</td>\n",
       "      <td>40.0</td>\n",
       "      <td>GALMAPSN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1358 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       trip_id transit_date        arrival_time      scheduled_time  \\\n",
       "16372   260005   2021-11-23 2021-11-23 08:02:01 2021-11-23 08:00:00   \n",
       "16373   260005   2021-11-23 2021-11-23 08:02:02 2021-11-23 08:00:15   \n",
       "16374   260005   2021-11-23 2021-11-23 08:02:20 2021-11-23 08:00:41   \n",
       "16375   260005   2021-11-23 2021-11-23 08:03:08 2021-11-23 08:01:01   \n",
       "16376   260005   2021-11-23 2021-11-23 08:03:46 2021-11-23 08:01:40   \n",
       "...        ...          ...                 ...                 ...   \n",
       "37430   264427   2021-11-23 2021-11-23 09:07:46 2021-11-23 09:08:52   \n",
       "37431   264427   2021-11-23 2021-11-23 09:08:26 2021-11-23 09:10:21   \n",
       "37432   264427   2021-11-23 2021-11-23 09:10:14 2021-11-23 09:11:25   \n",
       "37433   264427   2021-11-23 2021-11-23 09:10:36 2021-11-23 09:12:38   \n",
       "37434   264427   2021-11-23 2021-11-23 09:11:36 2021-11-23 09:14:13   \n",
       "\n",
       "       block_abbr  stop_sequence stop_id_original    route_id_dir  \\\n",
       "16372        2201              1          CLSFCLK  22_TO DOWNTOWN   \n",
       "16373        2201              2         CLICLANM  22_TO DOWNTOWN   \n",
       "16374        2201              3         CLIBUENM  22_TO DOWNTOWN   \n",
       "16375        2201              4         CLIBUENN  22_TO DOWNTOWN   \n",
       "16376        2201              5         BUEKIRNF  22_TO DOWNTOWN   \n",
       "...           ...            ...              ...             ...   \n",
       "37430        7600             38         DELCUMSN         76_LOOP   \n",
       "37431        7600             39         DELSTRSN         76_LOOP   \n",
       "37432        7600             40         OLD4AVWF         76_LOOP   \n",
       "37433        7600             41         OLD3AVWM         76_LOOP   \n",
       "37434        7600             42         GALMAPSN         76_LOOP   \n",
       "\n",
       "       zero_load_at_trip_end  y_pred_classes  \\\n",
       "16372                      0               0   \n",
       "16373                      0               0   \n",
       "16374                      0               0   \n",
       "16375                      0               0   \n",
       "16376                      0               0   \n",
       "...                      ...             ...   \n",
       "37430                      0               0   \n",
       "37431                      0               0   \n",
       "37432                      0               0   \n",
       "37433                      0               0   \n",
       "37434                      0               0   \n",
       "\n",
       "                                            y_pred_probs  sampled_loads  \\\n",
       "16372                               [-1, -1, -1, -1, -1]            1.0   \n",
       "16373                               [-1, -1, -1, -1, -1]            1.0   \n",
       "16374                               [-1, -1, -1, -1, -1]            2.0   \n",
       "16375                               [-1, -1, -1, -1, -1]            2.0   \n",
       "16376                               [-1, -1, -1, -1, -1]            2.0   \n",
       "...                                                  ...            ...   \n",
       "37430  [0.88960415, 0.086769514, 0.0236245, 1.8498251...            1.0   \n",
       "37431  [0.98913914, 0.009340478, 0.0015203691, 8.8738...            5.0   \n",
       "37432  [0.9560493, 0.03614658, 0.007804037, 7.252601e...            6.0   \n",
       "37433  [0.88010883, 0.098540016, 0.021349994, 1.21099...            4.0   \n",
       "37434  [0.846465, 0.12590528, 0.027625099, 4.6414466e...            2.0   \n",
       "\n",
       "      vehicle_id  vehicle_capacity   stop_id  timepoint  \n",
       "16372       1817              40.0   CLSFCLK          1  \n",
       "16373       1817              40.0  CLICLANM          0  \n",
       "16374       1817              40.0  CLIBUENM          0  \n",
       "16375       1817              40.0  CLIBUENN          0  \n",
       "16376       1817              40.0  BUEKIRNF          0  \n",
       "...          ...               ...       ...        ...  \n",
       "37430       1812              40.0  DELCUMSN          0  \n",
       "37431       1812              40.0  DELSTRSN          0  \n",
       "37432       1812              40.0  OLD4AVWF          0  \n",
       "37433       1812              40.0  OLD3AVWM          0  \n",
       "37434       1812              40.0  GALMAPSN          1  \n",
       "\n",
       "[1358 rows x 16 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# Vehicle assignments\n",
    "# Each vehicle config is a dict: {vehicle_capacity, blocks}\n",
    "DEFAULT_CAPACITY = 40.0\n",
    "overall_vehicle_plan = {}\n",
    "\n",
    "fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "trip_res_df = pd.read_pickle(fp)\n",
    "trip_res_df = trip_res_df[trip_res_df['vehicle_id'].isin(vehicle_list)]\n",
    "print(trip_res_df.trip_id.unique().shape)\n",
    "print(trip_res_df.vehicle_id.unique().shape)\n",
    "\n",
    "start_datetime = dt.datetime.strptime(f\"{DATE} {start_time}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "end_datetime = dt.datetime.strptime(f\"{DATE} {end_time}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "arr = []\n",
    "for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "    if (trip_df.scheduled_time.min() >= start_datetime) and (trip_df.scheduled_time.max() <= end_datetime):\n",
    "        arr.append(trip_df)\n",
    "\n",
    "trip_res_df = pd.concat(arr)\n",
    "print(trip_res_df.trip_id.unique().shape)\n",
    "print(trip_res_df.vehicle_id.unique().shape)\n",
    "trip_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: run again with vehicle_capacity (above)\n",
    "for vehicle_id, vehicle_df in trip_res_df.groupby('vehicle_id'):\n",
    "    vehicle_df = vehicle_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    vehicle_capacity = vehicle_df.iloc[0].vehicle_capacity\n",
    "    # vehicle_capacity = DEFAULT_CAPACITY\n",
    "    if np.isnan(vehicle_capacity):\n",
    "        vehicle_capacity = DEFAULT_CAPACITY\n",
    "    # TODO: This is not the baseline behavior\n",
    "    starting_depot = 'MCC5_1'\n",
    "    service_type = 'regular'\n",
    "    blocks = [block for block in vehicle_df.block_abbr.unique().tolist()]\n",
    "    trips = []\n",
    "    for block in blocks:\n",
    "        block_df = vehicle_df.query(\"block_abbr == @block\")\n",
    "        for trip in block_df.trip_id.unique().tolist():\n",
    "            trips.append((str(block), str(trip)))\n",
    "    overall_vehicle_plan[vehicle_id] = {'vehicle_capacity': vehicle_capacity, 'trips': trips, 'starting_depot': starting_depot, 'service_type': service_type}\n",
    "    \n",
    "len(overall_vehicle_plan)\n",
    "\n",
    "# Number of overload buses\n",
    "#   \"42\": {\n",
    "#     \"service_type\": \"overload\",\n",
    "#     \"starting_depot\": \"MCC5_1\",\n",
    "#     \"trips\": [\n",
    "#     ],\n",
    "#     \"vehicle_capacity\": 55.0\n",
    "#   }\n",
    "OVERLOAD_BUSES = 5\n",
    "for vehicle_id in range(41, 41 + OVERLOAD_BUSES):\n",
    "    overall_vehicle_plan[str(vehicle_id)] = {'vehicle_capacity': 55.0, 'trips': [], \"starting_depot\": \"MCC5_1\", 'service_type': \"overload\"}\n",
    "    \n",
    "with open(f'results/vehicle_plan_{DATE.replace(\"-\", \"\")}_10_limited.json', 'w') as fp:\n",
    "    json.dump(overall_vehicle_plan, fp, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Trip plan (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res_df = pd.read_pickle(fp)\n",
    "\n",
    "# Create a dict of {[block: {trip_ids:[]}, 'block'....]}\n",
    "# trip_id dict = {'route_id', route_direction_name', 'stop_id':[], 'schedule_time':[]}\n",
    "# Use block as grouper in baseline\n",
    "overall_block_plan = {}\n",
    "for block_abbr, block_df in trip_res_df.groupby('block_abbr'):\n",
    "    block_df = block_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    trip_ids = block_df.trip_id.unique().tolist()\n",
    "    start_time = block_df[block_df['trip_id'] == trip_ids[0]].iloc[0]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    end_time = block_df[block_df['trip_id'] == trip_ids[-1]].iloc[-1]['scheduled_time'].strftime('%Y-%m-%d %H:%M:%S')\n",
    "    overall_block_plan[block_abbr] = {'trip_ids': trip_ids,\n",
    "                                      'start_time': start_time,\n",
    "                                      'end_time': end_time}\n",
    "\n",
    "overall_trip_plan = {}\n",
    "for trip_id, trip_df in trip_res_df.groupby('trip_id'):\n",
    "    trip_df = trip_df.dropna(subset=['arrival_time']).sort_values(['scheduled_time'])\n",
    "    route_id_dir = trip_df.iloc[0].route_id_dir\n",
    "    route_id = int(route_id_dir.split(\"_\")[0])\n",
    "    route_direction = route_id_dir.split(\"_\")[1]\n",
    "    zero_load_at_trip_end = trip_df.iloc[-1].zero_load_at_trip_end.tolist()\n",
    "    scheduled_time = trip_df.scheduled_time.dt.strftime('%Y-%m-%d %H:%M:%S').tolist()\n",
    "    stop_sequence = trip_df.stop_sequence.tolist()\n",
    "    stop_sequence = list(range(0, len(stop_sequence)))\n",
    "    # stop_sequence = [ss - 1 for ss in stop_sequence]\n",
    "    stop_id_original = trip_df.stop_id_original.tolist()\n",
    "    \n",
    "    overall_trip_plan[trip_id] = {'route_id': route_id, \n",
    "                                  'route_direction': route_direction, \n",
    "                                  'scheduled_time': scheduled_time, \n",
    "                                  'stop_sequence': stop_sequence, \n",
    "                                  'stop_id_original': stop_id_original,\n",
    "                                  'zero_load_at_trip_end':zero_load_at_trip_end,\n",
    "                                  'last_stop_sequence': stop_sequence[-1],\n",
    "                                  'last_stop_id': stop_id_original[-1]}\n",
    "\n",
    "len(overall_trip_plan), len(overall_block_plan)\n",
    "\n",
    "with open(f'results/trip_plan_{DATE.replace(\"-\", \"\")}_10_limited.json', 'w') as fp:\n",
    "    json.dump(overall_trip_plan, fp, sort_keys=True, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(trip_res_df.query(\"trip_id == 259274\").head())\n",
    "print(trip_res_df.query(\"trip_id == 259274\").shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.query(\"trip_id == '243423'\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_id_dir = \n",
    "# trip_res.query(\"route_id_dir == @route_id_dir and block_abbr == @block and stop_id_original == @stop_id_original[@i] and scheduled_time == @scheduled_time[@i]\").iloc[0]['sampled_loads']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ons and offs from sampled loads\n",
    "* Needs the trip_res generated above\n",
    " ```\n",
    " fp = 'results/sampled_loads.pkl'\n",
    " trip_res.to_pickle(fp)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "    \n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "sampled_ons_offs = []\n",
    "for trip_id, trip_id_df in tqdm(trip_res.groupby(['transit_date', 'trip_id'])):\n",
    "    tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "    tdf['ons'] = 0\n",
    "    tdf['offs'] = 0\n",
    "    tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "    \n",
    "    # Intermediate stops\n",
    "    tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # first and last stops\n",
    "    tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "    tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "    sampled_ons_offs.append(tdf)\n",
    "    \n",
    "sampled_ons_offs = pd.concat(sampled_ons_offs)\n",
    "sampled_ons_offs = sampled_ons_offs.drop('next_load', axis=1)\n",
    "\n",
    "# fp = f'results/sampled_ons_offs_{DATE.replace(\"-\", \"\")}.pkl'\n",
    "# sampled_ons_offs.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a single event chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in trip_res.y_pred_classes]\n",
    "trip_res['sampled_loads'] = loads\n",
    "\n",
    "sampled_ons_offs = []\n",
    "for trip_id, trip_id_df in tqdm(trip_res.groupby(['transit_date', 'trip_id'])):\n",
    "    tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "    tdf['stop_sequence'] = list(range(1, len(tdf) + 1))\n",
    "    tdf['ons'] = 0\n",
    "    tdf['offs'] = 0\n",
    "    tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "    \n",
    "    # Intermediate stops\n",
    "    tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "    \n",
    "    # first and last stops\n",
    "    tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "    tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "    sampled_ons_offs.append(tdf)\n",
    "    \n",
    "df = pd.concat(sampled_ons_offs)\n",
    "df = df.drop('next_load', axis=1)\n",
    "\n",
    "display(df)\n",
    "df['key_pair'] = list(zip(df.route_id_dir, \n",
    "                          df.block_abbr,\n",
    "                          df.stop_sequence,\n",
    "                          df.stop_id_original, \n",
    "                          df.scheduled_time))\n",
    "df = df.set_index('key_pair')\n",
    "drop_cols = ['trip_id', 'route_id_dir', 'block_abbr', 'stop_id_original', 'stop_id', 'scheduled_time', \n",
    "                'transit_date', 'arrival_time', 'zero_load_at_trip_end', 'y_pred_classes', 'y_pred_probs',\n",
    "                'vehicle_capacity', 'vehicle_id', 'stop_sequence']\n",
    "drop_cols = [dc for dc in drop_cols if dc in df.columns]\n",
    "df = df.drop(drop_cols, axis=1)\n",
    "sampled_ons_offs_dict = df.to_dict('index')\n",
    "\n",
    "import pickle \n",
    "\n",
    "# with open(f'results/sampled_ons_offs_dict_{DATE.replace(\"-\", \"\")}.pkl', 'wb') as handle:\n",
    "#     pickle.dump(sampled_ons_offs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[('23_FROM DOWNTOWN', 2310, 'DWMRT', pd.Timestamp('2021-08-23 05:41:00'))]\n",
    "df.query(\"route_id_dir == '23_FROM DOWNTOWN' and block_abbr == 2310 and stop_id_original == 'DWMRT' and scheduled_time == '2021-08-23 05:41:00'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = list(sampled_ons_offs_dict.keys())[0]\n",
    "print(key)\n",
    "sampled_ons_offs_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.query(\"route_id_dir == '7_TO DOWNTOWN' and block_abbr == 5692 and stop_sequence == 20 and stop_id_original == 'MCC5_9'\")\n",
    "# df.query(\"route_id_dir == '7_TO DOWNTOWN' and block_abbr == 5692\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating multiple event chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compute_ons_offs(s):\n",
    "    curr_load = s['sampled_loads']\n",
    "    next_load = s['next_load']\n",
    "    if next_load > curr_load:\n",
    "        ons = next_load - curr_load\n",
    "        offs = 0\n",
    "    elif next_load < curr_load:\n",
    "        ons = 0\n",
    "        offs = curr_load - next_load\n",
    "    else:\n",
    "        ons = 0\n",
    "        offs = 0\n",
    "        \n",
    "    return ons, offs\n",
    "\n",
    "CHAINS = 5\n",
    "percentiles = [(0, 6.0), (6.0, 12.0), (12.0, 55.0), (55.0, 75.0), (75.0, 100.0)]\n",
    "\n",
    "# fp = f'results/sampled_loads_{DATE.replace(\"-\",\"\")}.pkl'\n",
    "# trip_res = pd.read_pickle(fp)\n",
    "trip_res = _trip_res\n",
    "for chain in tqdm(range(CHAINS)):\n",
    "    loads = [random.randint(percentiles[yp][0], percentiles[yp][1]) for yp in trip_res.y_pred_classes]\n",
    "    trip_res['sampled_loads'] = loads\n",
    "\n",
    "    sampled_ons_offs = []\n",
    "    for trip_id, trip_id_df in trip_res.groupby(['transit_date', 'trip_id']):\n",
    "        tdf = trip_id_df.sort_values('stop_sequence').reset_index(drop=True)\n",
    "        tdf['stop_sequence'] = list(range(1, len(tdf) + 1))\n",
    "        tdf['ons'] = 0\n",
    "        tdf['offs'] = 0\n",
    "        tdf['next_load'] = tdf['sampled_loads'].shift(-1)\n",
    "        \n",
    "        # Intermediate stops\n",
    "        tdf[['ons', 'offs']] = tdf.apply(compute_ons_offs, axis=1, result_type=\"expand\")\n",
    "        \n",
    "        # first and last stops\n",
    "        tdf.at[0, 'ons'] = tdf.iloc[0]['sampled_loads']\n",
    "        tdf.at[len(tdf) - 1, 'offs'] = tdf.iloc[-1]['sampled_loads']\n",
    "        sampled_ons_offs.append(tdf)\n",
    "        \n",
    "    df = pd.concat(sampled_ons_offs)\n",
    "    df['key_pair'] = list(zip(df.route_id_dir, \n",
    "                            df.block_abbr,\n",
    "                            df.stop_sequence,\n",
    "                            df.stop_id_original, \n",
    "                            df.scheduled_time))\n",
    "    df = df.set_index('key_pair')\n",
    "    drop_cols = ['trip_id', 'route_id_dir', 'block_abbr', 'stop_id_original', 'stop_id', 'scheduled_time', \n",
    "                 'transit_date', 'arrival_time', 'zero_load_at_trip_end', 'y_pred_classes', 'y_pred_probs',\n",
    "                 'vehicle_capacity', 'vehicle_id', 'stop_sequence']\n",
    "    drop_cols = [dc for dc in drop_cols if dc in df.columns]\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "    sampled_ons_offs_dict = df.to_dict('index')\n",
    "\n",
    "    with open(f'results/chains/ons_offs_dict_chain_{DATE.replace(\"-\",\"\")}_{chain}.pkl', 'wb') as handle:\n",
    "        pickle.dump(sampled_ons_offs_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create timepoint dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_times_df.iloc[0].arrival_time[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(x):\n",
    "    if x[0:2] == '24':\n",
    "        return '00'+x[2:]\n",
    "    if x[0:2] == '25':\n",
    "        return '01'+x[2:]\n",
    "    return x\n",
    "    \n",
    "stop_times_fp = 'data/GTFS/OCT2021/stop_times.txt'\n",
    "stop_times_df = pd.read_csv(stop_times_fp)\n",
    "# stop_times_df.query(\"trip_id == 264733\")\n",
    "stop_times_df['date'] = DATE\n",
    "stop_times_df['arrival_time'] = stop_times_df['arrival_time'].apply(lambda x: fix_time(x))\n",
    "stop_times_df['scheduled_time'] = pd.to_datetime(stop_times_df['date'] + ' ' + stop_times_df['arrival_time'])\n",
    "\n",
    "stop_times_df['key_pair'] = list(zip(stop_times_df.trip_id, stop_times_df.stop_id, stop_times_df.scheduled_time))\n",
    "stop_times_df = stop_times_df.set_index('key_pair')\n",
    "\n",
    "time_point_dict = stop_times_df.drop(['arrival_time', 'departure_time', 'stop_id', 'stop_sequence', 'stop_headsign', 'trip_id',\n",
    "                                      'pickup_type', 'drop_off_type', 'shape_dist_traveled', 'scheduled_time', 'date'], axis=1).to_dict('index')\n",
    "with open(f'results/time_point_dict_{DATE.replace(\"-\", \"\")}.pkl', 'wb') as handle:\n",
    "    pickle.dump(time_point_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# time_point_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, tp) for k, tp in time_point_dict.items() if k[0] == 263558][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_point_dict[(263558, 'GALBERNN', pd.Timestamp('2021-10-18 05:47:59'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT: \n",
    "* Copy these to `scenarios/baselines/data`\n",
    "    * results/sampled_ons_offs_dict\n",
    "    * results/chains/ons_offs_dict_chain_{chain}.pkl\n",
    "    * results/trip_plan.json\n",
    "    * results/vehicle_plan.json\n",
    "    * results/time_point_dict.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure:\n",
    "```\n",
    "{key:val}\n",
    "key: (tuple) (route_id_dir, block_abbr, stop_sequene, stop_id, scheduled_arrival_time)\n",
    "val: (dict) {'sampled_loads': A, 'ons': B, 'offs': C}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sanity check\n",
    "CHAINS = 5\n",
    "for chain in range(CHAINS):\n",
    "    with open(f'results/chains/ons_offs_dict_chain_{chain}.pkl', 'rb') as handle:\n",
    "        sampled_ons_offs_dict = pickle.load(handle)\n",
    "    # res = sampled_ons_offs_dict[('7_TO DOWNTOWN', 5692, 20, 'MCC5_9', pd.Timestamp('2021-08-23 14:39:00'))]\n",
    "    res = sampled_ons_offs_dict[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', pd.Timestamp('2021-10-18 14:15:00'))]\n",
    "    print(f\"chain {chain}: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sampled_ons_offs_dict.keys())[0], list(sampled_ons_offs_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "with open(f'results/chains/ons_offs_dict_chain_0.pkl', 'rb') as handle:\n",
    "    sampled_ons_offs_dict = pickle.load(handle)\n",
    "# ('7_TO DOWNTOWN', 5692, 1, 'HBHS', datetime.datetime(2021, 8, 23, 14, 9))\n",
    "# sampled_ons_offs_dict[('7_TO DOWNTOWN', 5692, 1, 'HBHS', dt.datetime(2021, 8, 23, 14, 9))]\n",
    "search_key = ('7_TO DOWNTOWN', 5692, 5)\n",
    "values = [value for key, value in sampled_ons_offs_dict.items() if search_key == key[:len(search_key)]]\n",
    "keys = [key for key, value in sampled_ons_offs_dict.items() if search_key == key[:len(search_key)]]\n",
    "values, keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(sampled_ons_offs_dict.keys())[0], list(sampled_ons_offs_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "sampled_ons_offs_dict[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))]\n",
    "('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))\n",
    "('14_FROM DOWNTOWN', '1400', 1, 'MCC4_20', dt.datetime(2021, 8, 23, 14, 15))\n",
    "# ('14_FROM DOWNTOWN', '1400', 1, 'MCC4_20', datetime.datetime(2021, 8, 23, 14, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 263159\n",
    "import pandas as pd\n",
    "\n",
    "fp = 'results/sampled_ons_offs_dict_20211018.pkl'\n",
    "df = pd.read_pickle(fp)\n",
    "list(df.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = '55_FROM DOWNTOWN'\n",
    "sid = 'MCC4_15'\n",
    "time = '2021-10-18 15:35:00'\n",
    "\n",
    "df[('14_FROM DOWNTOWN', 1400, 1, 'MCC4_20', pd.Timestamp('2021-10-18 14:15:00'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(k, v)for k, v in df.items() if k[0] == rid and k[3] == sid and k[4] == pd.Timestamp(time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "fp = '/home/jptalusan/gits/mta_simulator_redo/data_generation/results/sampled_ons_offs_dict_20220305.pkl'\n",
    "\n",
    "with open(fp, 'rb') as handle:\n",
    "    sampled_ons_offs_dict = pickle.load(handle)\n",
    "sampled_ons_offs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
