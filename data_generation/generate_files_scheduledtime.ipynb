{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampled using scheduled times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import gtfs_kit as gk\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon, LineString, Point\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 18:39:59 WARN Utils: Your hostname, scope-vanderbilt resolves to a loopback address: 127.0.1.1; using 10.2.218.69 instead (on interface enp8s0)\n",
      "22/10/04 18:39:59 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/04 18:40:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config('spark.executor.cores', '8').config('spark.executor.memory', '80g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '40g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "        .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "        .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gtfs_date', 'dayofweek', 'hour', 'gtfs_route_id', 'gtfs_direction_id', 'stop_id', 'transit_date', 'trip_id', 'day', 'overload_id', 'vehicle_id', 'block_abbr', 'activation_date', 'activation_date_str', 'arrival_time', 'arrival_time_str', 'block_stop_order', 'deactivation_date', 'deactivation_date_str', 'departure_time', 'departure_time_str', 'load', 'load_factor', 'map_latitude', 'map_longitude', 'offs', 'ons', 'pattern_num', 'route_direction_name', 'route_id', 'scheduled_time', 'scheduled_time_str', 'source_pattern_id', 'stop_id_list', 'stop_id_original', 'stop_name', 'stop_sequence', 'stop_sequence_list', 'transit_date_str', 'update_date', 'vehicle_capacity', 'zero_load_at_trip_end', 'count', 'darksky_temperature', 'darksky_humidity', 'darksky_nearest_storm_distance', 'darksky_precipitation_intensity', 'darksky_precipitation_probability', 'darksky_pressure', 'darksky_wind_gust', 'darksky_wind_speed', 'weatherbit_rh', 'weatherbit_wind_spd', 'weatherbit_app_temp', 'weatherbit_temp', 'weatherbit_snow', 'weatherbit_precip', 'gtfs_file', 'gtfs_shape_id', 'gtfs_start_date', 'gtfs_end_date', 'gtfs_number_of_scheduled_trips', 'gtfs_number_of_scheduled_trips_at_stop', 'delay_time', 'dwell_time', 'prev_sched', 'sched_hdwy', 'prev_depart', 'actual_hdwy', 'is_gapped', 'is_bunched', 'is_target', 'year', 'month']\n",
      "\n",
      "       SELECT trip_id, transit_date, arrival_time, block_abbr, scheduled_time, vehicle_id, stop_sequence, stop_id_original, load, ons, offs, route_id, route_direction_name\n",
      "       FROM apc\n",
      "       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "f = os.path.join('/home/jptalusan/mta_stationing_problem/data/processed/apc_weather_gtfs.parquet')\n",
    "apcdata = spark.read.load(f)\n",
    "print(apcdata.columns)\n",
    "get_columns = ['trip_id', 'transit_date', 'arrival_time', 'block_abbr', 'scheduled_time', 'vehicle_id',\n",
    "              'stop_sequence', 'stop_id_original', 'load', 'ons', 'offs', 'route_id', 'route_direction_name']\n",
    "get_str = \", \".join([c for c in get_columns])\n",
    "apcdata.createOrReplaceTempView(\"apc\")\n",
    "# # filter subset\n",
    "query = f\"\"\"\n",
    "       SELECT {get_str}\n",
    "       FROM apc\n",
    "       \"\"\"\n",
    "print(query)\n",
    "apcdata = spark.sql(query)\n",
    "apcdata = apcdata.withColumn(\"route_id_direction\", F.concat_ws('_',apcdata.route_id, apcdata.route_direction_name))\n",
    "apcdata = apcdata.drop('route_id', 'route_direction_name')\n",
    "apcdf = apcdata.toPandas()\n",
    "fp = os.path.join('results/simulator_baseline.pkl')\n",
    "apcdf.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup map\n",
    "* Match locations with vertices on the map\n",
    "* Need to update and add data to `_redo` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'shapefiles', \"tncounty\")\n",
    "gdf_county = gpd.read_file(fp)\n",
    "gdf_dav = gdf_county[gdf_county[\"NAME\"] == \"Davidson\"]\n",
    "gdf_dav = gdf_dav.to_crs(\"EPSG:4326\")\n",
    "\n",
    "G = ox.graph_from_polygon(gdf_dav.geometry.iloc[0], network_type='drive')\n",
    "G = ox.add_edge_speeds(G)\n",
    "G = ox.add_edge_travel_times(G)\n",
    "\n",
    "fp = os.path.join('data', 'davidson_graph.graphml')\n",
    "ox.save_graphml(G, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "f = os.path.join('/home/jptalusan/mta_stationing_problem/data/processed/apc_weather_gtfs.parquet')\n",
    "apcdata = spark.read.load(f)\n",
    "apcdata.columns\n",
    "get_columns = ['stop_sequence', 'stop_id_original', 'stop_name', 'map_latitude', 'map_longitude']\n",
    "get_str = \", \".join([c for c in get_columns])\n",
    "apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "# # filter subset\n",
    "query = f\"\"\"\n",
    "SELECT {get_str}\n",
    "FROM apc\n",
    "\"\"\"\n",
    "apcdata = spark.sql(query)\n",
    "apcdata = apcdata.drop_duplicates(['stop_id_original'])\n",
    "apcdf = apcdata.toPandas()\n",
    "\n",
    "apcdf['nearest_node'] = ox.nearest_nodes(G, apcdf['map_longitude'], apcdf['map_latitude'])\n",
    "apcdf['nearest_edge'] = ox.nearest_edges(G, apcdf['map_longitude'], apcdf['map_latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('results', 'stops_node_matching.pkl')\n",
    "apcdf.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ox.shortest_path(G, 202177835, 9702091174, weight='travel_time')\n",
    "ox.plot_graph_route(G, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = apcdf[apcdf['stop_id_original'] == 'MCC4_24']['nearest_node'].values[0]\n",
    "nn = apcdf[apcdf['stop_id_original'] == 'UNI2AEF']['nearest_node'].values[0]\n",
    "r = ox.shortest_path(G, cn, nn, weight='travel_time')\n",
    "cols = ['osmid', 'length', 'travel_time']\n",
    "attrs = ox.utils_graph.get_route_edge_attributes(G, r)\n",
    "tt = pd.DataFrame(attrs)[cols]['travel_time'].sum()\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert APC load data to something static and fast to access (look up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# fp = os.path.join('results/simulator_baseline.pkl')\n",
    "# baseline_data = pd.read_pickle(fp)\n",
    "# baseline_data['dow'] = baseline_data['scheduled_time'].dt.dayofweek\n",
    "# baseline_data['IsWeekend'] = (baseline_data[\"scheduled_time\"].dt.weekday >= 5).astype('int')\n",
    "# baseline_data['time'] = baseline_data['scheduled_time'].dt.time\n",
    "# baseline_data = baseline_data.query(\"load >= 0 and load <= 100\")\n",
    "# baseline_data = baseline_data.groupby(['route_id_direction', 'block_abbr', 'stop_id_original', 'time', 'IsWeekend']).agg({'load':list})\n",
    "# fp = os.path.join('results/loads_by_scheduled_time.pkl')\n",
    "# baseline_data.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For boarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# # fp = os.path.join('results/simulator_baseline.pkl')\n",
    "# baseline_data = pd.read_pickle(fp)\n",
    "# baseline_data['dow'] = baseline_data['scheduled_time'].dt.dayofweek\n",
    "# baseline_data['IsWeekend'] = (baseline_data[\"scheduled_time\"].dt.weekday >= 5).astype('int')\n",
    "# baseline_data['time'] = baseline_data['scheduled_time'].dt.time\n",
    "# baseline_data = baseline_data.query(\"ons >= 0 and ons <= 100\")\n",
    "# baseline_data = baseline_data.groupby(['route_id_direction', 'block_abbr', 'stop_id_original', 'time', 'IsWeekend']).agg({'ons':list})\n",
    "# fp = os.path.join('results/ons_by_scheduled_time.pkl')\n",
    "# baseline_data.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For alighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# fp = os.path.join('results/simulator_baseline.pkl')\n",
    "# baseline_data = pd.read_pickle(fp)\n",
    "# baseline_data['dow'] = baseline_data['scheduled_time'].dt.dayofweek\n",
    "# baseline_data['IsWeekend'] = (baseline_data[\"scheduled_time\"].dt.weekday >= 5).astype('int')\n",
    "# baseline_data['time'] = baseline_data['scheduled_time'].dt.time\n",
    "# baseline_data = baseline_data.query(\"offs >= 0 and offs <= 100\")\n",
    "# baseline_data = baseline_data.groupby(['route_id_direction', 'block_abbr', 'stop_id_original', 'time', 'IsWeekend']).agg({'offs':list})\n",
    "# fp = os.path.join('results/offs_by_scheduled_time.pkl')\n",
    "# baseline_data.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Travel Times\n",
    "* for each route_id_dir, block, stop to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('results/simulator_baseline.pkl')\n",
    "baseline_data = pd.read_pickle(fp).dropna(subset=['arrival_time']).sort_values(by=['transit_date', 'trip_id', 'stop_sequence'])\n",
    "baseline_data['dow'] = baseline_data['scheduled_time'].dt.dayofweek\n",
    "baseline_data['IsWeekend'] = (baseline_data[\"scheduled_time\"].dt.weekday >= 5).astype('int')\n",
    "baseline_data['time'] = baseline_data['scheduled_time'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import datetime as dt\n",
    "\n",
    "def get_traveltimes(tdf):\n",
    "    tdf = tdf.sort_values('stop_sequence')\n",
    "    if len(tdf) <= 2:\n",
    "        return pd.DataFrame()\n",
    "    # HACK: This is for correcting the issue that the first stop's arrival_time starts much earlier than the scheduled time\n",
    "\n",
    "    tdf = tdf.reset_index(drop=True)\n",
    "    tdf['scheduled_timestamp'] = (tdf['arrival_time'] - dt.datetime(1970,1,1)).dt.total_seconds()\n",
    "    tdf['time_to_next_stop'] = tdf['scheduled_timestamp'].shift(-1) - tdf['scheduled_timestamp']\n",
    "    tdf.at[0, 'time_to_next_stop'] = (tdf.at[1, 'arrival_time'] - tdf.at[0, 'scheduled_time']).total_seconds()\n",
    "    tdf = tdf.drop('scheduled_timestamp', axis=1)\n",
    "    tdf = tdf.fillna(0)\n",
    "    return tdf\n",
    "    \n",
    "def applyParallel(dfGrouped, func):\n",
    "    with Pool(cpu_count()) as p:\n",
    "        ret_list = p.map(func, [group for name, group in dfGrouped])\n",
    "    return pd.concat(ret_list)\n",
    "\n",
    "out_arr = applyParallel(baseline_data.groupby(['block_abbr', 'route_id_direction', 'transit_date', 'trip_id']), get_traveltimes)\n",
    "tdf = out_arr.groupby(['route_id_direction', 'block_abbr', 'stop_sequence', 'stop_id_original', 'time', 'IsWeekend']).agg({'time_to_next_stop':list})\n",
    "# fp = os.path.join('results/travel_time_by_scheduled_time.pkl')\n",
    "# tdf.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample travel times for a single day or chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "fp = os.path.join('results/travel_time_by_scheduled_time.pkl')\n",
    "tdf = pd.read_pickle(fp)\n",
    "\n",
    "random.seed(100)\n",
    "\n",
    "tdf = tdf.reset_index()\n",
    "tdf['sampled_travel_time'] = tdf.reset_index()['time_to_next_stop'].apply(lambda x: random.choice(x))\n",
    "tdf['sampled_travel_time'] = abs(tdf['sampled_travel_time'])\n",
    "fp = os.path.join('results/sampled_travel_times.pkl')\n",
    "tdf.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id_direction</th>\n",
       "      <th>block_abbr</th>\n",
       "      <th>stop_sequence</th>\n",
       "      <th>stop_id_original</th>\n",
       "      <th>time</th>\n",
       "      <th>IsWeekend</th>\n",
       "      <th>time_to_next_stop</th>\n",
       "      <th>sampled_travel_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14_FROM DOWNTOWN</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_20</td>\n",
       "      <td>06:15:00</td>\n",
       "      <td>0</td>\n",
       "      <td>[362.0, 238.0, 150.0, 228.0, 246.0, 158.0, 276...</td>\n",
       "      <td>328.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14_FROM DOWNTOWN</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_20</td>\n",
       "      <td>06:15:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[264.0, 288.0, 204.0, 264.0, 270.0, 296.0, 268...</td>\n",
       "      <td>402.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14_FROM DOWNTOWN</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_20</td>\n",
       "      <td>06:18:00</td>\n",
       "      <td>0</td>\n",
       "      <td>[162.0, 164.0, 240.0, 168.0, 160.0, 240.0, 254...</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14_FROM DOWNTOWN</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_20</td>\n",
       "      <td>06:18:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[580.0, 136.0, 178.0, 230.0, 154.0, 262.0, 212...</td>\n",
       "      <td>232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14_FROM DOWNTOWN</td>\n",
       "      <td>1400</td>\n",
       "      <td>1</td>\n",
       "      <td>MCC4_20</td>\n",
       "      <td>07:15:00</td>\n",
       "      <td>0</td>\n",
       "      <td>[148.0, 148.0, 148.0, 240.0, 238.0, 234.0, 242...</td>\n",
       "      <td>244.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533884</th>\n",
       "      <td>9_TO DOWNTOWN</td>\n",
       "      <td>8600</td>\n",
       "      <td>6</td>\n",
       "      <td>2AVJUNSF</td>\n",
       "      <td>07:48:54</td>\n",
       "      <td>0</td>\n",
       "      <td>[26.0, 30.0, 42.0, 24.0, 24.0, 40.0, 24.0, 22....</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533885</th>\n",
       "      <td>9_TO DOWNTOWN</td>\n",
       "      <td>8600</td>\n",
       "      <td>6</td>\n",
       "      <td>2AVJUNSF</td>\n",
       "      <td>07:48:57</td>\n",
       "      <td>0</td>\n",
       "      <td>[34.0, 58.0, 34.0]</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533886</th>\n",
       "      <td>9_TO DOWNTOWN</td>\n",
       "      <td>8600</td>\n",
       "      <td>7</td>\n",
       "      <td>2ASTOSM</td>\n",
       "      <td>07:50:02</td>\n",
       "      <td>0</td>\n",
       "      <td>[228.0, 242.0, 169.0, 188.0, 284.0, 208.0, 266...</td>\n",
       "      <td>284.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533887</th>\n",
       "      <td>9_TO DOWNTOWN</td>\n",
       "      <td>8600</td>\n",
       "      <td>7</td>\n",
       "      <td>2ASTOSM</td>\n",
       "      <td>07:50:06</td>\n",
       "      <td>0</td>\n",
       "      <td>[262.0, 328.0, 197.0]</td>\n",
       "      <td>262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533888</th>\n",
       "      <td>9_TO DOWNTOWN</td>\n",
       "      <td>8600</td>\n",
       "      <td>8</td>\n",
       "      <td>MCC5_2</td>\n",
       "      <td>07:54:00</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>533889 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       route_id_direction  block_abbr  stop_sequence stop_id_original  \\\n",
       "0        14_FROM DOWNTOWN        1400              1          MCC4_20   \n",
       "1        14_FROM DOWNTOWN        1400              1          MCC4_20   \n",
       "2        14_FROM DOWNTOWN        1400              1          MCC4_20   \n",
       "3        14_FROM DOWNTOWN        1400              1          MCC4_20   \n",
       "4        14_FROM DOWNTOWN        1400              1          MCC4_20   \n",
       "...                   ...         ...            ...              ...   \n",
       "533884      9_TO DOWNTOWN        8600              6         2AVJUNSF   \n",
       "533885      9_TO DOWNTOWN        8600              6         2AVJUNSF   \n",
       "533886      9_TO DOWNTOWN        8600              7          2ASTOSM   \n",
       "533887      9_TO DOWNTOWN        8600              7          2ASTOSM   \n",
       "533888      9_TO DOWNTOWN        8600              8           MCC5_2   \n",
       "\n",
       "            time  IsWeekend  \\\n",
       "0       06:15:00          0   \n",
       "1       06:15:00          1   \n",
       "2       06:18:00          0   \n",
       "3       06:18:00          1   \n",
       "4       07:15:00          0   \n",
       "...          ...        ...   \n",
       "533884  07:48:54          0   \n",
       "533885  07:48:57          0   \n",
       "533886  07:50:02          0   \n",
       "533887  07:50:06          0   \n",
       "533888  07:54:00          0   \n",
       "\n",
       "                                        time_to_next_stop  sampled_travel_time  \n",
       "0       [362.0, 238.0, 150.0, 228.0, 246.0, 158.0, 276...                328.0  \n",
       "1       [264.0, 288.0, 204.0, 264.0, 270.0, 296.0, 268...                402.0  \n",
       "2       [162.0, 164.0, 240.0, 168.0, 160.0, 240.0, 254...                240.0  \n",
       "3       [580.0, 136.0, 178.0, 230.0, 154.0, 262.0, 212...                232.0  \n",
       "4       [148.0, 148.0, 148.0, 240.0, 238.0, 234.0, 242...                244.0  \n",
       "...                                                   ...                  ...  \n",
       "533884  [26.0, 30.0, 42.0, 24.0, 24.0, 40.0, 24.0, 22....                 32.0  \n",
       "533885                                 [34.0, 58.0, 34.0]                 58.0  \n",
       "533886  [228.0, 242.0, 169.0, 188.0, 284.0, 208.0, 266...                284.0  \n",
       "533887                              [262.0, 328.0, 197.0]                262.0  \n",
       "533888  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                  0.0  \n",
       "\n",
       "[533889 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate travel distance pairs for all stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_id</th>\n",
       "      <th>next_stop_id</th>\n",
       "      <th>shape_dist_traveled_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MCC4_20</td>\n",
       "      <td>UNI2AEF</td>\n",
       "      <td>0.5776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNI2AEF</td>\n",
       "      <td>1SWOONM</td>\n",
       "      <td>0.6813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1SWOONM</td>\n",
       "      <td>1SJAMNM</td>\n",
       "      <td>0.3601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1SJAMNM</td>\n",
       "      <td>N1SOLDNM</td>\n",
       "      <td>0.3169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N1SOLDNM</td>\n",
       "      <td>DICGRANN</td>\n",
       "      <td>0.8610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    stop_id next_stop_id  shape_dist_traveled_km\n",
       "0   MCC4_20      UNI2AEF                  0.5776\n",
       "1   UNI2AEF      1SWOONM                  0.6813\n",
       "2   1SWOONM      1SJAMNM                  0.3601\n",
       "3   1SJAMNM     N1SOLDNM                  0.3169\n",
       "4  N1SOLDNM     DICGRANN                  0.8610"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = '/media/seconddrive/JP/wego-occupancy-JP/data/static_gtfs/WeGoRawGTFS/04-october-2021-fixed.zip'\n",
    "feed = gk.read_feed(fp, dist_units='km')\n",
    "feed.validate()\n",
    "stop_times_df = gk.get_stop_times(feed)\n",
    "stop_pairs = []\n",
    "for trip_id, trip_df in stop_times_df.groupby('trip_id'):\n",
    "    trip_df['next_stop_id'] = trip_df['stop_id'].shift(-1)\n",
    "    trip_df = trip_df.fillna(0)\n",
    "    trip_df['shape_dist_traveled_km'] = (trip_df['shape_dist_traveled'].shift(-1) - trip_df['shape_dist_traveled'])\n",
    "    trip_df = trip_df[['stop_id', 'next_stop_id', 'shape_dist_traveled_km']][:-1]\n",
    "    stop_pairs.append(trip_df)\n",
    "stop_pairs = pd.concat(stop_pairs)\n",
    "stop_pairs = stop_pairs.drop_duplicates()\n",
    "stop_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('results/gtfs_distance_pairs_km.pkl')\n",
    "stop_pairs.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_time_path = '/home/jptalusan/gits/mta_simulator_redo/code_root/scenarios/baseline/data/sampled_travel_times.pkl'\n",
    "sampled_travel_time = pd.read_pickle(travel_time_path)\n",
    "sampled_travel_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('/home/jptalusan/gits/mta_simulator_redo/code_root/scenarios/baseline/data/travel_time_by_scheduled_time.pkl')\n",
    "tdf = pd.read_pickle(fp).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apcdf.query(\"trip_id == '219844'\").sort_values('scheduled_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apcdf.trip_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11 * 60 + 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tdf.reset_index().query(\"route_id_direction == '34_FROM DOWNTOWN' and block_abbr == 3400 and stop_id_original == 'MCC4_22'\")\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.loc[('23_FROM DOWNTOWN', 2311, 8, 'VAIBRIEM',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = tdf.loc[('23_FROM DOWNTOWN', 2311)]\n",
    "adf = adf.query('IsWeekend == 0')\n",
    "# a = 26\n",
    "# b = 48\n",
    "# # adf.query('time_window == @a or time_window == @b').sample(1)['time_to_next_stop'].values[0]\n",
    "# adf = adf.explode('time_to_next_stop').query('time_to_next_stop > 0').reset_index()\n",
    "adf = adf.explode('time_to_next_stop').reset_index()\n",
    "# adf[adf['time_window'].isin(range(22, 32))].sample(1)\n",
    "\n",
    "adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tdf.loc[('3_TO DOWNTOWN', 300, 1, 'WHICHASF', 45, 0)]['time_to_next_stop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fp = os.path.join('/home/jptalusan/gits/mta_simulator_redo/code_root/scenarios/baseline/data/gtfs_distance_pairs_km.pkl')\n",
    "stop_pairs = pd.read_pickle(fp)\n",
    "if len(stop_pairs.query(\"stop_id == 'JP'\")) > 0:\n",
    "    print(\"KP\")\n",
    "stop_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Disruption probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Disruption probabilities\n",
    "# Get service disruption dataset\n",
    "fp = os.path.join('code/data/Service Disruptions_07_2019_08_2022.csv')\n",
    "disruptions_df = pd.read_csv(fp)\n",
    "disruptions_df.head()\n",
    "disruptions_df['DATETIME'] = disruptions_df['DATE'] + ' ' + disruptions_df['TIME']\n",
    "disruptions_df['DATE'] = pd.to_datetime(disruptions_df['DATE'], format='%m/%d/%y', errors='coerce')\n",
    "disruptions_df['TIME'] = pd.to_datetime(disruptions_df['TIME'], format='%H:%M:%S', errors='coerce')\n",
    "disruptions_df['DATETIME'] = pd.to_datetime(disruptions_df['DATETIME'], format='%m/%d/%y %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Remove weather related disruptions\n",
    "# disruptions_df = disruptions_df[(disruptions_df['REASON'] != 'Weather')].sort_values(by=['DATETIME']).reset_index(drop=True)\n",
    "print('Shape:', disruptions_df.shape)\n",
    "# disruptions_df = disruptions_df.drop(columns=['COMMENTS'])\n",
    "disruptions_df['BLOCK'] = disruptions_df['BLOCK'].astype('int32')\n",
    "\n",
    "# Convert to spark dataframe for merging\n",
    "# disruptions_sp = spark.createDataFrame(disruptions_df)\n",
    "# disruptions_sp = disruptions_sp.withColumn(\"BLOCK\", F.col(\"BLOCK\").cast(IntegerType()))\n",
    "disruptions_counts_df = disruptions_df.groupby('START_STOP_ABBR').agg('count')[['REASON']].reset_index()\n",
    "disruptions_counts_df.sort_values('REASON')\n",
    "# Count the number of trips throughout this time\n",
    "start_date = disruptions_df.sort_values(by=['DATETIME']).iloc[0]['DATETIME']\n",
    "end_date   = disruptions_df.sort_values(by=['DATETIME']).iloc[-1]['DATETIME']\n",
    "start_date, end_date\n",
    "\n",
    "# # filter subset\n",
    "get_columns = ['transit_date', 'trip_id', 'departure_time', 'stop_id_original']\n",
    "get_str = \", \".join([c for c in get_columns])\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT {get_str}\n",
    "FROM apc\n",
    "WHERE (transit_date >= '{start_date.date()}') AND (transit_date <= '{end_date.date()}')\n",
    "\"\"\"\n",
    "print(query)\n",
    "\n",
    "apcdataafternegdelete = spark.sql(query)\n",
    "apcdataafternegdelete = apcdataafternegdelete.dropna()\n",
    "trips_df = apcdataafternegdelete.toPandas()\n",
    "trips_df = trips_df.groupby('stop_id_original').agg('count').sort_values('trip_id').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(trips_df, disruptions_counts_df[['START_STOP_ABBR', 'REASON']], left_on='stop_id_original', right_on='START_STOP_ABBR')\n",
    "all_stop_probabilities = trips_df[['stop_id_original']]\n",
    "all_stop_probabilities = pd.merge(all_stop_probabilities, merged_df[['stop_id_original', 'probability']], on='stop_id_original', how='outer').fillna(0)\n",
    "all_stop_probabilities.sort_values('probability')\n",
    "\n",
    "fp = os.path.join('code/Scenarios/data/disruption_probabilities.pkl')\n",
    "all_stop_probabilities.to_pickle(fp)\n",
    "\n",
    "merged_df['probability'] = merged_df['REASON'] / merged_df['transit_date']\n",
    "merged_df['probability'] = merged_df['probability']/merged_df['probability'].max()\n",
    "merged_df.sort_values('probability').tail(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
