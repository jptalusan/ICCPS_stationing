{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampled using scheduled times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jptalusan/anaconda3/envs/py39/lib/python3.9/site-packages/geopandas/_compat.py:112: UserWarning: The Shapely GEOS version (3.10.2-CAPI-1.16.0) is incompatible with the GEOS version PyGEOS was compiled with (3.10.1-CAPI-1.16.0). Conversions between both will be slow.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import gtfs_kit as gk\n",
    "import numpy as np\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from shapely.geometry import Polygon, LineString, Point\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row, SparkSession\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark import SparkConf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/23 15:53:46 WARN Utils: Your hostname, scope-vanderbilt resolves to a loopback address: 127.0.1.1; using 10.2.218.69 instead (on interface enp8s0)\n",
      "22/09/23 15:53:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/23 15:53:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config('spark.executor.memory', '2g')\\\n",
    "        .config(\"spark.sql.session.timeZone\", \"UTC\").config('spark.driver.memory', '2g').master(\"local[26]\")\\\n",
    "        .appName(\"wego-daily\").config('spark.driver.extraJavaOptions', '-Duser.timezone=UTC').config('spark.executor.extraJavaOptions', '-Duser.timezone=UTC')\\\n",
    "        .config(\"spark.sql.datetime.java8API.enabled\", \"true\").config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "        .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\\\n",
    "        .config(\"spark.driver.maxResultSize\", 0)\\\n",
    "        .config(\"spark.shuffle.spill\", \"true\")\\\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")\\\n",
    "        .config(\"spark.executor.extraJavaOptions\", \"-Dio.netty.tryReflectionSetAccessible=true\")\\\n",
    "        .config(\"spark.ui.showConsoleProgress\", \"false\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = os.path.join('/home/jptalusan/mta_stationing_problem/data/processed/apc_weather_gtfs.parquet')\n",
    "apcdata = spark.read.load(f)\n",
    "print(apcdata.columns)\n",
    "get_columns = ['trip_id', 'transit_date', 'arrival_time', 'block_abbr', 'scheduled_time', 'vehicle_id',\n",
    "              'stop_sequence', 'stop_id_original', 'load', 'ons', 'offs', 'route_id', 'route_direction_name']\n",
    "get_str = \", \".join([c for c in get_columns])\n",
    "apcdata.createOrReplaceTempView(\"apc\")\n",
    "# # filter subset\n",
    "query = f\"\"\"\n",
    "       SELECT {get_str}\n",
    "       FROM apc\n",
    "       \"\"\"\n",
    "print(query)\n",
    "apcdata = spark.sql(query)\n",
    "apcdata = apcdata.withColumn(\"route_id_direction\", F.concat_ws('_',apcdata.route_id, apcdata.route_direction_name))\n",
    "apcdata = apcdata.drop('route_id', 'route_direction_name')\n",
    "apcdf = apcdata.toPandas()\n",
    "fp = os.path.join('../scenarios/baseline/data/simulator_baseline2.pkl')\n",
    "apcdf.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup map\n",
    "* Match locations with vertices on the map\n",
    "* Need to update and add data to `_redo` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('code/data', 'shapefiles', \"tncounty\")\n",
    "gdf_county = gpd.read_file(fp)\n",
    "gdf_dav = gdf_county[gdf_county[\"NAME\"] == \"Davidson\"]\n",
    "gdf_dav = gdf_dav.to_crs(\"EPSG:4326\")\n",
    "\n",
    "G = ox.graph_from_polygon(gdf_dav.geometry.iloc[0], network_type='drive')\n",
    "G = ox.add_edge_speeds(G)\n",
    "G = ox.add_edge_travel_times(G)\n",
    "\n",
    "fp = os.path.join('code/data', 'davidson_graph.graphml')\n",
    "ox.save_graphml(G, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = os.path.join('/home/jptalusan/mta_stationing_problem/data/processed/apc_weather_gtfs.parquet')\n",
    "apcdata = spark.read.load(f)\n",
    "apcdata.columns\n",
    "get_columns = ['stop_sequence', 'stop_id_original', 'stop_name', 'map_latitude', 'map_longitude']\n",
    "get_str = \", \".join([c for c in get_columns])\n",
    "apcdata.createOrReplaceTempView(\"apc\")\n",
    "\n",
    "# # filter subset\n",
    "query = f\"\"\"\n",
    "SELECT {get_str}\n",
    "FROM apc\n",
    "\"\"\"\n",
    "apcdata = spark.sql(query)\n",
    "apcdata = apcdata.drop_duplicates(['stop_id_original'])\n",
    "apcdf = apcdata.toPandas()\n",
    "\n",
    "apcdf['nearest_node'] = ox.nearest_nodes(G, apcdf['map_longitude'], apcdf['map_latitude'])\n",
    "apcdf['nearest_edge'] = ox.nearest_edges(G, apcdf['map_longitude'], apcdf['map_latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('code/data', 'stops_node_matching.pkl')\n",
    "apcdf.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = ox.shortest_path(G, 202177835, 9702091174, weight='travel_time')\n",
    "ox.plot_graph_route(G, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = apcdf[apcdf['stop_id_original'] == 'MCC4_24']['nearest_node'].values[0]\n",
    "nn = apcdf[apcdf['stop_id_original'] == 'UNI2AEF']['nearest_node'].values[0]\n",
    "r = ox.shortest_path(G, cn, nn, weight='travel_time')\n",
    "cols = ['osmid', 'length', 'travel_time']\n",
    "attrs = ox.utils_graph.get_route_edge_attributes(G, r)\n",
    "tt = pd.DataFrame(attrs)[cols]['travel_time'].sum()\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fp = os.path.join('code/Scenarios/data/sample_trip_plan.pkl')\n",
    "with open(fp, 'rb') as f:\n",
    "    overall_trip_plan = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_trip_plan[0]['nearest_node'] = overall_trip_plan[0]['stop_id'].apply(lambda x: apcdf[apcdf['stop_id_original'] == x]['nearest_node'].values[0])\n",
    "overall_trip_plan[0]['next_node'] = overall_trip_plan[0]['nearest_node'].shift(-1)\n",
    "overall_trip_plan[0] = overall_trip_plan[0].dropna(subset=['next_node'])\n",
    "\n",
    "def get_travel_time(cn, nn):\n",
    "    try:\n",
    "        r = ox.shortest_path(G, cn, nn, weight='travel_time')\n",
    "        cols = ['osmid', 'length', 'travel_time']\n",
    "        attrs = ox.utils_graph.get_route_edge_attributes(G, r)\n",
    "        tt = pd.DataFrame(attrs)[cols]['travel_time'].sum()\n",
    "        \n",
    "        return tt\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "overall_trip_plan[0]['r'] = overall_trip_plan[0].apply(lambda x: get_travel_time(x['nearest_node'], x['next_node']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_trip_plan[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert APC load data to something static and fast to access (look up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fp = os.path.join('../scenarios/baseline/data/simulator_baseline2.pkl')\n",
    "baseline_data = pd.read_pickle(fp)\n",
    "baseline_data['dow'] = baseline_data['scheduled_time'].dt.dayofweek\n",
    "baseline_data['IsWeekend'] = (baseline_data[\"scheduled_time\"].dt.weekday >= 5).astype('int')\n",
    "baseline_data['time'] = baseline_data['scheduled_time'].dt.time\n",
    "baseline_data = baseline_data.query(\"load >= 0 and load <= 100\")\n",
    "baseline_data = baseline_data.groupby(['route_id_direction', 'block_abbr', 'stop_id_original', 'time', 'IsWeekend']).agg({'load':list})\n",
    "fp = os.path.join('../scenarios/baseline/data/loads_by_scheduled_time.pkl')\n",
    "baseline_data.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For boarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fp = os.path.join('../scenarios/baseline/data/simulator_baseline2.pkl')\n",
    "baseline_data = pd.read_pickle(fp)\n",
    "baseline_data['dow'] = baseline_data['scheduled_time'].dt.dayofweek\n",
    "baseline_data['IsWeekend'] = (baseline_data[\"scheduled_time\"].dt.weekday >= 5).astype('int')\n",
    "baseline_data['time'] = baseline_data['scheduled_time'].dt.time\n",
    "baseline_data = baseline_data.query(\"ons >= 0 and ons <= 100\")\n",
    "baseline_data = baseline_data.groupby(['route_id_direction', 'block_abbr', 'stop_id_original', 'time', 'IsWeekend']).agg({'ons':list})\n",
    "fp = os.path.join('../scenarios/baseline/data/ons_by_scheduled_time.pkl')\n",
    "baseline_data.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For alighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fp = os.path.join('../scenarios/baseline/data/simulator_baseline2.pkl')\n",
    "baseline_data = pd.read_pickle(fp)\n",
    "baseline_data['dow'] = baseline_data['scheduled_time'].dt.dayofweek\n",
    "baseline_data['IsWeekend'] = (baseline_data[\"scheduled_time\"].dt.weekday >= 5).astype('int')\n",
    "baseline_data['time'] = baseline_data['scheduled_time'].dt.time\n",
    "baseline_data = baseline_data.query(\"offs >= 0 and offs <= 100\")\n",
    "baseline_data = baseline_data.groupby(['route_id_direction', 'block_abbr', 'stop_id_original', 'time', 'IsWeekend']).agg({'offs':list})\n",
    "fp = os.path.join('../scenarios/baseline/data/offs_by_scheduled_time.pkl')\n",
    "baseline_data.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Travel Times\n",
    "* for each route_id_dir, block, stop to stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('../scenarios/baseline/data/simulator_baseline2.pkl')\n",
    "baseline_data = pd.read_pickle(fp).dropna(subset=['arrival_time']).sort_values(by=['transit_date', 'trip_id', 'stop_sequence'])\n",
    "baseline_data['dow'] = baseline_data['scheduled_time'].dt.dayofweek\n",
    "baseline_data['IsWeekend'] = (baseline_data[\"scheduled_time\"].dt.weekday >= 5).astype('int')\n",
    "baseline_data['time'] = baseline_data['scheduled_time'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "import datetime as dt\n",
    "\n",
    "def get_traveltimes(tdf):\n",
    "    tdf = tdf.sort_values('stop_sequence')\n",
    "    if len(tdf) <= 2:\n",
    "        return pd.DataFrame()\n",
    "    # HACK: This is for correcting the issue that the first stop's arrival_time starts much earlier than the scheduled time\n",
    "\n",
    "    tdf = tdf.reset_index(drop=True)\n",
    "    tdf['scheduled_timestamp'] = (tdf['arrival_time'] - dt.datetime(1970,1,1)).dt.total_seconds()\n",
    "    tdf['time_to_next_stop'] = tdf['scheduled_timestamp'].shift(-1) - tdf['scheduled_timestamp']\n",
    "    tdf.at[0, 'time_to_next_stop'] = (tdf.at[1, 'arrival_time'] - tdf.at[0, 'scheduled_time']).total_seconds()\n",
    "    tdf = tdf.drop('scheduled_timestamp', axis=1)\n",
    "    tdf = tdf.fillna(0)\n",
    "    return tdf\n",
    "    \n",
    "def applyParallel(dfGrouped, func):\n",
    "    with Pool(cpu_count()) as p:\n",
    "        ret_list = p.map(func, [group for name, group in dfGrouped])\n",
    "    return pd.concat(ret_list)\n",
    "\n",
    "out_arr = applyParallel(baseline_data.groupby(['block_abbr', 'route_id_direction', 'transit_date', 'trip_id']), get_traveltimes)\n",
    "tdf = out_arr.groupby(['route_id_direction', 'block_abbr', 'stop_sequence', 'stop_id_original', 'time', 'IsWeekend']).agg({'time_to_next_stop':list})\n",
    "fp = os.path.join('../scenarios/baseline/data/travel_time_by_scheduled_time.pkl')\n",
    "tdf.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':1}\n",
    "if 'b' in a:\n",
    "    print(\"TES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('../scenarios/baseline/data/travel_time_by_scheduled_time.pkl')\n",
    "tdf = pd.read_pickle(fp).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apcdf.query(\"trip_id == '219844'\").sort_values('scheduled_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apcdf.trip_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "11 * 60 + 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tdf.reset_index().query(\"route_id_direction == '34_FROM DOWNTOWN' and block_abbr == 3400 and stop_id_original == 'MCC4_22'\")\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.loc[('23_FROM DOWNTOWN', 2311, 8, 'VAIBRIEM',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = tdf.loc[('23_FROM DOWNTOWN', 2311)]\n",
    "adf = adf.query('IsWeekend == 0')\n",
    "# a = 26\n",
    "# b = 48\n",
    "# # adf.query('time_window == @a or time_window == @b').sample(1)['time_to_next_stop'].values[0]\n",
    "# adf = adf.explode('time_to_next_stop').query('time_to_next_stop > 0').reset_index()\n",
    "adf = adf.explode('time_to_next_stop').reset_index()\n",
    "# adf[adf['time_window'].isin(range(22, 32))].sample(1)\n",
    "\n",
    "adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tdf.loc[('3_TO DOWNTOWN', 300, 1, 'WHICHASF', 45, 0)]['time_to_next_stop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate travel distance pairs for all stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '/media/seconddrive/wego-occupancy-JP/data/static_gtfs/WeGoRawGTFS/04-october-2021-fixed.zip'\n",
    "feed = gk.read_feed(fp, dist_units='km')\n",
    "feed.validate()\n",
    "stop_times_df = gk.get_stop_times(feed)\n",
    "stop_pairs = []\n",
    "for trip_id, trip_df in stop_times_df.groupby('trip_id'):\n",
    "    trip_df['next_stop_id'] = trip_df['stop_id'].shift(-1)\n",
    "    trip_df = trip_df.fillna(0)\n",
    "    trip_df['shape_dist_traveled_km'] = (trip_df['shape_dist_traveled'].shift(-1) - trip_df['shape_dist_traveled'])\n",
    "    trip_df = trip_df[['stop_id', 'next_stop_id', 'shape_dist_traveled_km']][:-1]\n",
    "    stop_pairs.append(trip_df)\n",
    "stop_pairs = pd.concat(stop_pairs)\n",
    "stop_pairs = stop_pairs.drop_duplicates()\n",
    "stop_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('code/Scenarios/data/gtfs_distance_pairs_km.pkl')\n",
    "stop_pairs.to_pickle(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stop_id</th>\n",
       "      <th>next_stop_id</th>\n",
       "      <th>shape_dist_traveled_km</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MCC4_20</td>\n",
       "      <td>UNI2AEF</td>\n",
       "      <td>0.5776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNI2AEF</td>\n",
       "      <td>1SWOONM</td>\n",
       "      <td>0.6813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1SWOONM</td>\n",
       "      <td>1SJAMNM</td>\n",
       "      <td>0.3601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1SJAMNM</td>\n",
       "      <td>N1SOLDNM</td>\n",
       "      <td>0.3169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N1SOLDNM</td>\n",
       "      <td>DICGRANN</td>\n",
       "      <td>0.8610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17869</th>\n",
       "      <td>DICMARSM</td>\n",
       "      <td>DICLUCSN</td>\n",
       "      <td>0.2335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17870</th>\n",
       "      <td>DICLUCSN</td>\n",
       "      <td>DICLIGSF</td>\n",
       "      <td>0.2654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17872</th>\n",
       "      <td>DICRICSN</td>\n",
       "      <td>DICEVASF</td>\n",
       "      <td>0.2464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17873</th>\n",
       "      <td>DICEVASF</td>\n",
       "      <td>DICCLESN</td>\n",
       "      <td>0.2461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17874</th>\n",
       "      <td>DICCLESN</td>\n",
       "      <td>DICCLESM</td>\n",
       "      <td>0.1908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2366 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        stop_id next_stop_id  shape_dist_traveled_km\n",
       "0       MCC4_20      UNI2AEF                  0.5776\n",
       "1       UNI2AEF      1SWOONM                  0.6813\n",
       "2       1SWOONM      1SJAMNM                  0.3601\n",
       "3       1SJAMNM     N1SOLDNM                  0.3169\n",
       "4      N1SOLDNM     DICGRANN                  0.8610\n",
       "...         ...          ...                     ...\n",
       "17869  DICMARSM     DICLUCSN                  0.2335\n",
       "17870  DICLUCSN     DICLIGSF                  0.2654\n",
       "17872  DICRICSN     DICEVASF                  0.2464\n",
       "17873  DICEVASF     DICCLESN                  0.2461\n",
       "17874  DICCLESN     DICCLESM                  0.1908\n",
       "\n",
       "[2366 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fp = os.path.join('/home/jptalusan/gits/mta_simulator_redo/code_root/scenarios/baseline/data/gtfs_distance_pairs_km.pkl')\n",
    "stop_pairs = pd.read_pickle(fp)\n",
    "if len(stop_pairs.query(\"stop_id == 'JP'\")) > 0:\n",
    "    print(\"KP\")\n",
    "stop_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Disruption probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Disruption probabilities\n",
    "# Get service disruption dataset\n",
    "fp = os.path.join('code/data/Service Disruptions_07_2019_08_2022.csv')\n",
    "disruptions_df = pd.read_csv(fp)\n",
    "disruptions_df.head()\n",
    "disruptions_df['DATETIME'] = disruptions_df['DATE'] + ' ' + disruptions_df['TIME']\n",
    "disruptions_df['DATE'] = pd.to_datetime(disruptions_df['DATE'], format='%m/%d/%y', errors='coerce')\n",
    "disruptions_df['TIME'] = pd.to_datetime(disruptions_df['TIME'], format='%H:%M:%S', errors='coerce')\n",
    "disruptions_df['DATETIME'] = pd.to_datetime(disruptions_df['DATETIME'], format='%m/%d/%y %H:%M:%S', errors='coerce')\n",
    "\n",
    "# Remove weather related disruptions\n",
    "# disruptions_df = disruptions_df[(disruptions_df['REASON'] != 'Weather')].sort_values(by=['DATETIME']).reset_index(drop=True)\n",
    "print('Shape:', disruptions_df.shape)\n",
    "# disruptions_df = disruptions_df.drop(columns=['COMMENTS'])\n",
    "disruptions_df['BLOCK'] = disruptions_df['BLOCK'].astype('int32')\n",
    "\n",
    "# Convert to spark dataframe for merging\n",
    "# disruptions_sp = spark.createDataFrame(disruptions_df)\n",
    "# disruptions_sp = disruptions_sp.withColumn(\"BLOCK\", F.col(\"BLOCK\").cast(IntegerType()))\n",
    "disruptions_counts_df = disruptions_df.groupby('START_STOP_ABBR').agg('count')[['REASON']].reset_index()\n",
    "disruptions_counts_df.sort_values('REASON')\n",
    "# Count the number of trips throughout this time\n",
    "start_date = disruptions_df.sort_values(by=['DATETIME']).iloc[0]['DATETIME']\n",
    "end_date   = disruptions_df.sort_values(by=['DATETIME']).iloc[-1]['DATETIME']\n",
    "start_date, end_date\n",
    "\n",
    "# # filter subset\n",
    "get_columns = ['transit_date', 'trip_id', 'departure_time', 'stop_id_original']\n",
    "get_str = \", \".join([c for c in get_columns])\n",
    "\n",
    "query = f\"\"\"\n",
    "SELECT {get_str}\n",
    "FROM apc\n",
    "WHERE (transit_date >= '{start_date.date()}') AND (transit_date <= '{end_date.date()}')\n",
    "\"\"\"\n",
    "print(query)\n",
    "\n",
    "apcdataafternegdelete = spark.sql(query)\n",
    "apcdataafternegdelete = apcdataafternegdelete.dropna()\n",
    "trips_df = apcdataafternegdelete.toPandas()\n",
    "trips_df = trips_df.groupby('stop_id_original').agg('count').sort_values('trip_id').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(trips_df, disruptions_counts_df[['START_STOP_ABBR', 'REASON']], left_on='stop_id_original', right_on='START_STOP_ABBR')\n",
    "all_stop_probabilities = trips_df[['stop_id_original']]\n",
    "all_stop_probabilities = pd.merge(all_stop_probabilities, merged_df[['stop_id_original', 'probability']], on='stop_id_original', how='outer').fillna(0)\n",
    "all_stop_probabilities.sort_values('probability')\n",
    "\n",
    "fp = os.path.join('code/Scenarios/data/disruption_probabilities.pkl')\n",
    "all_stop_probabilities.to_pickle(fp)\n",
    "\n",
    "merged_df['probability'] = merged_df['REASON'] / merged_df['transit_date']\n",
    "merged_df['probability'] = merged_df['probability']/merged_df['probability'].max()\n",
    "merged_df.sort_values('probability').tail(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88d12193eb5d2fbe298f9bb9e457ac6a535b56551d0f537fc14a1636657a2895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
